Title,TLDR,Abstract,Introduction,Conclusion,Link,Notes
Blood-derived amyloid-β protein induces Alzheimer’s disease pathologies,"Amyloid-beta can travel, cancer-like, to the brain from other parts of body. Surgically attaching one mouse with Alzheimer's-like disease to another disease-free mouse so that they share blood circulation caused the normal mouse to contract Alzheimer's.","The amyloid-β protein (Aβ) protein plays a pivotal role in the pathogenesis of Alzheimer’s disease (AD). It is believed that Aβ deposited in the brain originates from the brain tissue itself. However, Aβ is generated in both brain and peripheral tissues. Whether circulating Aβ contributes to brain AD-type pathologies remains largely unknown. In this study, using a model of parabiosis between APPswe/PS1dE9 transgenic AD mice and their wild-type littermates, we observed that the human Aβ originated from transgenic AD model mice entered the circulation and accumulated in the brains of wild-type mice, and formed cerebral amyloid angiopathy and Aβ plaques after a 12-month period of parabiosis. AD-type pathologies related to the Aβ accumulation including tau hyperphosphorylation, neurodegeneration, neuroinflammation and microhemorrhage were found in the brains of the parabiotic wild-type mice. More importantly, hippocampal CA1 long-term potentiation was markedly impaired in parabiotic wild-type mice. To the best of our knowledge, our study is the first to reveal that blood-derived Aβ can enter the brain, form the Aβ-related pathologies and induce functional deficits of neurons. Our study provides novel insight into AD pathogenesis and provides evidence that supports the development of therapies for AD by targeting Aβ metabolism in both the brain and the periphery.","The deposition of amyloid-β protein (Aβ) protein in the extracellular space of the cerebral cortex and the walls of cerebral blood vessels is the hallmark of Alzheimer’s disease (AD) neuropathology.1 Aβ is generated by the proteolytic cleavage of the amyloid β precursor protein (APP), an integral type I membrane protein expressed in many tissues. It has traditionally been thought that the Aβ deposited in the brain originates from the brain itself. Circulating Aβ is capable of crossing the blood–brain barrier.2,3 Whether Aβ derived from the periphery contributes to AD-type pathologies in the brain remains largely unknown.","In conclusion, our study reveals that blood-derived Aβ can induce AD-type pathologies and functional deficits of neurons in the brains of Wt mice. Our findings provide novel insight into the understanding of AD pathogenesis and provide evidence that supports the development of therapies for AD by targeting Aβ metabolism in both the brain and the periphery.",https://twitter.com/foundmyfitness/status/1517867327399727104,
"Early Time-Restricted Feeding Improves 24-Hour Glucose Levels and Affects Markers of the Circadian Clock, Aging, and Autophagy in Humans","Time-restricted eating (6hr window) lowered mean blood glucose levels, increased ketones, lowered evening cortisol, elevated evening BDNF, and increased the levels of an autophagy gene by 22% compared to eating same calories within a 12hr window (humans).","Time-restricted feeding (TRF) is a form of intermittent fasting that involves having a longer daily fasting period. Preliminary studies report that TRF improves cardiometabolic health in rodents and humans. Here, we performed the first study to determine how TRF affects gene expression, circulating hormones, and diurnal patterns in cardiometabolic risk factors in humans. Eleven overweight adults participated in a 4-day randomized crossover study where they ate between 8 am and 2 pm (early TRF (eTRF)) and between 8 am and 8 pm (control schedule). Participants underwent continuous glucose monitoring, and blood was drawn to assess cardiometabolic risk factors, hormones, and gene expression in whole blood cells. Relative to the control schedule, eTRF decreased mean 24-hour glucose levels by 4 ± 1 mg/dl (p = 0.0003) and glycemic excursions by 12 ± 3 mg/dl (p = 0.001). In the morning before breakfast, eTRF increased ketones, cholesterol, and the expression of the stress response and aging gene SIRT1 and the autophagy gene LC3A (all p < 0.04), while in the evening, it tended to increase brain-derived neurotropic factor (BNDF; p = 0.10) and also increased the expression of MTOR (p = 0.007), a major nutrient-sensing protein that regulates cell growth. eTRF also altered the diurnal patterns in cortisol and the expression of several circadian clock genes (p < 0.05). eTRF improves 24-hour glucose levels, alters lipid metabolism and circadian clock gene expression, and may also increase autophagy and have anti-aging effects in humans.","Intermittent fasting (IF) covers a broad class of interventions that alternate periods of eating and extended fasting. IF interventions include periodic 24-hour fasts, intermittent energy restriction (e.g., the 5:2 diet), and time-restricted feeding. In animal models, IF has been found to improve cardiometabolic health, reduce cancer incidence, slow tumor growth, regenerate organs by increasing stem cell production, and increase lifespan [1,2]. In humans, data on IF is limited but suggest that it decreases body weight, insulin levels, blood pressure, inflammation, and appetite, and that it improves insulin sensitivity and lipid profiles [1,3,4,5]. These clinical benefits are driven by a reduction in insulin levels; improved insulin signaling; a reduction in oxidative stress; an increase in antioxidant defenses and autophagy; a reprogramming of aging-related pathways and hormones such as sirtuin 1 (SIRT1), brain-derived neurotrophic factor (BDNF), mechanistic target of rapamycin (mTOR), and insulin-like growth factor (IGF-1); and other mechanisms [6,7].While the benefits of some types of IF may stem mostly or entirely from energy restriction [8,9], one form of IF, called time-restricted feeding (TRF), has demonstrated benefits independent of energy restriction in both animals [10,11,12,13,14,15,16,17] and humans [18,19]. Since the median American eats over a 12-hour period [20], we define TRF as eating within a ≤10-hour period and fasting for at least 14 hours per day. (Although TRF can include Ramadan fasting, we consider Ramadan fasting to be a separate type of IF.) Studies in rodents report that TRF reduces body weight, improves glycemic control, lowers insulin levels, reduces blood pressure, prevents hyperlipidemia, decreases hepatic fat, improves inflammatory markers, slows tumor growth, and increases lifespan, even when food intake is matched to the control group [10,11,12,13,14,15,16,17,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]. To date, there have been nine pilot-sized trials of TRF in humans [18,19,41,42,43,44,45,46,47]. Interestingly, TRF improved weight loss and cardiometabolic endpoints—such as insulin levels, insulin sensitivity, and blood pressure—when participants ate early or in the middle of the day [18,19,41,42,43,44,45], but worsened cardiometabolic health or had null effects when participants ate late in the day [46,47,48].The circadian system may explain these dichotomous time-of-day effects. The circadian system orchestrates approximately 24-hour rhythms in metabolism, physiology, and behavior. It produces these rhythms through coordinated transcriptional–translational feedback loops involving clock genes such as BMAL1, CLOCK, PER1/2, and CRY1/2, which in turn cause oscillations in a myriad of downstream targets. For instance, insulin sensitivity and the thermic effect of food exhibit 24-hour rhythms, peaking in the morning [49]. A large number of plasma lipids [50] and age-related hormones such as cortisol, insulin, and growth hormone [51,52] also vary across the 24-hour day. Many of these metabolic and hormonal rhythms peak in the morning and are downregulated in the evening, implicating the morning as optimal for food intake [49]. Therefore, eating in sync with these rhythms may improve cardiometabolic health, as suggested by a growing number of human studies [53,54,55,56,57,58]. In contrast, eating in circadian misalignment with these rhythms by eating late in the day worsens several cardiometabolic endpoints, particularly glucose tolerance [59,60,61,62]. Therefore, TRF interventions where food intake is limited to early in the day may be particularly effective at improving cardiometabolic health.We recently conducted the first clinical trials of early time-restricted feeding (eTRF), which combines the benefits of intermittent fasting with eating early in the day to be in sync with circadian rhythms in metabolism [18,19]. eTRF is tantamount to eating dinner in the mid-afternoon and fasting for the rest of the day. In our first 5-week crossover study, we found that eTRF reduces insulin levels, improves insulin sensitivity, lowers blood pressure, and decreases lipid peroxidation in men with prediabetes [18]. In our second 4-day crossover study, we investigated the effects of eTRF on energy metabolism in adults who are overweight and found that eTRF does not affect energy expenditure but increases fat oxidation, reduces the hunger hormone ghrelin, and improves subjective appetite [18,19]. Here, we extend our analyses from the 4-day trial to perform the first study of how TRF affects diurnal patterns in cardiometabolic risk factors, selected hormones, and the expression of glycemic and circadian clock genes in humans. As an exploratory aim, we also investigated the effects on the expression of genes related to aging, autophagy, and oxidative stress. We hypothesized that eTRF would decrease mean 24-hour glucose levels, positively impact hormones such as IGF-1 and BDNF, and alter circadian clock gene expression.","Time-restricted feeding (TRF) is a novel form of intermittent fasting that improves cardiometabolic health, slows tumor progression, delays aging, and increases lifespan in rodents [10,11,12,13,14,15,16,17,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]. Pilot studies in humans similarly suggest that TRF improves clinical outcomes such as body weight, blood pressure, and insulin sensitivity [18,19,41,42,43,44,45,46,47], at least when food intake is limited to early or the middle of the day. These time-of-day effects may be explained by the circadian system, as eating in alignment with circadian rhythms in metabolism appears to improve cardiometabolic health [53,54,55,56,57,58]. However, the molecular mechanisms underlying TRF in humans were unknown. Here, to our knowledge and excluding studies on Ramadan fasting, we performed the first clinical trial to determine how TRF affects gene expression and diurnal patterns in cardiometabolic risk factors in humans. Our study is the first investigation into the molecular mechanisms underlying TRF in humans, as well as the second clinical trial of early time-restricted feeding (eTRF) in humans.Relative to the control schedule, eTRF decreased mean 24-hour levels of glucose. More than two dozen studies have previously reported diurnal rhythms in glycemic control, with glucose tolerance peaking in the morning [49]. The circadian system causes both insulin sensitivity and first-phase insulin secretion to be upregulated in the morning, and human studies report that the incremental glucose AUC is up to two-fold higher in the evening relative to the morning [64]. Since glucose tolerance is highest in the morning, we expected that shifting a majority of daily food intake to the morning would decrease mean 24-hour glucose levels. Indeed, the largest temporal differences in plasma glucose were observed in the late evening and while sleeping. In fact, in the control arm, glucose levels remained elevated during nearly half of the sleep episode. Given that the control arm was designed to represent median adult eating times in the US, eating dinner at 8 pm leads to a prolonged elevation of glucose levels while asleep and may have adverse metabolic consequences, such as impairing fat oxidation [65].In addition to lowering mean 24-hour glucose levels, eTRF also lowered fasting glucose and insulin in the morning, increased fasting insulin in the evening, and decreased 24-hour glycemic excursions. The decreased glucose and insulin in the morning were accompanied by an increase in AKT2 expression. Since the protein Akt2 is a downstream target of insulin signaling via the phosphatidylinositol 3-kinase (PI3K) pathway and plays a key role in insulin-stimulated glucose uptake, this suggests that eTRF may improve insulin signaling in the morning. This is consistent with data from our previous trial showing that 5 weeks of eTRF reduced insulin levels and improved insulin sensitivity during an oral glucose tolerance test administered in the morning [18]. Although we are not certain how to interpret the evening increases in fasting insulin and expression of the insulin signaling protein IRS2 (an insulin receptor substrate that modulates mitogenic and anti-apoptotic signaling pathways) in the eTRF arm, we note that they may be reflective of differences in cumulative energy intake at that time point during the day. The decrease in glycemic excursions was somewhat contrary to our expectations. We would have expected peak postprandial glucose levels to be higher in the eTRF arm since meals were eaten in short succession. However, we found the opposite to be true. We speculate that one possible explanation for the decrease in peak glucose levels, particularly at lunchtime, may be that circulating insulin levels were still elevated because breakfast was still being digested and therefore the β-cells in the pancreas did not have to be “re-awakened” to secrete insulin, eliminating the lag time between rising glucose and insulin levels and thereby lessening any spikes in plasma glucose. Importantly, our data suggest that TRF and any other approach where meals are eaten in short succession before the prior meal is fully digested may lower glycemic excursions—conferring additional glycemic benefits through mechanisms independent of the circadian system. This suggests that TRF interventions where the inter-meal interval is short may be particularly effective at improving 24-hour glucose levels. Conversely, we speculate that TRF interventions where meals are eaten too far apart (e.g., more than 4–5 hours apart), such as when the daily eating period is longer than 8–10 hours and/or involves only 2 meals/day, may be less effective at improving 24-hour glucose levels. This underscores the fact that while IF interventions are often viewed as synonymous with a reduction in meal frequency, practicing IF and reducing meal frequency are not the same thing, and future studies on IF should investigate whether the inter-meal interval and meal frequency influence health outcomes.Lipids and hormones were also affected by meal timing. eTRF increased LDL and HDL cholesterol in the morning, which may be attributed to the prolonged fasting period and greater reliance on fat oxidation in the eTRF arm [19]. It will be important to confirm in future studies that the slight increase in both LDL and HDL cholesterol in the morning is not pathophysiologic. However, we did not observe any increases in triglycerides and free fatty acids, as would be expected from the extended fasting. These results are similar to those reported in [42] but contrast with those reported in [18]. The reasons for the latter dissimilarity are unclear but may be due to differences in the intervention duration, the study population, or other factors. eTRF also increased β-hydroxybutyrate in the morning, relative to the control arm, thus demonstrating that even short-term daily fasting can modestly increase circulating ketones. Elevated ketone levels reduce oxidative stress, preserve lean mass [66], and have other metabolic effects such as decreasing hunger, although it is unclear whether the modest changes that we observed would be clinically significant. Among the hormonal endpoints, cortisol, which is a metabolic and circadian hormone, tended to increase in the morning and decrease in the evening. This suggests that eTRF may have increased the amplitude of the cortisol rhythm, providing a mechanism through which meal timing may impact the circadian system. Therefore, contrary to widespread belief, meal timing may directly impact the central circadian clock. Of the growth-related hormones, eTRF tended to elevate BDNF levels in the evening. BNDF promotes neuronal growth, development, and survival and is widely known to be increased by intermittent fasting in rodents [6]. Our study is one of the first trials to demonstrate that intermittent fasting can increase BDNF levels in humans. Although we additionally expected to observe a decrease in IGF-1, which is associated with cancer risk and aging, declines in IGF-1 and IGFBP-1 in the evening did not quite reach statistical significance (both p = 0.11).We also measured diurnal changes in gene expression. Only 4 days of eTRF surprisingly induced wide-sweeping changes in circadian clock gene expression, with 6 out of 8 circadian genes affected. Our data are corroborated by a trial reporting that a single bout of breakfast skipping changes the postprandial expression of several clock genes in whole blood [67], suggesting that a tightly-controlled, bidirectional feedback loop exists between meal timing and the circadian system. We also detected changes in several exploratory gene targets. Both SIRT1 and LC3A were upregulated in the morning before breakfast, while MTOR was upregulated in the evening. mTOR is a nutrient-sensing phosphatidylinositol 3-kinase-related kinase, which is stimulated by insulin, protein, and growth factors to drive protein synthesis and regulate cell growth, differentiation, and metabolism. Its observed upregulation in the evening likely mirrored the increase in fasting insulin. SIRT1 is nicotinamide adenosine dinucleotide (NAD)-dependent deacetylase that promotes insulin secretion and action; upregulates fat metabolism; protects against inflammation, oxidative stress, and DNA damage; increases telomere stability; and extends lifespan [68]. The increase in SIRT1 expression in the morning suggests that eTRF may also promote longevity in humans, as it does in animals [34]. A recent study in rodents found that at least 40% of the lifespan-extending effects of caloric restriction could instead be attributed to TRF [34]. Lastly, eTRF increased LC3A expression by 22% in the morning at the end of the 18-hour fast. LC3A encodes an essential structural component of autophagosomal membranes, and autophagy has been shown to play a major role in protecting against multiple chronic diseases such as diabetes, heart disease, cancer, and neurodegenerative diseases, by recycling damaged and used proteins and organelles. Increasing autophagy may have anti-aging or rejuvenating effects. Although no previous studies examining TRF as a meal timing intervention have investigated autophagy in either animals or humans, other studies on intermittent fasting conclude that several of the benefits of intermittent fasting are mediated through enhanced autophagy [69,70]. By comparison, we observed no changes in the expression of the four antioxidant genes measured.This study has several limitations. Although our study was highly rigorous in design, the sample size was small, and several of our endpoints were likely underpowered. For instance, substantial decreases in IGF-1 and IGFBP-1 (both p = 0.11) and increases in NOS3 gene expression (p = 0.13), which would provide further evidence that eTRF may slow aging and reduce cancer risk, did not reach statistical significance. Thus, our null findings for serum analytes and genes should be interpreted with caution, and these targets need to be re-tested in larger trials. Similarly, because some of our CGM data was not well-calibrated and had to be excluded, resulting in a smaller sample size, such favorable data should also be viewed more cautiously. Although, in this case, prior studies do concur that shifting food intake to early in the day improves mean daily glucose levels [42,54]. Second, our study intervention was only 4 days, which may be insufficient for circadian and/or metabolic adaptation to occur. Third, with the exception of glucose, all endpoints were measured at only two times of day. We lack data on endpoints in the postprandial state and also while sleeping. Because some endpoints, such as autophagy, are likely maximally upregulated during sleep, we may have missed detecting or underestimated the effect sizes of diurnal changes in several endpoints. Also, although the PM blood draws were taken after a 6-hour fast in both arms, there were differences in cumulative food intake between the study arms, which may have impacted the endpoints measured in the evening. In our trial, we were not able to draw blood at multiple time points throughout the day while our participants resided in the respiratory chamber, but future studies would benefit from measuring these endpoints across the 24-hour day. Lastly, gene expression data are known to be limited in nature and not necessarily reflective of changes in protein levels or activation.Collectively, our data suggest that eTRF improves several facets of health through both circadian- and fasting-related mechanisms. eTRF improves glycemic control by lowering 24-hour glucose levels, reducing glycemic excursions, and potentially by improving insulin signaling. Importantly, some of these improvements in glycemic excursions may be driven not only by eating earlier in the day but also by having a short inter-meal interval, suggesting that TRF interventions with longer inter-meal intervals may be less effective at improving glucose levels. We also found that eTRF alters diurnal patterns in fasting cholesterol, ketones, cortisol, and circadian clock genes; in particular, it modestly increases ketone levels in the morning and improves the amplitude of the cortisol rhythm. Finally, eTRF favorably affects hormones and genes related to longevity and autophagy such as BDNF, SIRT1, and LC3A. These important findings demonstrate that eTRF improves cardiometabolic health, alters diurnal rhythms, and may have anti-aging effects. Further research in humans is needed to replicate and extend these results.",https://twitter.com/foundmyfitness/status/1517554308178276352,
"Early Time-Restricted Feeding Improves Insulin Sensitivity, Blood Pressure, and Oxidative Stress Even without Weight Loss in Men with Prediabetes","Another randomized controlled clinical trial found time-restricted eating within a 6-hour window without reducing calories or losing weight improves insulin sensitivity, beta-cell function, blood pressure, oxidative stress and reduces evening appetite.","Intermittent fasting (IF) improves cardiometabolic health; however, it is unknown whether these effects are due solely to weight loss. We conducted the first supervised controlled feeding trial to test whether IF has benefits independent of weight loss by feeding participants enough food to maintain their weight. Our proof-of-concept study also constitutes the first trial of early time-restricted feeding (eTRF), a form of IF that involves eating early in the day to be in alignmentwithcircadianrhythmsinmetabolism.Menwith prediabetes were randomized to eTRF (6-hr feeding period, with dinner before 3 p.m.) or a control schedule (12-hr feeding period) for 5 weeks and later crossed over to the other schedule. eTRF improved insulin sensitivity, b cell responsiveness, blood pressure, oxidative stress, and appetite. We demonstrate for the first time in humans that eTRFimprovessome aspects of cardiometabolic health and that IF’s effects are not solely due to weight loss.","The circadian system, or internal biological clock, may explain why the effects of TRF appear to depend on the time of day. Glucose, lipid, and energy metabolism are all regulated by the circadian system, which upregulates them at some times of day and downregulates them at others (Poggiogalle et al., 2018; Scheeretal., 2009). For instance, in humans, insulin sensitivity, b cell responsiveness, and the thermic effect of food are all higher in the morning than in the afternoon or evening, suggesting that human metabolism is optimized for food intake in the morning (Morris et al., 2015a, 2015b; Poggiogalle et al., 2018; Scheer et al., 2009). Indeed, studies in humans show that eating in alignment with circadian rhythms in metabolism by increasing food intake at breakfast time and by reducing it at dinnertime improves glycemic control, weight loss, and lipid levels and also reduces hunger (Garaulet et al., 2013; Gill and Panda, 2015; Jakubowicz et al., 2013a, 2013b; Jakubowicz et al., 2015; Keim et al., 1997; Ruiz-Lozano et al., 2016). This suggests that the efficacy of IF interventions may depend not only on weight loss but also on the time of day of food intake. Moreover, these data from circadian studies suggest that combining two different meal timing strategies—IF and eating in alignment with circadian rhythms—may be a particularly beneficial form of IF. Wecallsuchacombinedintervention‘‘earlytime-restricted feeding’’ (early TRF; eTRF), and we define it as a subtype of TRF in which dinner is eaten in the mid-afternoon. To date, however, there had been no trials of eTRF in humans. We therefore decided to test eTRF in our proof-of-concept trial. Our goals were 2-fold: (1) to determine whether eTRF can improve cardiometabolic health and (2) to determine whether IF can havebenefits independent ofweight loss andfood intake. Our objective was not to examine the effectiveness or feasibility of eTRF but rather to determine the efficacy of eTRF when participants strictly adhere to their assigned meal times, food intake is precisely matched and monitored, and no weight loss occurs—that is, to measure the pure physiologic effects of eTRF uncontaminated by non-adherence. As such, our study is both the first clinical trial of eTRF and the most rigorously controlled trial of any form of IF in humans. We hypothesized that eTRF would improve glycemic control, improve vascular function, and reduce markers of inflammation and oxidative stress even when food intake is matched and no weight loss occurs.","In conclusion, 5 weeks of eTRF improved insulin levels, insulin sensitivity, b cell responsiveness, blood pressure, and oxidative stress levels in men with prediabetes—even though food intake was matched to the control arm and no weight loss occurred. Our trial was the first randomized controlled trial to show that IF has benefits independent of food intake and weight loss in humans. Our study was also the first clinical trial to test eTRF in humans and to show that eTRF improves some aspects of cardiometabolic health. Our trial tested eTRF in menwith prediabetes—a population at great risk of developing diabetes—and indicates that eTRF is an efficacious strategy for treating both prediabetes and likely also prehypertension. We speculate that eTRF—by virtue of combining daily intermittent fasting and eating in alignment with circadian rhythms in metabolism—will prove to be a particularly efficacious form of IF. In light of these promising results, future research is needed to better elucidate the mechanisms behind both intermittent fasting and meal timing, to determine which formsofIFandmealtimingareefficacious, and to translate them into effective interventions for the general population.",https://twitter.com/foundmyfitness/status/1517554311064023040,
Calorie Restriction with or without Time-Restricted Eating in Weight Loss,Study concluded time-restricted eating is no more beneficial than calorie restriction.Calorie intake was similar but time-restricted eating group only stopped eating 2-3 hrs max before the calorie-restricted group.Weak conclusion. Need better data.,"The long-term efficacy and safety of time-restricted eating for weight loss are not clear. We randomly assigned 139 patients with obesity to time-restricted eating (eating only between 8:00 a.m. and 4:00 p.m.) with calorie restriction or daily calorie restriction alone. For 12 months, all the participants were instructed to follow a calorie-restricted diet that consisted of 1500 to 1800 kcal per day for men and 1200 to 1500 kcal per day for women. The primary outcome was the difference between the two groups in the change from baseline in body weight; secondary outcomes included changes in waist circumference, body-mass index (BMI), amount of body fat, and measures of metabolic risk factors.Of the total 139 participants who underwent randomization, 118 (84.9%) completed the 12-month follow-up visit. The mean weight loss from baseline at 12 months was −8.0 kg (95% confidence interval [CI], −9.6 to −6.4) in the time-restriction group and −6.3 kg (95% CI, −7.8 to −4.7) in the daily-calorie-restriction group. Changes in weight were not significantly different in the two groups at the 12-month assessment (net difference, −1.8 kg; 95% CI, −4.0 to 0.4; P=0.11). Results of analyses of waist circumferences, BMI, body fat, body lean mass, blood pressure, and metabolic risk factors were consistent with the results of the primary outcome. In addition, there were no substantial differences between the groups in the numbers of adverse events. Among patients with obesity, a regimen of time-restricted eating was not more beneficial with regard to reduction in body weight, body fat, or metabolic risk factors than daily calorie restriction. (Funded by the National Key Research and Development Project [No. 2018YFA0800404] and others; ClinicalTrials.gov number, NCT03745612","Obesity is a major global public health challenge.1 Weight loss by means of lifestyle modification has been documented to be the cornerstone of weight management.2 Daily calorie restriction is a well-established primary weight-loss strategy for obese patients.3 However, most trials of dietary approaches to weight loss have reported modest (<5%) mean weight loss after 12 months,4 and long-term maintenance of weight loss remains a challenge. Thus, identification of alternative and feasible dietary interventions for weight loss is a major public health priority. Time-restricted eating is an intermittent-fasting regimen that involves a shortened period of time for eating within each 24-hour period. The method has gained popularity because it is a weight-loss strategy that is simple to follow, which may enhance adherence. Observational studies have suggested that the practice of eating meals later in the day was associated with weight gain and influenced the success of weight-loss therapy.5,6 Several pilot clinical studies showed that time-restricted eating resulted in reduction over time in the body weight and fat mass in patients with obesity.7-10 Lowe and colleagues tested the short-term effect of time-restricted eating on weight loss (in which food was eaten only during the period from 12:00 p.m. to 8:00 p.m.) in 116 obese patients. They found that weight loss with time-restricted eating was similar to that with ad libitum calorie intake.11 However, these studies did not provide information that was sufficiently conclusive to support evidence-based clinical guidelines for obesity. In addition, the long-term efficacy and safety of time-restricted eating as a weight-loss strategy are still uncertain, and the long-term effects on weight loss of time-restricted eating as compared with daily calorie restriction alone have not been fully explored. We conducted a randomized clinical trial to assess time-restricted eating with calorie restriction as compared with daily calorie restriction alone for the effects on weight loss and metabolic risk factors in obese patients.","In this 12-month trial, we found that the 8-hour time-restricted–eating regimen did not produce greater weight loss than the regimen of daily calorie restriction, with both regimens resulting in similar caloric deficits. In addition, time-restricted eating and daily calorie restriction produced similar effects with respect to reductions in body fat, visceral fat, blood pressure, glucose levels, and lipid levels over the 12-month intervention period. These results indicate that caloric intake restriction explained most of the beneficial effects seen with the time-restricted–eating regimen. Even so, our findings suggest that the time-restricted–eating regimen worked as an alternative option for weight management. We speculate that these data support the importance of caloric intake restriction when adhering to a regimen of time-restricted eating.",https://twitter.com/foundmyfitness/status/1517535767429492736,
Psychological stress and corticotropin-releasing hormone increase intestinal permeability in humans by a mast cell-dependent mechanism,"Psychological stress increases gut permeability commonly known as ""leaky gut."" A stress hormone called CRH activates immune cells in the gut which degrade proteins that hold the gut epithelium together. One more reason to buffer stress with exercise.","Intestinal permeability and psychological stress have been implicated in the pathophysiology of IBD and IBS. Studies in animals suggest that stress increases permeability via corticotropin-releasing hormone (CRH)-mediated mast cell activation. Our aim was to investigate the effect of stress on intestinal permeability in humans and its underlying mechanisms. Small intestinal permeability was quantified by a 2 h lactulose–mannitol urinary excretion test. In a first study, 23 healthy volunteers were subjected to four different conditions: control; indomethacin; public speech and anticipation of electroshocks. In a second study, five test conditions were investigated in 13 volunteers: control; after pretreatment with disodium cromoglycate (DSCG); administration of CRH; DSCG+CRH and DSCG+public speech. Indomethacin, as a positive comparator (0.071±0.040 vs 0.030±0.022; p<0.0001), and public speech (0.059±0.040; p<0.01), but not the shock protocol increased intestinal permeability. Similarly, salivary cortisol was only increased after public speech. Subgroup analysis demonstrated that the effect of public speech on permeability was only present in subjects with a significant elevation of cortisol. CRH increased the lactulose–mannitol ratio (0.042±0.021 vs 0.028±0.009; p=0.02), which was inhibited by the mast cell stabiliser DSCG. Finally, intestinal permeability was unaltered by public speech with DSCG pretreatment. Acute psychological stress increases small intestinal permeability in humans. Peripheral CRH reproduces the effect of stress and DSCG blocks the effect of both stress and CRH, suggesting the involvement of mast cells. These findings provide new insight into the complex interplay between the central nervous system and GI function in man.","The study of the interaction between psychological states and GI function is a complex and developing field. The bidirectional neurohumoral communication system between the brain and the gut (‘brain– gut axis’, BGA), mediating the effects of stress on the GI tract, has been considered a pivotal player in the pathogenesis of ‘functional’ GI disorders like IBS and functional dyspepsia for many years.1 Psychosocial stress and psychiatric comorbidity are common in IBS, can influence the onset of symptoms and predict the clinical outcome.2–4 However, the exact mechanisms through which stress exerts its role in IBS remain unclear. More recently, the importance of psychosocial stressors and the BGA has also been recognised in overt organic GI disease, such as IBD. Emerging evidence (reviewed in Ref. 5) suggests that a bidirectional brain–gut interplay exists in IBD, where psychological stress can result from active disease but can also contribute to triggering or exacerbating intestinal symptoms and inflammation. Increased intestinal permeability has been demonstrated in both functional and organic GI diseases.6–8 Mucosal inflammation is the key feature of IBD, but low-grade inflammation has also been implicated in symptom generation in IBS.9 Studies in animal models have identified increased intestinal permeability as the potential link between psychological stress and the activation of a mucosal immune response via enhanced penetration of luminal antigens.10–12 In rodents, psychological stressors like water-avoidance stress,11 13 14 restraint stress13 15 and crowding stress10 induced intestinal hyperpermeability in a corticotropinreleasing hormone (CRH) and mast cell-dependent fashion. However, the effect of psychological stressors on human intestinal permeability is unclear and mechanistic data in humans are lacking. The available data are limited to the effect of artificial stressors like hand immersion in cold water which resulted in increased secretion of albumin in the intestinal lumen.16 17 However, the relevance of this type of stressors in daily life is debatable. Moreover, it is unclear how albumin secretion relates to lumen-to-blood permeability. Recently, increased intestinal permeability quantified by a lactulose–mannitol excretion test was reported in troops after a 4 -week combat training,18 but due to the nature of the training programme, it is impossible to separate the effects of psychological from physical stress. The aim of the present study was to investigate the effect of acute psychological stress on small intestinal permeability in healthy humans and to unravel the underlying mechanism. Based on the available evidence in animals, we hypothesised that acute psychosocial stress would increase intestinal permeability in healthy humans and that this effect can be blocked with mast cell stabilisation. We also hypothesised that exogenous CRH would reproduce the effect of stress on small intestinal permeability.","In conclusion, we demonstrated that an acute psychological stressor increases small intestinal permeability in a subset of healthy humans with endocrinological signs of stress axis activation. Exogenous peripheral CRH recapitulated the effects of stress on barrier function. Finally, both the stress-induced and CRH-induced hyperpermeability were suppressed by previous mast cell stabilisation. Our data support further evaluation of CRH receptors, mast cells and mucosal barrier function as therapeutic targets in stress-sensitive GI disorders like IBS and IBD.",https://twitter.com/foundmyfitness/status/1516866751010512896,
Blood–brain barrier breakdown is an early biomarker of human cognitive dysfunction,"Maintaining blood-brain barrier integrity may be key to the prevention of brain aging. Approximately half of all dementias, including Alzheimer’s, may ultimately begin with the breakdown of the smallest blood vessels in the brain.","Vascular contributions to cognitive impairment are increasingly recognized1,2,3,4,5 as shown by neuropathological6,7, neuroimaging4,8,9,10,11, and cerebrospinal fluid biomarker4,12 studies. Moreover, small vessel disease of the brain has been estimated to contribute to approximately 50% of all dementias worldwide, including those caused by Alzheimer’s disease (AD)3,4,13. Vascular changes in AD have been typically attributed to the vasoactive and/or vasculotoxic effects of amyloid-β (Aβ)3,11,14, and more recently tau15. Animal studies suggest that Aβ and tau lead to blood vessel abnormalities and blood–brain barrier (BBB) breakdown14,15,16. Although neurovascular dysfunction3,11 and BBB breakdown develop early in AD1,4,5,8,9,10,12,13, how they relate to changes in the AD classical biomarkers Aβ and tau, which also develop before dementia17, remains unknown. To address this question, we studied brain capillary damage using a novel cerebrospinal fluid biomarker of BBB-associated capillary mural cell pericyte, soluble platelet-derived growth factor receptor-β8,18, and regional BBB permeability using dynamic contrast-enhanced magnetic resonance imaging8,9,10. Our data show that individuals with early cognitive dysfunction develop brain capillary damage and BBB breakdown in the hippocampus irrespective of Alzheimer’s Aβ and/or tau biomarker changes, suggesting that BBB breakdown is an early biomarker of human cognitive dysfunction independent of Aβ and tau.","We studied individuals who were cognitively normal as well as individuals with early cognitive dysfunction who were stratified on cerebrospinal fluid (CSF) analysis as either amyloid-β (Aβ)-positive (Aβ42+, <190 pg ml−1) or Aβ-negative (Aβ42−, >190 pg ml−1), or pTau-positive (pTau+, >78 pg ml−1) or pTau-negative (pTau−, <78 pg ml−1), using accepted cutoff values19,20,21. Supplementary Tables 1 and 2 show the patient demographics, clinical data and prevalence of vascular risk factors (VRFs) by level of clinical dementia rating (CDR) score and the number of impaired cognitive domains, respectively. Individuals diagnosed with vascular dementia and vascular cognitive impairment, and other disorders that might account for cognitive impairment, were excluded (see Methods).","In summary, we show that older adults with early cognitive dysfunction develop brain capillary damage associated with mural cell pericyte injury and BBB breakdown in the HC irrespective of Aβ and/or tau changes, suggesting that BBB breakdown is an independent, early biomarker of cognitive impairment unrelated to Aβ and tau. The independence of the BBB breakdown pathway from the Aβ/tau pathway in predicting cognitive impairment is further supported by logistic regression models indicating that BBB breakdown is not mediating the relationship between AD biomarkers and cognitive impairment (Supplementary Tables 7–10). Biomarker-based diagnostic approaches, including the recent research recommendations for AD17, mention vascular biomarkers, but suggest that CSF Aβ42 and pTau, and amyloid PET and tau PET, are the key biomarkers defining AD pathology, although they may not be causal to the disease process5,17,40. Our present findings support that neurovascular dysfunction may represent a previously underappreciated factor contributing to cognitive and functional decline, independent of the classic pathophysiological hallmarks of AD. Moreover, our findings point to the brain vasculature as an important new biomarker of cognitive dysfunction in both individuals without and with Aβ or pTau positivity, the latter indicating individuals in the Alzheimer’s continuum17.",,"The paper had no intrroduction or conclusion. First paragraph of Main is plugged in as Intro and last paragraph of main starting with ""In Summary"" is plugged in as conclusion"
Auditory deep sleep stimulation in older adults at home: a randomized crossover trial,A wearable device that plays specific sounds at timed intervals was able to promote deep sleep through auditory brain stimulation in some participants.Deep sleep is important for brain repair and also for memory. ,"Auditory stimulation has emerged as a promising tool to enhance non-invasively sleep slow waves, deep sleep brain oscillations that are tightly linked to sleep restoration and are diminished with age. While auditory stimulation showed a beneficial effect in lab-based studies, it remains unclear whether this stimulation approach could translate to real-life settings. We present a fully remote, randomized, cross-over trial in healthy adults aged 62–78 years (clinicaltrials.gov: NCT03420677). We assessed slow wave activity as the primary outcome and sleep architecture and daily functions, e.g., vigilance and mood as secondary outcomes, after a two-week mobile auditory slow wave stimulation period and a two-week Sham period, interleaved with a two-week washout period. Participants were randomized in terms of which intervention condition will take place first using a blocked design to guarantee balance. Participants and experimenters performing the assessments were blinded to the condition. Out of 33 enrolled and screened participants, we report data of 16 participants that received identical intervention. We demonstrate a robust and significant enhancement of slow wave activity on the group-level based on two different auditory stimulation approaches with minor effects on sleep architecture and daily functions. We further highlight the existence of pronounced inter- and intra-individual differences in the slow wave response to auditory stimulation and establish predictions thereof. While slow wave enhancement in healthy older adults is possible in fully remote settings, pronounced inter-individual differences in the response to auditory stimulation exist. Novel personalization solutions are needed to address these differences and our findings will guide future designs to effectively deliver auditory sleep stimulations using wearable technology.","While basic and applied research in promoting human health has increased substantially in the last decades, its translation into clinical applications with access for a wider public has lagged behind1,2,3. In contrast, there is an ever-growing range of mobile health devices and technologies that found their way into the consumer market and into people’s homes, yet clinical validations of most of these applications is lacking4,5. With these technologies, an important platform has been developed to move the findings from well-controlled lab studies to in-field applications6. However, to establish the potential and limitations of preventive and therapeutic health approaches in real-life settings, studies are needed that identify individual differences, establish predictors for successful applications, and elucidate intended and unintended effects of the prolonged intervention in different contexts. In this regard, translating in-lab methods that promote healthy sleep have gained increased attention because sleep could be a key player in promoting brain and body health up until old age7,8. Sleep in older adults is hallmarked by reduced sleep duration, increased sleep fragmentation, and reduced sleep efficiency9,10,11. Most pronouncedly, sleep becomes more superficial with age, which is reflected in a decrease of sleep slow waves12,13. Slow waves are dominant low-frequency (i.e., <4 Hz) brain rhythms during deep non-rapid eye movement (NREM) sleep, which are considered to mirror sleep intensity and are fundamental for healthy and consolidated sleep14. They have been implicated in promoting brain plasticity15, memory formation16, immune-supportive functions17, and have recently been inversely related to amyloid burden in older adults18,19. Therefore, deep sleep enhancement strategies will potentially contribute to healthy aging, which is highly relevant considering the growing proportion of older people in our society. Specifically, the enhancement of slow waves might be an ideal target to improve sleep quality. Consequently, applications that enhance slow waves in a non-pharmacological, non-invasive, and physiological way have become a central topic in sleep research. Among others, the application of acoustic stimuli during sleep have crystallized as a very promising avenue. In 2013, Ngo and colleagues20 reported that acoustic stimuli specifically targeted to the up-phase of the endogenous slow wave can enhance slow wave amplitude along with improved declarative memory in young participants. Thereafter, several in-lab studies from different research groups replicated the beneficial effect of phase-targeted or rhythmic (i.e., around 1 Hz) acoustic stimuli to boost slow waves mainly in young participants, but also in middle-aged, and older populations20,21,22,23,24,25,26,27,28,29,30. These slow wave enhancement effects particularly benefited declarative memory consolidation20,21,22,23,24,25,31, but single studies have also reported benefits for executive29, immune-supportive17, and autonomic functions28. Collectively, there is convincing evidence that slow waves can be enhanced in in-lab settings during single session applications along with some specific brain and body functions. Owing to these first in-lab results, this deep sleep modulation approach has already found its way into the consumer-market32,33. However, no auditory sleep stimulation technology has yet established itself as certified medical wearable. To date, no controlled, randomized clinical trial exists that investigated the effect of auditory up-phase stimulation over multiple nights, in at-home settings and its effects on sleep and daytime functioning. In order to overcome this gap, we recently developed a portable sleep monitoring and feedback-controlled slow wave modulation device with research-grade accuracy34. Here, we used this system in the first randomized controlled cross-over clinical trial applying auditory stimulation during slow wave sleep over multiple weeks in healthy older adults between 60 and 80 years of age and without diagnosed sleep disorders in order to establish effectiveness thereof in enhancing slow wave activity as our primary outcome. Older adults are of specific interest because of their reduced slow waves and their less pronounced potential to benefit from auditory stimulation as revealed in an in-lab study35. The device was worn at home and self-applied by the participants. Their sleep schedule was not restricted to a specific schedule. In addition, we implemented mobile phone based daily ecological momentary assessments (EMA) of mood, subjective sleep quality, and vigilance to determine the direct effects of stimulation on these secondary outcome parameters. We demonstrate on the group level that slow waves can be enhanced over multiple days in older adults, also in non-controlled, in-home settings, and for two different stimulation protocols. We establish predictors that define participants with strong and weak responses to auditory stimulation, and predictors that relate to nightly variance within participants. While slow wave enhancement in healthy older adults is possible in at-home settings and could potentially benefit related functions, we must consider that clear inter-individual differences in the response to auditory stimulation exists and novel personalization solutions are needed to address these differences to achieve high efficacy and effectiveness in the field.","In conclusion, in this first in-home clinical trial we robustly show that SWA in the low-frequency range can be enhanced in healthy older adults over multiple nights in a real-life setting. We illustrate that there are strong and weak responders, which represents a trait characteristic and is mainly predicted by baseline SWA. We further highlight that more stimulations are not necessarily more beneficial and that future rational design of auditory deep sleep stimulation should consider the optimal implementation of breaks to maximize overall deep sleep enhancement. While this study represents an example of a successful translation of in-lab findings to uncontrolled environments in a population with reduced slow waves, it also raises concerns about possible un-intended secondary effects that are independent of the modulation of deep sleep, such as a slight reduction in REM sleep along with a reduced subjective mood rating the next day. These findings highlight the importance of conducting future large-scale studies to establish secondary effects across time and diverse populations. Our study provides means to optimize the rational design of auditory deep sleep stimulation and represents a crucial step towards successful and safe in-home implementation for prevention and therapy in the general population.",https://twitter.com/foundmyfitness/status/1514647989582635031,
Detection of microplastics in human lung tissue using μFTIR spectroscopy,"We have to stop the ubiquitousness of single-use plastics. Microplastics are small pieces of plastic that are everywhere in the environment including soil, drinking water, fresh- and saltwater bodies, and air. Now they have been found in human lungs!","Airborne microplastics (MPs) have been sampled globally, and their concentration is known to increase in areas of high human population and activity, especially indoors. Respiratory symptoms and disease following exposure to occupational levels of MPs within industry settings have also been reported. It remains to be seen whether MPs from the environment can be inhaled, deposited and accumulated within the human lungs. This study analysed digested human lung tissue samples (n = 13) using μFTIR spectroscopy (size limitation of 3 μm) to detect and characterise any MPs present. In total, 39 MPs were identified within 11 of the 13 lung tissue samples with an average of 1.42 ± 1.50 MP/g of tissue (expressed as 0.69 ± 0.84 MP/g after background subtraction adjustments). The MP levels within tissue samples were significantly higher than those identified within combined procedural/laboratory blanks (n = 9 MPs, with a mean ± SD of 0.53 ± 1.07, p = 0.001). Of the MPs detected, 12 polymer types were identified with polypropylene, PP (23%), polyethylene terephthalate, PET (18%) and resin (15%) the most abundant. MPs (unadjusted) were identified within all regions of the lung categorised as upper (0.80 ± 0.96 MP/g), middle/lingular (0.41 ± 0.37 MP/g), and with significantly higher levels detected in the lower (3.12 ± 1.30 MP/g) region compared with the upper (p = 0.026) and mid (p = 0.038) lung regions. After subtracting blanks, these levels became 0.23 ± 0.28, 0.33 ± 0.37 and 1.65 ± 0.88 MP/g respectively. The study demonstrates the highest level of contamination control and reports unadjusted values alongside different contamination adjustment techniques. These results support inhalation as a route of exposure for environmental MPs, and this characterisation of types and levels can now inform realistic conditions for laboratory exposure experiments, with the aim of determining health impacts.","Microplastics (MPs), defined herein as plastic particles between 1 μm and 5 mm (Hartmann et al., 2019), are present in all environmental compartments; from marine and freshwater bodies (GESAMP, 2015), to soil (Wang et al., 2019), food, drinking water (Danopoulos et al., 2020a; Danopoulos et al., 2020b), and air (Allen et al., 2019; Dris et al., 2017; Cai et al., 2017; Jenner et al., 2021). For the latter, suspended MP particles have been isolated from many atmospheric locations, including urbanised city centres (Cai et al., 2017; Wright et al., 2019a; Liu et al., 2019a), indoor households (Dris et al., 2017; Jenner et al., 2021; Vianello et al., 2019; Zhang et al., 2020), and remote outdoor regions (Allen et al., 2019). Previous work highlights that citizens are exposed to higher concentrations of MP within their homes (Jenner et al., 2021) or outdoor areas of high human activity (Jenner et al., 2022), and this results in ubiquitous and unavoidable human exposure (Prata et al., 2020). Consequently, there is an increasing concern regarding the hazards associated with MP ingestion, dermal contact, and inhalation (Prata et al., 2020). Synthetic fibres have previously been observed within human lung tissue samples (Pauly et al., 1998), yet limited studies confirm the presence of MPs within the lungs alongside chemical analysis tools, such as μRaman and μFTIR spectroscopy (Amato-Lourenço et al., 2021). Reliance upon observational criteria alone to distinguish between MP and non-MPs, can lead to over and under-estimated MP counts, and a lack of information relating to polymer or additive type (Eriksen et al., 2013; Hidalgo-Ruz et al., 2012). The plausibility of MP inhalation has been highlighted (Prata, 2018; Wright and Kelly, 2017) and MPs with a width as small as 5 μm have been reported within air samples (Wright et al., 2019a; Li et al., 2020). Upon environmental release, plastics are exposed to oxidation, mechanical stress and biological action, resulting in embrittlement and fragmentation, forming MPs, and eventually nanoplastics (NPs) (<1 μm), as well as release into the environment in their primary form (Hidalgo-Ruz et al., 2012). Historical studies report respiratory symptoms and disease at an occupational level of exposure in synthetic textile, flock, and vinyl chloride workers (Prata, 2018), and as such, support inhalation as an exposure route for MPs. However, it remains unclear whether MPs can enter and remain in the lungs of the general population due to environmental exposure, rather than the chronic levels seen within industry settings. MPs are designed to be robust materials, unlikely to break down within the lungs (Law et al., 1990), potentially leading to accumulation over time depending on aerodynamic diameter and respiratory defences (Prata, 2018). The mounting concern surrounding airborne MPs stems from the unknown polymer types, levels of exposure, and consequences of their inhalation. MP characteristics such as size, shape, vectored absorbed pollutants and pathogens, as well as plastic monomer or additive leaching, have been highlighted as potential promoters of cytotoxicity (Wright and Kelly, 2017). MPs are consistently identified within air samples, their concentration is highest indoors (Dris et al., 2017; Vianello et al., 2019; Zhang et al., 2020) and within highly populated areas (Cai et al., 2017), they are readily suspended at times of high human activity (Zhang et al., 2020) and are often small and fibrous (Liu et al., 2019a). Together, these concerns highlight the necessity for accurate tissue analysis to understand the potential for these synthetic polymers to penetrate the human respiratory system and cause harm. This study aims to identify any MP particles present in digested human lung tissue samples, while also accounting for procedural and laboratory blank contamination. Any particles isolated from lung tissue have been chemically characterised using μFTIR spectroscopy (with a 3 μm lower size limit of detection).","In summary, this study is the first to report MPs within human lung tissue samples, using μFTIR spectroscopy. The abundance of MPs within samples, significantly above that of blanks, supports human inhalation as a route of environmental exposure. MPs with dimensions as small as 4 μm but also, surprisingly, >2 mm were identified within all lung region samples, with the majority being fibrous and fragmented. The knowledge that MPs are present in human lung tissues can now direct future cytotoxicity research to investigate any health implications associated with MP inhalation.",https://twitter.com/foundmyfitness/status/1513596432023887875,
"Use of medium chain triglyceride (MCT) oil in subjects with Alzheimer's disease: A randomized, double-blind, placebo-controlled, crossover study, with an open-label extension","Two tablespoons of medium-chain triglycerides (MCTs) daily for 3 months, improved or stabilized cognition in older adults with Alzheimer's disease (compared to placebo control).","Cerebral glucose and insulin metabolism is impaired in Alzheimer's disease (AD). Ketones provide alternative energy. Will medium chain triglyceride (MCT) oil, a nutritional source of ketones, impact cognition in AD? This was a 6‐month randomized, double‐blind, placebo‐controlled, crossover study, with 6‐month open‐label extension in probable AD subjects, on stable medications. MCT dose was 42 g/day, or maximum tolerated. Cognition was assessed with Mini‐Mental State Examination (MMSE), Montreal Cognitive Assessment (MoCA), and Cognigram®. Twenty subjects, average age 72.6 years, 45% women, 70% university educated had baseline MMSE 22.6/30 (10–29); MoCA 15.6/30 (4–27); baseline Cognigram® Part 1: 65–106, Part 2: 48–107. Average MCT oil consumption was 1.8 tablespoons/day (25.2 g, 234 kcal). Eighty percent remained stable or improved. Longer MCT exposure and age > 73, resulted in higher final MMSE (P < .001) and Cognigram® 1 scores. This is the longest duration MCT AD study to date. Eighty percent had stabilization or improvement in cognition, and better response with 9‐month continual MCT oil.","Alzheimer's disease (AD) is the most common form of dementia affecting one in four people over the age of 80. Worldwide prevalence is estimated at 50 million cases, 1 with cases expected to double every 20 years. 2 Current pharmacotherapy provides symptomatic relief at best, and does not affect the disease trajectory. 3 No new pharmaceutical agents have reached the Canadian market since 1997. The brain is an obligate glucose metabolizer using 120 to 130 g/day of glucose. 4 It uses 16% of the body's total O2 consumption, despite representing only 2.0% to 2.3% of adult body weight. In conditions of low carbohydrate intake or fasting, the body uses ketones (acetoacetate and beta hydroxybutyrate [BHB]) as an alternative energy source to glucose. Ketones are normally generated in fasting states from beta‐oxidation of adipose stores to maintain cerebral function. In long‐term fasting, ketones can supply > 60% of the brain's energy requirements, 4 , 5 and are preferentially taken up by the brain over glucose. 6 , 7 This occurs in cognitively normal younger and older adults, as well as in those with mild cognitive impairment (MCI) and AD. 8 , 9 Ketones can also be induced with a very low carbohydrate high fat (VLCHF) diet. 13 Medium chain triglyceride (MCT) oil has the potential to produce a nutritional source of ketones for an alternative brain fuel to glucose, 10 , 11 , 12 , 13 , 14 , 15 , 16 or by the consumption of MCT oil or esterases in freeze‐dried form. 15 This is independent of the fasting state or carbohydrate intake. Long‐term compliance with fasting or VLCHF and LCHF diet regimes is challenging and requires strict medical supervision. 17 Hence, the potential advantage of nutritional ketone sources (MCT) over these restrictive diets. Our recent study showed a clear dose‐dependent effect on ketone (BHB) generation with varying doses of MCT supplementation, and was found to be equivalent in young, elderly, and AD subjects 18 . In AD, the brain is unable to use glucose normally, 19 , 20 causing hypofunction of 20% to 40% in key areas of the brain responsible for the symptoms in AD. This hypometabolism can be demonstrated with fluorodeoxyglucose positron emission tomography scanning 21 even in preclinical AD. It results from several factors: an abnormality of cerebral glucose receptors 22 , cerebral insulin resistance 23 , and abnormal cerebral glucose metabolism 24 . Hence, MCT oil may provide benefit as a source of readily available alternative energy: ketones. (Metabolism of MCT is discussed in the supporting information Appendix A). Because ketones generated in the liver cannot be used by the liver for energy, all therefore flow from the liver to extra‐hepatic tissues as fuel. 27 Given their origin from saturated fat sources (palm and coconut oil), there is potential concern for effects on the lipid profile or body composition in subjects consuming the oil regularly. 28 In humans, ketone infusions have been shown to reduce hormonal responses to acute hypoglycemia and improve cognitive functioning. 29 They also increase cerebral blood flow. 9 This has had a clinical benefit in conditions such as intractable epilepsy 30 . However, the data in cognitive impairment is mixed 12 , 14 , 15 , 16 , 31 , 32 , 33 , 34 , continuing the speculation about their potential role in aging and AD. 35 The few clinical studies that have been done are only in subjects with MCI or early AD, and are of short duration—a few weeks or months. The purpose of this study was to use MCT oil supplementation to address some of the gaps in the current literature, including its effect on cognition in subjects with mild to moderate AD, the effect of longer duration (> 6 months), and the tolerance and safety (weight, lipid profile) with longer duration therapy. We hypothesized that intermittent elevation of serum BHB, provided by regular ingestion of MCT oil, would result in stabilization or improvement of cognitive function in subjects with mild–moderate AD, without significant cardiovascular safety concerns.","Overall, there was marked stability of the cognitive function over the 15 months of the trial. For efficacy, this study showed (1) an effect on attention and psychomotor domains of Cognigram® proportional to amount of MCT oil consumed; (2) better response in those with a higher baseline MMSE, independent of age; and (3) no apparent effect of APOE ε4 status on response. In terms of safety, there was (1) no effect on body weight, composition, or serum lipids with 11 months of MCT oil and 4 months of olive oil consumption; and (2) MCT at three tablespoons daily (42 g) was difficult for some to tolerate (due to GI side effects) Given the paucity of new therapeutic options for AD treatment, it is increasingly urgent to explore other therapeutic options. The expanding basic science and clinical research highlighting the physiological and clinical basis for efficacy of ketone supplementation, raises an intriguing and hopeful new therapeutic addition for AD dementia treatment.",https://twitter.com/foundmyfitness/status/1512145084040433683,
Sulforaphane exhibits antiviral activity against pandemic SARS-CoV-2 and seasonal HCoV-OC43 coronaviruses in vitro and in mice,Sulforaphane (compound high in broccoli sprouts) has antiviral activity against SARS-CoV2 including Omicron. Prophylactic sulforaphane treatment in animals reduced viral replication in the lungs by 1.5 orders of magnitude and decreased lung injury.,"Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), the cause of coronavirus disease 2019 (COVID-19), has incited a global health crisis. Currently, there are limited therapeutic options for the prevention and treatment of SARS-CoV-2 infections. We evaluated the antiviral activity of sulforaphane (SFN), the principal biologically active phytochemical derived from glucoraphanin, the naturally occurring precursor present in high concentrations in cruciferous vegetables. SFN inhibited in vitro replication of six strains of SARS-CoV-2, including Delta and Omicron, as well as that of the seasonal coronavirus HCoV-OC43. Further, SFN and remdesivir interacted synergistically to inhibit coronavirus infection in vitro. Prophylactic administration of SFN to K18-hACE2 mice prior to intranasal SARS-CoV-2 infection significantly decreased the viral load in the lungs and upper respiratory tract and reduced lung injury and pulmonary pathology compared to untreated infected mice. SFN treatment diminished immune cell activation in the lungs, including significantly lower recruitment of myeloid cells and a reduction in T cell activation and cytokine production. Our results suggest that SFN should be explored as a potential agent for the prevention or treatment of coronavirus infections.","The coronavirus disease 2019 (COVID-19) pandemic has resulted in substantial global morbidity and mortality. While an unprecedented effort has led to the development of highly effective vaccines, many people remain vulnerable to developing severe disease due to inadequate accessibility or unwillingness to be vaccinated, as well as poor immune responses in certain populations. Other therapeutic approaches have also been developed for COVID-19, including early treatments with monoclonal antibodies against Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2)1, convalescent plasma2,3, and antivirals4. Immunomodulators have also been utilized to modify disease and prevent mortality5. Early intervention after symptom onset has been shown to be most effective in preventing severe disease and hospitalizations6,7. Therefore, the ideal therapy should be one that is readily available and easily administered to patients. Among the direct-acting antivirals, molnupiravir and ritonavir-boosted nirmatrelvir (Paxlovid) are the only oral agents currently authorized by the United States Food and Drug Administration for the treatment of patients with COVID-197,8,9. Additional oral antiviral therapeutics are urgently needed to prevent more severe disease, hospitalization, and death. The multi-functional phytochemical sulforaphane (SFN) is the isothiocyanate derived from enzymatic hydrolysis of its precursor glucoraphanin, a glucosinolate found in high concentrations in broccoli (Brassica oleracea italica) and other cruciferous vegetables. SFN is a potent naturally occurring activator of the transcription factor nuclear factor erythroid 2-related factor 2 (NRF2), with well-documented antioxidant and anti-inflammatory effects10,11,12. Treatment with SFN increased phagocytic activity of alveolar macrophages13 and reduced lung injury in animal models of acute respiratory distress syndrome (ARDS)14. SFN also decreased the levels of IL-6 and viral load in human subjects infected with live attenuated influenza virus15,16. Numerous clinical trials utilizing SFN have demonstrated favorable pharmacokinetics after oral dosing and documented excellent tolerability and safety10,17,18,19. SFN was identified after an exploratory screening of readily available drugs and compounds for efficacy against human coronaviruses. Initial testing was performed in vitro using seasonal coronavirus HCoV-OC43. Subsequently, drugs that exhibited at least moderate activity against HCoV-OC43 were tested in vitro against SARS-CoV-2. We report here that SFN inhibits in vitro HCoV-OC43 and SARS-CoV-2 infections of mammalian cells and appears to have a synergistic interaction with remdesivir. In addition, SFN reduces viral load and pulmonary pathology in a mouse model of SARS-CoV-2 infection.","In summary, we documented that SFN can inhibit in vitro and in vivo replication of SARS-CoV-2 at pharmacologically and potentially therapeutically achievable concentrations. Further, it can modulate the inflammatory response, thereby decreasing the consequences of infection in mice when administered prior to infection. Given that SFN is orally bioavailable, commercially available, and has limited side effects, our results suggest it could be a promising approach for the prevention and treatment of COVID-19 as well as other coronavirus infections. Further studies are needed to address these possibilities.",https://twitter.com/foundmyfitness/status/1508866504107855875,
Fragmented Sleep Accelerates Tumor Growth and Progression through Recruitment of Tumor-Associated Macrophages and TLR4 Signaling,"Poor-quality sleep can speed cancer growth, increase tumor aggressiveness and lower the immune system's ability to eradicate early tumors. Animals that had their sleep disrupted for 7 days formed tumors that were twice as large and aggressive.","Sleep fragmentation (SF) is a highly prevalent condition and a hallmark of sleep apnea, a condition that has been associated with increased cancer incidence and mortality. In this study, we examined the hypothesis that sleep fragmentation promotes tumor growth and progression through proinflammatory TLR4 signaling. In the design, we compared mice that were exposed to sleep fragmentation one week before engraftment of syngeneic TC1 or LL3 tumor cells and tumor analysis four weeks later. We also compared host contributions through the use of mice genetically deficient in TLR4 or its effector molecules MYD88 or TRIF. We found that sleep fragmentation enhanced tumor size and weight compared with control mice. Increased invasiveness was apparent in sleep fragmentation tumors, which penetrated the tumor capsule into surrounding tissues, including adjacent muscle. Tumor-associated macrophages (TAM) were more numerous in sleep fragmentation tumors, where they were distributed in a relatively closer proximity to the tumor capsule compared with control mice. Although tumors were generally smaller in both MYD88−/− and TRIF−/− hosts, the more aggressive features produced by sleep fragmentation persisted. In contrast, these more aggressive features produced by sleep fragmentation were abolished completely in TLR4−/− mice. Our findings offer mechanistic insights into how sleep perturbations can accelerate tumor growth and invasiveness through TAM recruitment and TLR4 signaling pathways.","In recent years, the possibility that sleep duration and overall sleep characteristics may affect overall cancer outcomes has been advanced (1). Indeed, in several epidemiologic studies spanning the last decade, the presence of altered sleep duration, both shortened and prolonged sleep, has been associated with higher incidence or adverse prognosis for several solid tumors (2–17). However, although the role of the circadian clock system in tumorigenesis has been extensively explored (18, 19), no animal models have thus far examined whether the association between disrupted sleep and tumorigenesis is indeed recapitulated, and if so, what potential mechanisms may underlie such associations. In this context, some efforts to explore causal associations between a highly prevalent sleep disorder, namely obstructive sleep apnea (OSA), and cancer have also taken place (20, 21), and have operated under the assumption that the intermittent hypoxemia that characterizes patients with OSA during their sleep period is likely to mimic the biologic events that drive tumor growth (1, 22–27). The major findings from these initial studies indicate that the periodic oscillations in overall oxygenation during sleep in patients with OSA impose overall adaptive changes in the tumor metabolic cellular substrate that enhances their proliferative and invasiveness properties (28). However, these studies failed to explore another hallmark characteristic of OSA, namely sleep fragmentation (SF), that is, the presence of recurrent arousals aimed at restoring airflow that lead to sleep discontinuity. Using a similar logical paradigm, we hypothesized that chronic sleep fragmentation, a very frequent occurrence in many human disorders, including OSA, would be associated with altered solid tumor proliferation and invasiveness in a murine model (29, 30). Furthermore, we posited that sustained sleep fragmentation would promote changes in the phenotypic distribution of tumor-associated macrophages (TAM). Indeed, TAMs have been identified as critically important constituents of cancer microenvironment, and are major contributors to cancer progression by releasing a vast array of growth factors, cytokines, inflammatory mediators, and proteolytic enzymes that underlie key components of tumor growth and invasion (31, 32).","In summary, the present study conclusively demonstrates that perturbed sleep leads to major changes in tumorigenesis, characterized by increased tumor cell proliferation and invasion. Alterations in TAM phenotypes, particularly in the tumor periphery, and in TLR4 expression in TAM further suggest that sleep fragmentation–induced effects on tumor growth and invasion may be mediated by host-related responses, particularly those involving innate immunity, and that improved understanding of such pathways may permit improved therapeutic interventions. Considering the high prevalence of sleep disorders and cancer in middle age or older populations, there are far reaching implications to current findings about potential adverse outcomes in patients in whom the two conditions coexist.",https://twitter.com/foundmyfitness/status/1506299697220124674,
Dietary Manipulation of Oncogenic MicroRNA Expression in Human Rectal Mucosa: A Randomized Trial,300 g of lean red meat per day for 4 weeks increased levels of a biomarker that is linked to cancer development & progression but consuming 40 g of resistant starch along with red meat completely negated this increase.,"High red meat (HRM) intake is associated with increased colorectal cancer risk, while resistant starch is probably protective. Resistant starch fermentation produces butyrate, which can alter microRNA (miRNA) levels in colorectal cancer cells in vitro; effects of red meat and resistant starch on miRNA expression in vivo were unknown. This study examined whether a HRM diet altered miRNA expression in rectal mucosa tissue of healthy volunteers, and if supplementation with butyrylated resistant starch (HRM+HAMSB) modified this response. In a randomized cross-over design, 23 volunteers undertook four 4-week dietary interventions; an HRM diet (300 g/day lean red meat) and an HRM+HAMSB diet (HRM with 40 g/day butyrylated high amylose maize starch), preceded by an entry diet and separated by a washout. Fecal butyrate increased with the HRM+HAMSB diet. Levels of oncogenic mature miRNAs, including miR17–92 cluster miRNAs and miR21, increased in the rectal mucosa with the HRM diet, whereas the HRM+HAMSB diet restored miR17–92 miRNAs, but not miR21, to baseline levels. Elevated miR17–92 and miR21 in the HRM diet corresponded with increased cell proliferation, and a decrease in miR17–92 target gene transcript levels, including CDKN1A. The oncogenic miR17–92 cluster is differentially regulated by dietary factors that increase or decrease risk for colorectal cancer, and this may explain, at least in part, the respective risk profiles of HRM and resistant starch. These findings support increased resistant starch consumption as a means of reducing risk associated with an HRM diet.","The majority of colorectal cancers occur sporadically, with development influenced by environmental and lifestyle factors, including diet (1). Systematic reviews of cohort and case–control studies have found high red meat (HRM) or processed meat intake to be a convincing risk factor (1, 2), with intake of more than 500 g of cooked red meat per week significantly increasing colorectal cancer risk (1). Plausible mechanisms include inducing DNA strand breaks and enhancing promutagenic DNA adduct formation (3, 4). HRM consumption has also been linked to gut microbiome changes and inflammation (5, 6). In contrast, dietary fiber probably protects against colorectal cancer, with systematic review evidence identifying a dose–response relationship, and 10% decreased risk per 10 g fiber intake per day (1). Interventional studies provide less conclusive evidence, and longer-term trials and higher fiber levels may be needed to reproduce effects from observational studies (7). One protective mechanism for fiber is the production of fermentation products, particularly the short-chain fatty acid (SCFA) butyrate (1). Butyrate is a histone deacetylase inhibitor, with antitumorigenic effects (8–12). Aberrant microRNA (miRNA) expression contributes to colorectal cancer development (13–15), with miRNAs such as miR21 and the miR17–92 cluster of miRNAs often increased in colorectal cancers and possessing oncogenic properties (16, 17). We have shown that butyrate can modulate miRNA expression in colorectal cancer cells in vitro (18). The miR17–92 cluster, comprising miR17, miR18a, miR19a, miR20a, miR19b, and miR92a, was significantly decreased with butyrate. This decrease may be partially responsible for the antiproliferative effects of butyrate, with addition of miR17–92 mimics reversing this and increasing proliferation; miR19a and miR19b in particular were key promoters of proliferation (18). Through epigenetic mechanisms, butyrate may be able to reverse the miRNA dysregulation observed in colorectal cancer (18). Higher colonic butyrate levels can be achieved with resistant starch supplementation compared with other fiber sources (19). Resistant starch can also be acetylated with butyrate; butyrylated high amylose maize starch (HAMSB) can deliver esterified butyrate to the human colon, leading to increased fecal butyrate compared with standard high amylose maize starch (P < 0.0001; refs. 20 and 21). In rodents, resistant starch supplementation to an HRM diet increased colonic butyrate, altered gut microbiota, decreased inflammation, and attenuated red meat-induced DNA damage (3, 5, 22). HAMSB was more effective than standard amylose maize starch in lowering genetic damage (23). One human trial has suggested inconclusively that fiber may modify DNA adduct formation in the context of HRM consumption (4); however, to date no other human trials have examined the combined effects of red meat and resistant starch. There has been no previous examination in vivo of the effects of these substances on miRNA expression in colorectal cells. This human trial aimed to determine if consumption of a diet high in lean red meat altered miRNA expression in rectal mucosa tissue, and if supplementation with resistant starch could protect against this dysregulation by increasing butyrate levels in the colorectum. In a randomized cross-over design, markers of colorectal cancer risk were measured in healthy human volunteers who undertook 2 4-week intervention diets, an HRM diet, and an HRM diet supplemented with butyrylated resistant starch (HRM+HAMSB; StarPlus, National Starch and Food Innovation, Bridgewater, NJ). It was hypothesized that regulation of miRNA expression may partially explain some of the chemo-protective effects of resistant starch and the increased colorectal cancer risk associated with HRM intake.","This study presents the first evidence in humans that HRM and butyrylated resistant starch have opposing effects on miRNA levels in rectal mucosa. Several studies have examined the effect of dietary components in other in vivo models. Examination of the miRNA response in rats fed diets containing corn or fish oil with pectin or cellulose and injected with a carcinogen or saline control particularly demonstrated a novel role for fish oil in protecting the colon from carcinogen-induced miRNA dysregulation, rather than a role for fiber (46, 47). Shah and colleagues (47) did however demonstrate that various dietary combinations and carcinogen exposure modulated a number of miRNAs, including miR17–92 cluster miRNAs and miR21 (47). The oncogenic miR17–92 cluster was shown to be differentially regulated by dietary factors that increase or decrease colorectal cancer risk, and this may explain, at least in part, the respective risk profiles of HRM and resistant starch. Although the HRM diet increased miR17–92 cluster miRNA levels in rectal mucosa, with downstream consequences, addition of butyrylated resistant starch to the HRM diet restored miR17–92 levels to baseline. Although the red meat intake during the trial may exceed levels consumed by many in the general population, red meat intake in developed countries is substantial. Total meat consumption in the United States, European Union (EU), and the developed world has continued to increase from 1961 to 2003; nearly doubling in the EU and increasing 1.5-fold in the United States (48). In the United States, per capita total loss-adjusted meat consumption in 2004 was 154 g per day (48). The quantity of resistant starch used in the trial could be realistically applied to the general population. Long-term resistant starch supplementation in select populations has been shown to be feasible (49), and there has been a recent expansion in commercially available foods with increased resistant starch content (50). The findings in this study support increased resistant starch consumption as a means of reducing risk associated with an HRM diet.",https://twitter.com/foundmyfitness/status/1505992023647481861,
β-Hydroxybutyrate Prevents Vascular Senescence through hnRNP A1-Mediated Upregulation of Oct4,"Beta-hydroxybutyrate produced from fasting or a ketogenic diet may reduce cellular senescence, a hallmark of aging. Reduced senescence was found in vascular cells in animals due to increased activation of Oct4, a regulator of stem cell differentiation.","β-hydroxybutyrate (β-HB) elevation during fasting or caloric restriction is believed to induce anti-aging effects and alleviate aging-related neurodegeneration. However, whether β-HB alters the senescence pathway in vascular cells remains unknown. Here we report that β-HB promotes vascular cell quiescence, which significantly inhibits both stress-induced premature senescence and replicative senescence through p53-independent mechanisms. Further, we identify heterogeneous nuclear ribonucleoprotein A1 (hnRNP A1) as a direct binding target of β-HB. β-HB binding to hnRNP A1 markedly enhances hnRNP A1 binding with Octamer-binding transcriptional factor (Oct) 4 mRNA, which stabilizes Oct4 mRNA and Oct4 expression. Oct4 increases Lamin B1, a key factor against DNA damage-induced senescence. Finally, fasting and intraperitoneal injection of β-HB upregulate Oct4 and Lamin B1 in both vascular smooth muscle and endothelial cells in mice in vivo. We conclude that β-HB exerts anti-aging effects in vascular cells by upregulating an hnRNP A1-induced Oct4-mediated Lamin B1 pathway.","Vascular aging is considered as a main risk factor for developing cardiovascular diseases (CVDs) (Costantino et al., 2016, North and Sinclair, 2012). Cellular aging, known as senescence, mainly contributes to vascular diseases associated with inflammation and dysfunction of the endothelial and smooth muscle cells (Kovacic et al., 2011, Minamino and Komuro, 2007). Despite years of intense study, therapeutic strategies to mitigate vascular diseases through preventing or removing vascular senescence are still required (Childs et al., 2015). Cellular senescence is an irreversible cell-cycle arrest which restricts the proliferation of abnormal cells. Cellular senescence has multiple physiological functions, including preventing cancer development (Collado and Serrano, 2010) and restricting liver and cardiac fibrosis (Kim et al., 2013). However, the accumulation of senescent cells also contributes to, and can even exacerbate, aging and age-related diseases, such as atherosclerosis (Wang and Bennett, 2012), Alzheimer’s disease (Bredesen et al., 2016), and cancer (Finkel et al., 2007). In contrast, cellular quiescence is a reversible cell-cycle arrest and usually occurs due to lack of nutrition or growth factors. Maintaining a quiescent state is thought to be a mechanism by which cells can avoid initiating senescence programs (Salmenperä et al., 2016). Thus, identification of endogenous mechanisms that regulate cell senescence and quiescence may provide novel insights into age-related diseases. It is established that fasting and caloric restriction extend both the average and the maximal lifespan and prevent age-related diseases through energy sensors dependent on Sirtuin 1 (SIRT1) or Forkhead box O3 (FoxO3) activation (Martin et al., 2006). Additionally, calorie restriction-mediated moderation of cell senescence is reported in recent studies (Anton et al., 2013, Ning et al., 2013). However, it is unknown whether alternative metabolic fuels generated during energy deficit, such as the ketone bodies β-hydroxybutyrate (β-HB) and acetoacetate (AcAc), affect cell senescence and quiescence, nor what the underlying mechanisms may be. Quiescence not only avoids initiation of the senescent program but also contributes to the maintenance of stemness, allowing cells to be resistant to genotoxic stress, a major trigger of cellular senescence (Cai et al., 2004, van Deursen, 2014). Octamer-binding transcriptional factor 4 (Oct4, POU5F1), a regulator of pluripotency in embryonic stem cells, was once thought to be permanently silenced in adult somatic cells (Lengner et al., 2008). Interestingly, a recent study suggests that Oct4 is reactivated in adult vascular smooth cells, as demonstrated by its protective role against atherosclerosis (Cherepanova et al., 2016). However, other functions of Oct4 reactivation remain undefined. In addition, the upstream molecules which regulate Oct4 expression at the posttranscriptional level are also poorly characterized. Recent studies suggest that Oct4 expression can be regulated by heterogeneous nuclear ribonucleoproteins A2/B (hnRNP A2/B), a subfamily of the large A-U hnRNP family (Choi et al., 2013). hnRNP A1 is a member of the hnRNP family that plays a critical role in RNA processing, including alternative mRNA splicing, transcriptional regulation (Jean-Philippe et al., 2013), and stabilization of mRNA under various stress conditions via formation of stress granules (Guil et al., 2006). However, it remains unknown precisely how these proteins interact. Elevation of ketone bodies, such as β-HB, during fasting or caloric restriction is believed to induce anti-aging effects and alleviate aging-related neurodegeneration (Paoli et al., 2014, Tieu et al., 2003). However, whether ketone body alters pathways in the aging process, specifically the senescence signal in vascular cells, remains unknown. Here we report that β-HB-induced cellular quiescence significantly inhibits both stress-induced premature senescence and replicative senescence through its binding with hnRNP A1, which can bind to and stabilize Oct4 mRNA, thus upregulating Oct4 expression at the posttranscriptional levels.","In summary, our studies reveal an important role for β-HB in cellular anti-senescence mediated by Oct4 upregulation in the vasculature system. It provides novel strategy to prevent senescence-associated vascular aging via accumulating or maintaining quiescent vascular cells. Thus, our research identifies Oct4 as a potent therapeutic target and β-HB and S-β-HB as a potent treatment for anti-aging or age-related vascular diseases.",https://twitter.com/foundmyfitness/status/1504149446157750274,
Fish Oil–Derived Fatty Acids in Pregnancy and Wheeze and Asthma in Offspring,Pregnant mothers (with low baseline omega-3 levels) that supplemented with 2.4 grams of fish oil daily during the third trimester decreased the risk of persistent wheeze and asthma during the first 5 years of a child’s life by 54%.,"Reduced intake of n−3 long-chain polyunsaturated fatty acids (LCPUFAs) may be a contributing factor to the increasing prevalence of wheezing disorders. We assessed the effect of supplementation with n−3 LCPUFAs in pregnant women on the risk of persistent wheeze and asthma in their offspring. We randomly assigned 736 pregnant women at 24 weeks of gestation to receive 2.4 g of n−3 LCPUFA (fish oil) or placebo (olive oil) per day. Their children formed the Copenhagen Prospective Studies on Asthma in Childhood2010 (COPSAC2010) cohort and were followed prospectively with extensive clinical phenotyping. Neither the investigators nor the participants were aware of group assignments during follow-up for the first 3 years of the children’s lives, after which there was a 2-year follow-up period during which only the investigators were unaware of group assignments. The primary end point was persistent wheeze or asthma, and the secondary end points included lower respiratory tract infections, asthma exacerbations, eczema, and allergic sensitization. A total of 695 children were included in the trial, and 95.5% completed the 3-year, double-blind follow-up period. The risk of persistent wheeze or asthma in the treatment group was 16.9%, versus 23.7% in the control group (hazard ratio, 0.69; 95% confidence interval [CI], 0.49 to 0.97; P=0.035), corresponding to a relative reduction of 30.7%. Prespecified subgroup analyses suggested that the effect was strongest in the children of women whose blood levels of eicosapentaenoic acid and docosahexaenoic acid were in the lowest third of the trial population at randomization: 17.5% versus 34.1% (hazard ratio, 0.46; 95% CI, 0.25 to 0.83; P=0.011). Analyses of secondary end points showed that supplementation with n−3 LCPUFA was associated with a reduced risk of infections of the lower respiratory tract (31.7% vs. 39.1%; hazard ratio, 0.75; 95% CI, 0.58 to 0.98; P=0.033), but there was no statistically significant association between supplementation and asthma exacerbations, eczema, or allergic sensitization. Supplementation with n−3 LCPUFA in the third trimester of pregnancy reduced the absolute risk of persistent wheeze or asthma and infections of the lower respiratory tract in offspring by approximately 7 percentage points, or one third.","The incidence of asthma and wheezing disorders has more than doubled in westernized countries in recent decades.1 These conditions often originate in early childhood2 and currently affect one in five young children.3 Concomitantly, the increased use of vegetable oils in cooking and of grain in the feeding of livestock has resulted in an increase in the intake of n−6 polyunsaturated fatty acids and a decrease in the intake of n−3 polyunsaturated fatty acids, especially the long-chain polyunsaturated fatty acids (LCPUFAs) — eicosapentaenoic acid (20:5n–3, EPA) and docosahexaenoic acid (22:6n–3, DHA) — found in cold-water fish.4 Observational studies have suggested an association between a diet that is deficient in n−3 LCPUFA during pregnancy and an increased risk of asthma and wheezing disorders in offspring,5,6 whereas randomized, controlled trials of n−3 LCPUFA supplementation in pregnant women have generally been underpowered and have produced ambiguous results.7-10 Therefore, we conducted a double-blind, randomized, controlled trial of n−3 LCPUFA supplementation during the third trimester of pregnancy to assess the effect on the risk of persistent wheeze and asthma in offspring. The children were followed prospectively for the first 3 years of life with comprehensive clinical phenotyping,11 during which time both the investigators and the participants were unaware of group assignments. For an additional 2 years of follow-up, only the investigators were unaware of group assignments.","In conclusion, these findings show that n−3 LCPUFA supplementation during pregnancy was associated with a significantly diminished burden of wheezing and asthma in children in this Danish birth cohort. To address the question of whether similar effects could be seen in other populations, further studies are required.",https://twitter.com/foundmyfitness/status/1499458755225800729,
Association of Vitamin D Supplementation in Cardiorespiratory Fitness and Muscle Strength in Adult Twins: A Randomized Controlled Trial,"Daily supplementation with 2,000 IU of vitamin D improved cardiorespiratory fitness by increasing maximum oxygen consumption by 28% and increased muscle strength by 18% after 60 days compared to participants (identical twins) given a placebo supplement.","Although vitamin D is related to cardiorespiratory fitness and muscle strength, there is no evidence in the literature about the genetic influence of the response to vitamin D supplementation and improvements in these parameters. Therefore, we evaluate the effect of longitudinal supplementation of vitamin D on parameters of physical fitness in monozygotic twins. In total, 74 participants were included, with a mean age of 25 years, divided into two groups, one group received supplementation with cholecalciferol for 60 days and the other group did not. Cardiorespiratory fitness and muscle strength were measured before and after supplementation through maximal treadmill tests and dynamometry, respectively. Wilcoxon tests were used to compare intragroup results and the Mann-Whitney test to examine intergroup differences. There was an increase in the serum concentration of vitamin D in participants who ingested the supplementation. Cardiorespiratory fitness improved after supplementation through increases in the values of maximum oxygen consumption of 28% (p < .001). Muscle strength in left hand grip increased 18% in participants who received the supplement (p = .007). Sixty days of cholecalciferol supplementation improved cardiorespiratory fitness and upper limb muscle strength.","Vitamin D is involved in several non-skeletal functions, playing an important role in immunity and in cardiopulmonary and muscle functions [1]. Vitamin D receptors are present in several types of cells, including skeletal muscle, heart muscle, and vascular smooth muscle [2]. Thus, vitamin D insuciency has been shown to be related to muscle weakness, arterial thickening, myocardial hypertrophy, and hypertension [3, 4]. Page 3/17 Aerobic capacity measured through Maximum Oxygen Consumption (VO2max), has been widely used in studies as a good indicator of cardiovascular condition [5, 6]. Low levels of cardiorespiratory tness have been associated with health and quality of life problems, such as lower longevity, lower disposition, higher body fat content, worse lipid prole, and disturbances in glucose metabolism [7]. Thus, better cardiorespiratory tness can reduce the risk of death from cardiovascular and metabolic diseases [8, 9]. There is signicant evidence that vitamin D is an inuential factor for VO2max [10, 11, 12]. However, the majority of studies on this theme are observational, raising the question as to whether the serum concentration of 25-hydroxyvitamin D (25 (OH) D) is a cause of increased VO2max or simply a result with little signicance [13]. This increases the importance of clinical trials that help elucidate the relationship between vitamin D and VO2max [14]. It is also known that vitamin D affects muscle strength, muscle size, and neuromuscular performance [15, 16] and the reduction in muscle strength and power also impairs the quality of life of individuals, as these factors inuence the functional capacity to perform basic activities of daily life, generating limitations for their execution [17]. Both VO2max and muscle strength are inuenced by genetic factors [18] and the only human study model that enables control of the genetic factor is the study of monozygotic twins (MZ). MZ twins develop from a single egg cell, giving rise to two individuals who share the same genetic load and, because they have the same genotype, the differences between them occur mainly due to the external inuences of the environment where they are inserted; the greater the differences in these inuences, the greater the probabilities of different phenotypic observations. Thus, the realization of a study with twins enables analysis of the inuence on the phenotype through the isolation of the interference of the genetic factor [25], in the case of the current study, the divergent external inuence will be vitamin D supplementation. Thus, clinical trials with MZ allow control of one of these factors, making them extremely relevant [19]. Therefore, knowing the relationship of vitamin D with cardiorespiratory tness and muscle strength, we aimed to establish whether low doses of this nutrient would bring improvements to the health of individuals. Therefore, the aim of this study is to evaluate the effect of cholecalciferol supplementation on cardiorespiratory tness and muscle strength in healthy adult MZ twins.","Increasing the serum concentration of vitamin D to values above 50 generates health benets, improving cardiorespiratory tness and slightly increasing muscle strength of the non-dominant hand. Our ndings contribute to the growing evidence regarding the non-skeletal benets of vitamin D.",https://twitter.com/foundmyfitness/status/1498381154419892231,
"Does Strength-Promoting Exercise Confer Unique Health Benefits? A Pooled Analysis of Data on 11 Population Cohorts With All-Cause, Cancer, and Cardiovascular Mortality Endpoints",Participation in any strength-promoting exercise was associated with a 23% reduction in all-cause mortality and a 31% reduction in cancer mortality. Own bodyweight exercises without any equipment also yielded comparable results to gym-based activities.,"Public health guidance includes recommendations to engage in strength-promoting exercise (SPE), but there is little evidence on its links with mortality. Using data from the Health Survey for England and the Scottish Health Survey from 1994–2008, we examined the associations between SPE (gym-based and own-body-weight strength activities) and all-cause, cancer, and cardiovascular disease mortality. Multivariable-adjusted Cox regression was used to examine the associations between SPE (any, low-/high-volume, and adherence to the SPE guideline (≥2 sessions/week)) and mortality. The core sample comprised 80,306 adults aged ≥30 years, corresponding to 5,763 any-cause deaths (736,463 person-years). Following exclusions for prevalent disease/events occurring in the first 24 months, participation in any SPE was favorably associated with all-cause (hazard ratio (HR) = 0.77, 95% confidence interval (CI): 0.69, 0.87) and cancer (HR = 0.69, 95% CI: 0.56, 0.86) mortality. Adhering only to the SPE guideline was associated with all-cause (HR = 0.79, 95% CI: 0.66, 0.94) and cancer (HR = 0.66, 95% CI: 0.48, 0.92) mortality; adhering only to the aerobic activity guideline (equivalent to 150 minutes/week of moderate-intensity activity) was associated with all-cause (HR = 0.84, 95% CI: 0.78, 0.90) and cardiovascular disease (HR = 0.78, 95% CI: 0.68, 0.90) mortality. Adherence to both guidelines was associated with all-cause (HR = 0.71, 95% CI: 0.57, 0.87) and cancer (HR = 0.70, 95% CI: 0.50, 0.98) mortality. Our results support promoting adherence to the strength exercise guidelines over and above the generic physical activity targets.","There is a well-established association between participation in regular physical activity and reductions in all-cause, cardiovascular disease (CVD), diabetes, and cancer-related mortality (1, 2). In the last decade, strength-promoting exercise (SPE) has become an integral component of physical activity guidelines around the world (3, 4), with the World Health Organization recommending at least 2 sessions per week. Current SPE guidelines are primarily intended to increase strength and function, and there are few data on associations with chronic disease and mortality. Participation in strengthening exercise has been associated with reduced risk of type 2 diabetes in men (ages 40–75 years) (5), women (ages 36–81 years) (6), and working-age populations (ages 30–64 years) (7). These associations were independent of aerobic exercise, conferred greater benefit when combined with aerobic exercise (5, 6), and were more pronounced in older adults (7). Compared with aerobic forms of physical activity, SPE is unique in its ability to promote increases in muscle size and strength, with higher muscle mass (8, 9) and strength (10) being found to be associated with a lower mortality risk. Thus, SPE may be promising for reducing premature mortality and chronic disease risk. However, few studies have explored associations between SPE and cause-specific mortality. SPE has been shown to be associated with reduced risk of fatal and nonfatal myocardial infarction among adult men (11) and reduced risk of all-cause mortality in cancer survivors (12), and recent studies have also shown reductions in all-cause mortality among adults who meet the guidelines of 2 sessions per week (13–15). However, limited conclusions can be drawn, and the few studies that have been published have usually been limited to older adults residing in the United States (13) and small cohorts (12, 14, 15), with no measures being taken to account for reverse causality by removing prevalent cases (13–15) or by excluding events taking place during the first few months or years of follow-up (11, 13–15). Our aim in this study was to examine the associations between SPE and all-cause, CVD, and cancer mortality and to compare the SPE and aerobic activity guidelines in terms of their associations with mortality outcomes.","In conclusion, participation in any SPE was associated with a 23% reduction in all-cause mortality and a 31% reduction in cancer mortality. In terms of mortality risk reduction, adherence to the SPE guideline on physical activity appears to be at least as important as adherence to the aerobic guideline. Our results support the value of specifically promoting adherence to the strength exercise guideline over and above the generic physical activity targets.",https://twitter.com/foundmyfitness/status/1496195899751608321,
Efficacy and safety of psilocybin-assisted treatment for major depressive disorder: Prospective 12-month follow-up,Psilocybin may have long-lasting antidepressant effects. A new study found two doses of psilocybin treatment administered two weeks apart produced large decreases in depression severity that remained low up to 1-year after treatment.,"Preliminary data suggest that psilocybin-assisted treatment produces substantial and rapid antidepressant effects in patients with major depressive disorder (MDD), but little is known about long-term outcomes.This study sought to examine the efficacy and safety of psilocybin through 12 months in participants with moderate to severe MDD who received psilocybin. This randomized, waiting-list controlled study enrolled 27 patients aged 21–75 with moderate to severe unipolar depression (GRID-Hamilton Depression Rating Scale (GRID-HAMD) ⩾ 17). Participants were randomized to an immediate or delayed (8 weeks) treatment condition in which they received two doses of psilocybin with supportive psychotherapy. Twenty-four participants completed both psilocybin sessions and were followed through 12 months following their second dose. All 24 participants attended all follow-up visits through the 12-month timepoint. Large decreases from baseline in GRID-HAMD scores were observed at 1-, 3-, 6-, and 12-month follow-up (Cohen d = 2.3, 2.0, 2.6, and 2.4, respectively). Treatment response (⩾50% reduction in GRID-HAMD score from baseline) and remission were 75% and 58%, respectively, at 12 months. There were no serious adverse events judged to be related to psilocybin in the long-term follow-up period, and no participants reported psilocybin use outside of the context of the study. Participant ratings of personal meaning, spiritual experience, and mystical experience after sessions predicted increased well-being at 12 months, but did not predict improvement in depression. These findings demonstrate that the substantial antidepressant effects of psilocybin-assisted therapy may be durable at least through 12 months following acute intervention in some patients.","Major depressive disorder (MDD) affects over 260 million people worldwide and is a leading cause of disability and healthcare expenditures (James et al., 2018). First-line treatments, including pharmacotherapy and psychotherapy, may take weeks or months to produce clinically meaningful symptom reduction, and patients can have difficulty with treatment adherence (Cuijpers et al., 2008; Kolovos et al., 2017; Lam, 2012). At least 30% of patients ultimately meet criteria for treatment-resistant depressive illness after failing to respond to multiple attempts at treatment (Nemeroff, 2007). MDD also has a highly recurrent course, with 40–60% of those diagnosed with a single episode eventually relapsing, and rate of relapse increasing with each subsequent episode (Richards, 2011; Solomon et al., 2000). Novel interventions are needed that can act rapidly and produce sustained remission. Several preliminary studies suggest that psilocybin-assisted treatment may have substantial antidepressant effects in patients with MDD, with treatment response occurring within a week of administration of just one or two doses in the context of psychotherapy (Carhart-Harris et al., 2018, 2021; Davis et al., 2021). In an initial report of primary outcomes following two doses of psilocybin using a randomized waitlist-control study design, we reported a large effect size (Cohen d = 2.3) and high rates of treatment response and remission (71% and 54%) at 1 month following intervention (Davis et al., 2021). Treatment-resistant patients also appear to have a favorable response rate (Carhart-Harris et al., 2018). A more recent study used a double-blind double dummy design to compare high-dose psilocybin plus 6 weeks of placebo with very low dose psilocybin plus 6 weeks of escitalopram (Carhart-Harris et al., 2021). The authors failed to show a significant difference between the two groups at 6 weeks in their designated primary outcome measure (Quick Inventory of Depressive Symptoms). The majority of results for secondary outcome measures including other depression severity scores favored the high-dose psilocybin group, though analyses were not corrected for multiple comparisons. Although psilocybin treatment of MDD appears promising, little is known about long-term efficacy and safety. The three studies conducted to date demonstrated efficacy at their longest follow-up assessments of 4 weeks (Davis et al., 2021), 6 weeks (Carhart-Harris et al., 2021), and 6 months (Carhart-Harris et al., 2018), although depression severity scores were trending upward at 3- and 6-month follow-up timepoints. Carhart-Harris et al. (2018), with the longest period of follow-up, had an open-label design. Given the chronicity and relapsing disease course of MDD (Richards, 2011), the present study represents a significant extension of these previous findings by assessing efficacy and safety of a psilocybin intervention throughout a 12-month follow-up period.","The present study provides new information about qualitative features of the acute psilocybin experience that predict subsequent enduring effects. Patient ratings of personal meaning, spiritual significance, and MEQ30 scores after psilocybin sessions significantly correlated with a measure of overall well-being at most follow-up timepoints. However, except for ratings of personal meaning and spiritual significance at the first long-term follow-up assessment at 4 weeks, none of patient ratings of psilocybin experience at the time of the session were predictive of improvements in depression. Notably, two previous studies in individuals with cancer-related depression and anxiety (Griffiths et al., 2016; Ross et al., 2016) showed positive associations of MEQ30 session experiences with improvements in depression symptoms at 5 or 6 weeks. Considering that the direction of correlation at 4 weeks was in the predicted direction (rs = 0.38, p = .066), it is possible that the present study was underpowered to detect such an effect with MEQ30 or other measures of psilocybin experience. Alternatively, this difference may reflect a lack of such relationship in a sample of individuals with MDD as opposed to depressive symptoms secondary to a cancer diagnosis. There were no serious adverse events, depression symptoms were not significantly exacerbated in any participant, and there was no reported use of psilocybin or other psychedelic drugs during the follow-up period. This latter observation contrasts with a previous study in which 5 of 19 participants reported use of psilocybin outside of the research setting by the end of the 6-month follow-up period (Carhart-Harris et al., 2018). Reasons for this difference are unknown, but the observation indicates the importance of assessing use of psychedelics outside of a clinical trial. Although the safety results presented herein are favorable, larger phase 3 and 4 studies will be needed to more fully assess safety.",https://twitter.com/foundmyfitness/status/1494425399874383897,"No Conclusion in paper, last two paragraphs of Discussion section used as conclusion"
"Divergent effects of resistance and endurance exercise on plasma bile acids, FGF19, and FGF21 in humans",Aerobic exercise on a bike caused an increase in the hormone FGF21 3x higher than strength training with weights. FGF21 is a hormone that regulates addictive behavior. It may be a biological explanation why cardio exercise can treat addiction.,"Exercise has profound pleiotropic health benefits, yet the underlying mechanisms remain incompletely understood. Endocrine FGF21, bile acids (BAs), and BA-induced FGF19 have emerged as metabolic signaling molecules. Here, we investigated if dissimilar modes of exercise, resistance exercise (RE) and endurance exercise (EE), regulate plasma BAs, FGF19, and FGF21 in humans. Ten healthy, moderately trained males were enrolled in a randomized crossover study of 1 hour of bicycling at 70% of VO2peak (EE) and 1 hour of high-volume RE. Hormones and metabolites were measured in venous blood and sampled before and after exercise and at 15, 30, 60, 90, 120, and 180 minutes after exercise. We observed exercise mode–specific changes in plasma concentrations of FGF19 and FGF21. Whereas FGF19 decreased following RE (P < 0.001), FGF21 increased in response to EE (P < 0.001). Total plasma BAs decreased exclusively following RE (P < 0.05), but the composition of BAs changed in response to both types of exercise. Notably, circulating levels of the potent TGR5 receptor agonist, lithocholic acid, increased with both types of exercise (P < 0.001). This study reveals divergent effects of EE and RE on circulating concentrations of the BA species, FGF19, and FGF21. We identify temporal relationships between decreased BA and FGF19 following RE and a sharp disparity in FGF21 concentrations, with EE eliciting a clear increase parallel to that of glucagon.","Exercise has pleiotropic health benefits, and understanding the mechanisms by which exercise regulates human biology might advance better exercise protocols for disease prevention and could give rise to novel exercise-mimicking pharmacotherapies (1). Endocrine signals are upstream of a wide range of exercise-governed molecular and physiological adaptations and are profoundly determined by the mode of exercise. Our understanding, however, of how exercise-induced secreted factors promote long-term systemic health benefits is incomplete. The bile acid (BA)/FGF19 axis is an endocrine feedback system of the enterohepatic circulation and crucial to the absorption of dietary lipids (2). Over the past decade the BA/FGF19 axis has emerged as a relevant endocrine axis in energy metabolism. Serum BAs contribute to the metabolic benefits of bariatric surgery and through activation of the membrane receptor TGR5, BAs increase energy expenditure and improve lipid and glucose metabolism (3–5). FGF19 is an endocrine FGF that is expressed upon BA-induced farnesoid X receptor (FXR) activation in the small intestine (6, 7). In addition to suppressing hepatic BA synthesis, FGF19 also influences energy balance and glucose metabolism via FGF receptors in metabolic target tissues, including liver, adipose tissue, and the brain, and is an important endocrine signal for metabolic health (8). Recently, pharmacological FGF19 administration was reported to induce skeletal muscle hypertrophy in mice, indicating a therapeutic potential for FGF19 in preventing atrophy-related complications, such as sarcopenia and cachexia (9). This feature is unique to FGF19, as pharmacological administration of FGF21, another member of the endocrine FGF family, has no direct effect on muscle size or strength in mice (9). Conversely, plasma FGF21 is reported to be increased with endurance exercise (EE) and may thus contribute to the benefits of this type of exercise on glucose and lipid metabolism (10–14). However, the temporal resolution of circulating FGF21 in response to EE, as well as the effect of resistance exercise (RE) on FGF21, has not yet been determined. Moreover, it remains to be investigated if the BA/FGF19 axis is regulated by exercise. In the present work, a controlled randomized crossover study (Figure 1) was used to determine if exercise regulates circulating BA species, FGF19, and FGF21 in young healthy human male subjects and to unravel possible exercise mode–specific alterations. Because of the interconnectedness among BA, the endocrine FGFs, and canonical substrate-regulating hormones, we also profiled circulating insulin and glucagon in response to EE and RE.","In conclusion, we report distinct effects of EE and RE on circulating concentrations of BA species, FGF19, and FGF21 in young healthy humans. For the first time to our knowledge, we identify temporal relationships between decreased BA and FGF19 following resistance training and a sharp disparity in FGF21 concentrations with EE eliciting a clear increase parallel to that of glucagon.",https://twitter.com/foundmyfitness/status/1494066317497876480,
Various Factors May Modulate the Effect of Exercise on Testosterone Levels in Men,Reaching a threshold of intensity of 90% of VO2 max may be important for achieving substantial increases in testosterone from physical activity. Young men experienced a 31% increase in serum testosterone after 40 min of maximum intensity training.,"Exercise has been proposed to increase serum testosterone concentrations. The analysis of existing literature demonstrates a large degree of variability in hormonal changes during exercise. In our manuscript, we summarized and reviewed the literature, and concluded that this variability can be explained by the effect of numerous factors, such as (a) the use of different types of exercise (e.g., endurance vs. resistance); (b) training intensity and/or duration of resting periods; (c) study populations (e.g., young vs. elderly; lean vs. obese; sedentary vs. athletes); and (d) the time point when serum testosterone was measured (e.g., during or immediately after vs. several minutes or hours after the exercise). Although exercise increases plasma testosterone concentrations, this effect depends on many factors, including the aforementioned ones. Future studies should focus on clarifying the metabolic and molecular mechanisms whereby exercise may affect serum testosterone concentrations in the short and long-terms, and furthermore, how this affects downstream mechanisms","Testosterone is the most potent naturally secreted steroid androgenic hormone. It is required for promotion of secondary male-sex characteristics, as well as muscle growth and neuromuscular adaptation. At the muscle level, testosterone is known to exert its anabolic effect via the following two mechanisms: (a) stimulating amino acid uptake and protein synthesis; and (b) inhibiting protein degradation by counteracting cortisol signaling. Age, higher body weight, poor nutritional status, stress, sleep deprivation, and alcohol consumption are known physiological factors leading to lower serum testosterone concentrations. Low plasma testosterone concentration is associated with fatigue, sexual dysfunction, depressed mood, difficulty concentrating, and hot flashes [4]. If left untreated, patients may develop anemia, low bone mass density (i.e., osteoporosis), higher pro-atherogenic lipoprotein-associated changes, and muscle wasting . Thus, maintaining physiological levels of testosterone has significant health benefits. Exercise has significant health-related benefits and it is proposed to increase plasma testosterone concentrations . However, analysis of the existing literature demonstrates a large degree of inter-individual and inter-study variability in hormonal changes during exercise. Age, body weight, and exercise type, together with exercise intensity, volume, and the involved muscle type, were studied as factors modulating these hormonal changes. This review article intends to clarify the factors that contribute to the variability in serum testosterone concentrations during exercise, and the underlying mechanisms. Part 1 will focus on the acute or immediate post-exercise changes in plasma testosterone concentrations, and Part 2 will discuss the changes in basal or resting plasma testosterone concentrations after completion of exercise protocols. An online search through the Pubmed and Medline databases was initially performed using the combination of the following keywords: “testosterone”, “exercise”, and “men”. Additional exercise description such as “type” and “intensity” were added. “Obesity” and “age” as key words were added during advanced search for population-focused data. The exclusion criteria included: publications written in languages other than English, publications involving subjects with chronic medical conditions, such as congestive heart failure and diabetes, and publications involving subjects on testosterone replacement.","In conclusion, the up-to-date data on the effect of exercise on serum testosterone concentrations in men have significant inter-individual and inter-study variability. This variability can be explained by (a) the use of different types of exercise (e.g., endurance vs. resistance); (b) the other factors of the training (e.g., training intensity or duration of resting periods); (c) the variety in study populations (e.g., young vs. elderly; lean vs. obese; sedentary vs. athletes); and (d) the time points when testosterone was measured (e.g., during or immediately after vs. several minutes or hours after the exercise). It is our conclusion that future studies should focus on clarifying the metabolic and molecular mechanisms whereby exercise may affect testosterone production in the short- and long-term, and furthermore how this release affects downstream mechanisms; such knowledge will be the key to understanding the exercise-testosterone-muscle hypertrophy axis.",https://twitter.com/foundmyfitness/status/1493322857895501824,
Six-Minute Walking Test Performance Relates to Neurocognitive Abilities in Preschoolers,4 to 6-year-old children who have higher cardiorespiratory fitness (CRF) also do better on cognitive tests and other measures of brain function suggesting the link between CRF and brain health is evident even earlier in life than previously appreciated.,"This study investigated the relationship between six-minute walking test (6MWT) distance walked and preschool-aged children’s academic abilities, and behavioral and event-related potentials (ERP) indices of cognitive control. There were 59 children (25 females; age: 5.0 ± 0.6 years) who completed a 6MWT (mean distance: 449.6 ± 82.0 m) to estimate cardiorespiratory fitness. The Woodcock Johnson Early Cognitive and Academic Development Test evaluated academic abilities. A modified Eriksen flanker, hearts and flowers task, and auditory oddball task eliciting ERPs (N2, P3) assessed cognitive control. After adjusting for adiposity, diet, and demographics, linear regressions resulted in positive relationships between 6MWT distance and General Intellectual Ability (β = 0.25, Adj R2 = 0.04, p = 0.04) and Expressive Language (β = 0.30, Adj R2 = 0.13, p = 0.02). 6MWT distance was positively correlated with congruent accuracy (β = 0.29, Adj R2 = 0.18, p < 0.01) and negatively with incongruent reaction time (β = −0.26, Adj R2 = 0.05, p = 0.04) during the flanker task, and positively with homogeneous (β = 0.23, Adj R2 = 0.21, p = 0.04) and heterogeneous (β = 0.26, Adj R2 = 0.40, p = 0.02) accuracy on the hearts and flowers task. Higher fit children showed faster N2 latencies and greater P3 amplitudes to target stimuli; however, these were at the trend level following the adjustment of covariates. These findings indicate that the positive influence of cardiorespiratory fitness on cognitive function is evident in 4–6-year-olds. ","A promising avenue for such examinations is the use of ERPs. While ERP work is becoming more plentiful in preadolescent samples, it is still rare in preschool-aged children. In samples of children aged 8–11 years old, the N2 and P3b components of the ERP waveform, thought to index response inhibition and the allocation of attention resources, respectively, have been studied in inhibitory control paradigms. These studies have shown that higher-fit children exhibit smaller (i.e., less negative) N2 components and larger P3 components compared to their lower-fit counterparts. Similar results have been shown in examinations of selective attention, with higher-fit children showing increased P3b amplitudes on a visual oddball task. Furthermore, past research examining cardiorespiratory fitness effects on cognitive control among preschool-aged children has not accounted for children’s degree of excess fat mass or adiposity. Lack of accounting of adiposity is problematic given that excess adiposity negatively impacts cardiorespiratory fitness and has been shown to exert independent and detrimental effects on cognitive control . Specifically, children with overweight/obesity completing a six-minute walking test, used to estimate their cardiorespiratory fitness, have exhibited shorter distance, lower heart rate, higher systolic blood pressure, and lower oxygen saturation . Pertinent to pediatric populations, excess visceral adipose tissue (VAT), an adipose depot site well recognized for its detrimental metabolic implications, has been shown to predict poorer cognitive function in preadolescent children as well as adults . Furthermore, dietary intake may also influence cognitive function due to nutritional implications for brain development , neuroinflammation, and the provision of energy. Previous work has shown that school-aged children (7–9-year-olds) with greater adherence to the recommended Dietary Guidelines of Americans, as assessed by the Healthy Eating Index-2005 (HEI-2005), exhibited greater cognitive control, even after adjusting for cardiorespiratory fitness and adiposity . This was consistent with a more extensive study involving over 5000 school-aged children that related dietary quality to greater academic performance. However, to our knowledge, none of the previous studies have accounted for the confounding influence of dietary intake and adiposity when examining the relationship between estimated cardiorespiratory fitness, early academic achievement, and cognitive abilities among school-aged children. Therefore, additional research examining the impact of estimated cardiorespiratory fitness—while accounting for demographic, adiposity, and dietary factors—is necessary to characterize the independent influence of estimated cardiorespiratory fitness on academic skills and cognitive abilities among children of preschool age. Accordingly, the present work aimed to investigate the relationship between estimated cardiorespiratory fitness, assessed using a six-minute walking test (6MWT) of submaximal exercise capacity, and children’s early academic skills and cognitive abilities, following adjustment of pertinent covariates such as demographic factors, adiposity, and diet quality. Furthermore, using a subsample, we examined the relationship between physical fitness and a brain-based measure of attention in preschool-aged children via an auditory oddball task completed while participants were wearing an electroencephalogram (EEG) cap. We hypothesized that, consistent with previous work in older school-aged children, greater 6MWT distance would be positively and independently associated with children’s academic and cognitive abilities. Furthermore, we anticipated that children with higher fitness would show larger modulations of the P3 and N2 waveforms as indicated by difference waves across task conditions.","While several studies exist examining the relationship between cardiorespiratory fitness and neurocognitive abilities and academic achievement, much of this work has focused on school-aged children. The work presented here provides important evidence demonstrating that greater 6MWT distance is related to a wide range of cognitive function measures including academic achievement, intellectual abilities, cognitive flexibility, and attentional inhibition in children between 4–6 years. We also have provided some evidence that 6MWT distance has a tendency towards a relationship with neural signatures underlying selective attention, although more EEG work is needed to develop a more definitive understanding of this potential relationship. Given the prevalence of physical inactivity and poor fitness in youth across the globe, future experimental studies are needed to determine whether the benefits of physical activity interventions, evident in some studies in older children, are also evident in preschool-aged children.",https://twitter.com/foundmyfitness/status/1489349579053178894,
Effect of Urolithin A Supplementation on Muscle Endurance and Mitochondrial Health in Older Adults,"Supplementation with urolithin A improved endurance and improved mitochondrial function in older adults compared to placebo. Urolithin A is a byproduct formed from gut bacteria after metabolizing polyphenols found in pomegranates, berries, and nuts.","Aging is associated with a decline in mitochondrial function and reduced exercise capacity. Urolithin A is a natural gut microbiome–derived food metabolite that has been shown to stimulate mitophagy and improve muscle function in older animals and to induce mitochondrial gene expression in older humans.To investigate whether oral administration of urolithin A improved the 6-minute walk distance, muscle endurance in hand and leg muscles, and biomarkers associated with mitochondrial and cellular health.This double-blind, placebo-controlled randomized clinical trial in adults aged 65 to 90 years was conducted at a medical center and a cancer research center in Seattle, Washington, from March 1, 2018, to July 30, 2020. Muscle fatigue tests and plasma analysis of biomarkers were assessed at baseline, 2 months, and 4 months. Six-minute walk distance and maximal ATP production were assessed using magnetic resonance spectroscopy at baseline and at the end of study at 4 months. The analysis used an intention-to-treat approach. Participants were randomized to receive daily oral supplementation with either 1000 mg urolithin A or placebo for 4 months.The primary end point was change from baseline in the 6-minute walk distance and change from baseline to 4 months in maximal ATP production in the hand skeletal muscle. The secondary end points were change in muscle endurance of 2 skeletal muscles (tibialis anterior [TA] in the leg and first dorsal interosseus [FDI] in the hand). Cellular health biomarkers were investigated via plasma metabolomics. Adverse events were recorded and compared between the 2 groups during the intervention period.A total of 66 participants were randomized to either the urolithin A (n = 33) or the placebo (n = 33) intervention group. These participants had a mean (SD) age of 71.7 (4.94) years, were predominantly women (50 [75.8%]), and were all White individuals. Urolithin A, compared with placebo, significantly improved muscle endurance (ie, increase in the number of muscle contractions until fatigue from baseline) in the FDI and TA at 2 months (urolithin A: FDI, 95.3 [115.5] and TA, 41.4 ; placebo: FDI, 11.6 and TA, 5.7 ). Plasma levels of several acylcarnitines, ceramides, and C-reactive protein were decreased by urolithin A, compared with placebo, at 4 months (baseline vs 4 mo: urolithin A, 2.14 vs 2.07 ; placebo, 2.17 vs 2.65). The mean (SD) increase from baseline in the 6-minute walk distance was 60.8 (67.2) m in the urolithin A group and 42.5 (73.3) m in the placebo group. The mean (SD) change from baseline to 4 months in maximal ATP production in the FDI was 0.07 (0.23) mM/s in the urolithin A group and 0.06 (0.20) mM/s in the placebo group; for the TA, it was −0.03 (0.10) mM/s in the urolithin A group and 0.03 (0.10) mM/s in the placebo group. These results showed no significant improvement with urolithin A supplementation compared with placebo. No statistical differences in adverse events were observed between the 2 groups. This randomized clinical trial found that urolithin A supplementation was safe and well tolerated in the assessed population. Although the improvements in the 6-minute walk distance and maximal ATP production in the hand muscle were not significant in the urolithin A group vs the placebo group, long-term urolithin A supplementation was beneficial for muscle endurance and plasma biomarkers, suggesting that urolithin A may counteract age-associated muscle decline; however, future work is needed to confirm this finding.","Older adults (aged ≥60 years) are the fastest growing age group in the world and projected to represent 1 in every 4 adults by 2050. Aging is associated with a progressive loss of muscle mass and strength that manifests as reduced physical performance and endurance capacity, imposing a burden on both the individual and society. Evidence has shown that mitochondrial dysfunction plays an important role in age-related diseases.In skeletal muscle, the decline in mitochondrial efficiency and capacity for adenosine triphosphate (ATP) production is associated with decreased performance and increased fatigue. With age, a progressive decline in the cell’s capacity to eliminate its dysfunctional mitochondria by a selective autophagy process named mitophagy contributes to poor mitochondrial quality. Therefore, restoring levels of mitophagy is an interesting approach to improving mitochondrial function. Urolithin A is a natural food metabolite of the gut microbiome that has been shown to stimulate mitophagy and improve muscle function in aged animals and in models of muscular dystrophy, while also being safe, bioavailable, and able to induce mitochondrial gene expression in older adults.The current study was designed to test the hypothesis that long-term supplementation with urolithin A would improve mitochondrial function and muscle performance in older adults. Specifically, we aimed to investigate whether oral administration of urolithin A improved the 6-minute walk distance, the muscle endurance in hand and leg muscles, and the biomarkers associated with mitochondrial and cellular health. Potential participants were screened for average physical performance using the 6-minute walk distance test and mitochondrial function using magnetic resonance spectroscopy (MRS) in the skeletal muscle. A supplementation period of 4 months, with efficacy readouts at 2 and 4 months, was chosen as the earliest time point at which improvements in muscle function end points would be likely observed.","This randomized clinical trial found that urolithin A was safe and well tolerated by older adults. The improvements in the primary end point of the 6-minute walk distance and maximal ATP production in hand muscles were not significant for urolithin A, but long-term urolithin A supplementation had a beneficial effect on the secondary end points of muscle endurance and biomarkers of mitochondrial health. The findings from this exploratory work suggest that urolithin A is a promising approach to counteracting age-associated muscle decline. However, future work is needed to confirm the beneficial role of urolithin A supplementation in healthy aging.",https://twitter.com/foundmyfitness/status/1486794303464640512,
Using an erythrocyte fatty acid fingerprint to predict risk of all-cause mortality: the Framingham Offspring Cohort,"Higher levels of omega-3 fatty acids in red blood cells are associated with a 5-year increase in life expectancy. In contrast, smoking was linked to a 5-year decrease in life expectancy, the same gain if you have high red blood cell levels of omega-3!","RBC long-chain omega-3 (n–3) fatty acid (FA) percentages (of total fatty acids) are associated with lower risk for total mortality, but it is unknown if a suite of FAs could improve risk prediction. The objective of this study was to compare a combination of RBC FA levels with standard risk factors for cardiovascular disease (CVD) in predicting risk of all-cause mortality. Framingham Offspring Cohort participants without prevalent CVD having RBC FA measurements and relevant baseline clinical covariates (n = 2240) were evaluated during 11 y of follow-up. A forward, stepwise approach was used to systematically evaluate the association of 8 standard risk factors (age, sex, total cholesterol, HDL cholesterol, hypertension treatment, systolic blood pressure, smoking status, and prevalent diabetes) and 28 FA metrics with all-cause mortality. A 10-fold cross-validation process was used to build and validate models adjusted for age and sex. Four of 28 FA metrics [14:0, 16:1n–7, 22:0, and omega-3 index (O3I; 20:5n–3 + 22:6n–3)] appeared in ≥5 of the discovery models as significant predictors of all-cause mortality. In age- and sex-adjusted models, a model with 4 FA metrics was at least as good at predicting all-cause mortality as a model including the remaining 6 standard risk factors (C-statistic: 0.778; 95% CI: 0.759, 0.797; compared with C-statistic: 0.777; 95% CI: 0.753, 0.802). A model with 4 FA metrics plus smoking and diabetes (FA + Sm + D) had a higher C-statistic (0.790; 95% CI: 0.770, 0.811) compared with the FA (P < 0.01) or Sm + D models alone (C-statistic: 0.766; 95% CI: 0.739, 0.794; P < 0.001). A variety of other highly correlated FAs could be substituted for 14:0, 16:1n–7, 22:0, or O3I with similar predicted outcomes. In this community-based population in their mid-60s, RBC FA patterns were as predictive of risk for death during the next 11 y as standard risk factors. Replication is needed in other cohorts to validate this FA fingerprint as a predictor of all-cause mortality.","The Framingham Heart Study provided unique insights into cardiovascular disease (CVD) risk factors  and led to the development of the Framingham Risk Score based on 8 baseline standard risk factors—that is, age, sex, smoking, hypertension treatment, diabetes status, systolic blood pressure, total cholesterol (TC), and HDL cholesterol. CVD is still the leading cause of death globally, and risk can be reduced by changing behavioral risk factors such as unhealthy diet, physical inactivity, and use of tobacco and alcohol. Therefore, biomarkers integrating lifestyle choices might help identify individuals at risk and be useful to assess treatment approaches, prevent morbidity, and delay death. Among the diet-based biomarkers are fatty acids (FAs), whether measured in plasma or RBC membranes. The FAs most clearly associated with reduced risk for CVD and for total mortality (i.e., death from any cause) are the omega-3 FAs, EPA (20:5n–3) and DHA (22:6n–3) . In a 2018 report including 2500 participants in the Framingham Offspring Cohort followed for a median of 7.3 y (i.e., between ages ∼66 and 73 y), the baseline RBC EPA + DHA content [the omega-3 index (O3I)] was significantly and inversely associated with risk for death from all causes. Individuals in the highest quintile were 33% less likely to succumb during the follow-up years compared with those in the lowest quintile. Similar associations have been seen in the Women's Health Initiative Memory Study , the Heart and Soul Study , and the Ludwigshafen Risk and Cardiovascular Health Study . However, these prior investigations evaluated only 1 FA metric (i.e., the O3I) as an exposure variable. Other FA biomarker-based studies have focused only on linoleic acid , FAs in the de novo lipogenesis pathway , trans FAs , dairy-derived FAs , or very-long-chain saturated FAs (VLCFAs) in relation to select disease outcomes. In 2009, Shearer et al. attempted to define an “FA risk fingerprint” using a cross-sectional design with ∼1350 individuals, half of whom were confirmed acute coronary syndrome patients and half were outpatient controls. In that study, RBC FA profiles were quantified, and the association of each FA with acute coronary syndrome was systematically evaluated. A suite of 10 FAs was identified and compared with a suite of standard risk factors—that is, age, sex, TC, HDL cholesterol, smoking status, and self-reported history of hypertension and diabetes. The RBC FA profile discriminated cases from controls significantly better than did standard risk factors. However, that approach is not easily translated into clinical use, partly because it was cross-sectional and used a case–control design. In the current investigation, we posed a similar question in a prospective setting using the Framingham Offspring Cohort, which was followed for clinical events for 11 y after RBC FAs were measured. Here, we explore how a fingerprint or pattern of RBC FAs measured in older Americans compares with standard risk factors as predictors of risk of all-cause mortality.","In conclusion, in this cohort followed for 11 y, the information carried in the concentrations of 4 RBC FA metrics was as useful as that carried in lipid levels, blood pressure, smoking, and diabetic status with regard to predicting total mortality. The best predictions were made with the FA metrics and smoking/diabetes status. In the future, when larger data sets are available, additional model-building approaches may be worth exploring (e.g., random forest, partial least-squares–discriminant analysis, and principal component analysis), along with replication in other cohorts; however, the cross-validation approach is robust and suggests a strong association between this RBC FA fingerprint and risk for all-cause mortality.",https://twitter.com/foundmyfitness/status/1418281979263414272,
Effectiveness of Covid-19 Vaccines against the B.1.617.2 (Delta) Variant,"Good news for those concerned about variants impacting vaccine-induced immunity: Two doses of the Pfizer-BioNTech vaccine were 88% effective in protecting against symptomatic COVID-19 from the SARS-CoV-2 Delta variant, according to a new paper in NEJM.","The B.1.617.2 (delta) variant of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes coronavirus disease 2019 (Covid-19), has contributed to a surge in cases in India and has now been detected across the globe, including a notable increase in cases in the United Kingdom. The effectiveness of the BNT162b2 and ChAdOx1 nCoV-19 vaccines against this variant has been unclear. We used a test-negative case–control design to estimate the effectiveness of vaccination against symptomatic disease caused by the delta variant or the predominant strain (B.1.1.7, or alpha variant) over the period that the delta variant began circulating. Variants were identified with the use of sequencing and on the basis of the spike (S) gene status. Data on all symptomatic sequenced cases of Covid-19 in England were used to estimate the proportion of cases with either variant according to the patients’ vaccination status. Effectiveness after one dose of vaccine (BNT162b2 or ChAdOx1 nCoV-19) was notably lower among persons with the delta variant (30.7%; 95% confidence interval [CI], 25.2 to 35.7) than among those with the alpha variant (48.7%; 95% CI, 45.5 to 51.7); the results were similar for both vaccines. With the BNT162b2 vaccine, the effectiveness of two doses was 93.7% (95% CI, 91.6 to 95.3) among persons with the alpha variant and 88.0% (95% CI, 85.3 to 90.1) among those with the delta variant. With the ChAdOx1 nCoV-19 vaccine, the effectiveness of two doses was 74.5% (95% CI, 68.4 to 79.4) among persons with the alpha variant and 67.0% (95% CI, 61.3 to 71.8) among those with the delta variant. Only modest differences in vaccine effectiveness were noted with the delta variant as compared with the alpha variant after the receipt of two vaccine doses. Absolute differences in vaccine effectiveness were more marked after the receipt of the first dose. This finding would support efforts to maximize vaccine uptake with two doses among vulnerable populations. (Funded by Public Health England.)","India has experienced a surge in cases of coronavirus disease 2019 (Covid-19) since late March 2021, reaching more than 400,000 cases and 4000 deaths reported each day in early May 2021.1 This increase has resulted in hospital services becoming overwhelmed and in a scarcity of oxygen supplies.2 Although only a small proportion of samples have been sequenced, B.1.617 lineages of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) have dominated. The B.1.617.2 (delta) variant was first detected in India in December 2020 and became the most commonly reported variant in the country starting in mid-April 2021. As of May 19, 2021, the variant had been detected in 43 countries across six continents in GISAID (originally an acronym for global initiative on sharing avian influenza data but more recently a site for compiling sequence data on viruses, particularly influenza and coronaviruses, that threaten to cause a pandemic). In the United Kingdom, a rapid increase in cases with this variant has been seen associated with travel from India and with community transmission. In the United Kingdom, vaccination was initially prioritized for older adults, caregivers, and health and social care workers, with subsequent rollout to persons in clinical risk groups and younger-age cohorts. At an early stage of the rollout, a policy decision, based on advice from the Joint Committee on Vaccination and Immunisation, was made to use an extended administration interval of up to 12 weeks in order to maximize the number of vulnerable persons receiving the first dose during the second wave of the pandemic in the context of constraints on vaccine supply and delivery. Vaccines have been found to be highly efficacious at preventing symptomatic disease, as shown by clinical trials and real-world evidence. The B.1.1.7 (alpha) variant, first identified in the United Kingdom, was the predominant lineage seen between January and May 2021. Levels of protection against the alpha variant that are conferred by vaccination are similar to those observed in clinical trials, with additional protection against severe disease. Laboratory data indicate that the B.1.351 (beta) variant has reduced neutralization, according to analysis of serum samples obtained from vaccinated persons. Observational data from Qatar indicated modestly reduced effectiveness against symptomatic disease caused by this variant but high levels of effectiveness against severe, critical, or fatal disease in persons vaccinated with the BNT162b2 vaccine (Pfizer–BioNTech).17 Furthermore, a trial of the NVX-CoV2373 vaccine (Novavax) showed 51.0% efficacy against the beta variant. Finally, high levels of neutralization have been seen with the P.1 (gamma) variant in serum samples obtained from persons vaccinated with the BNT162b2 vaccine, and one study showed only minimally reduced vaccine effectiveness against test-positive cases with one dose of messenger RNA vaccine. The delta variant is characterized by the spike protein mutations T19R, Δ157-158, L452R, T478K, D614G, P681R, and D950N.1 Several of these mutations may affect immune responses directed toward the key antigenic regions of receptor-binding protein (452 and 478) and deletion of part of the N-terminal domain. P681R is at the S1–S2 cleavage site, and it appears that strains with mutations at that site may have increased replication, which leads to higher viral loads and increased transmission. Data on the effectiveness of Covid-19 vaccines against clinical outcomes with this variant have been limited. In this study, we aimed to estimate the effectiveness of two Covid-19 vaccines, BNT162b2 and ChAdOx1 nCoV-19 (AstraZeneca), against symptomatic disease caused by the delta variant.","Overall, we found high levels of vaccine effectiveness against symptomatic disease with the delta variant after the receipt of two doses. These estimates were only modestly lower than the estimate of vaccine effectiveness against the alpha variant. Our finding of reduced effectiveness after the first dose would support efforts to maximize vaccine uptake with two doses among vulnerable groups in the context of circulation of the delta variant.",https://twitter.com/foundmyfitness/status/1417975194556059648,
"Meat consumption and risk of incident dementia: cohort study of 493,888 UK Biobank participants",Unprocessed red meat consumption was associated with a lower dementia and Alzheimer's disease incidence while consumption of processed meat was associated with higher risks.,"Worldwide, the prevalence of dementia is increasing and diet as a modifiable factor could play a role. Meat consumption has been cross-sectionally associated with dementia risk, but specific amounts and types related to risk of incident dementia remain poorly understood. We aimed to investigate associations between meat consumption and risk of incident dementia in the UK Biobank cohort. Meat consumption was estimated using a short dietary questionnaire at recruitment and repeated 24-h dietary assessments. Incident all-cause dementia comprising Alzheimer disease (AD) and vascular dementia (VD) was identified by electronic linkages to hospital and mortality records. HRs for each meat type in relation to each dementia outcome were estimated in Cox proportional hazard models. Interactions between meat consumption and the apolipoprotein E (APOE) ε4 allele were additionally explored. Among 493,888 participants included, 2896 incident cases of all-cause dementia, 1006 cases of AD, and 490 cases of VD were identified, with mean ± SD follow-up of 8 ± 1.1 y. Each additional 25 g/day intake of processed meat was associated with increased risks of incident all-cause dementia (HR: 1.44; 95% CI: 1.24, 1.67; P-trend < 0.001) and AD (HR: 1.52; 95% CI: 1.18, 1.96; P-trend = 0.001). In contrast, a 50-g/d increment in unprocessed red meat intake was associated with reduced risks of all-cause dementia (HR: 0.81; 95% CI: 0.69, 0.95; P-trend = 0.011) and AD (HR: 0.70; 95% CI: 0.53, 0.92; P-trend = 0.009). The linear trend was not significant for unprocessed poultry and total meat. Regarding incident VD, there were no statistically significant linear trends identified, although for processed meat, higher consumption categories were associated with increased risks. The APOE ε4 allele increased dementia risk by 3 to 6 times but did not modify the associations with diet significantly. These findings highlight processed-meat consumption as a potential risk factor for incident dementia, independent of the APOE ε4 allele.","Dementia is a major public health concern with around 50 million cases globally and an incidence of nearly 10 million new cases per annum. It comprises Alzheimer disease (AD), which contributes to 50–70% of dementia cases, vascular dementia (VD), which contributes to ∼25%, and other forms of dementia. Dementia development and progression are associated with both genetic and environmental factors, including diet and lifestyle. Lifestyle-related and dietary factors associated with dementia are potentially modifiable and thus represent targets for primary prevention. Meat consumption has gained increasing interest in relation to health, since high consumption of processed meat and probably red meat were found to be consistently associated with an increased risk of colorectal cancer. In recent decades meat consumption has doubled or even tripled globally, especially in developing countries. This dietary transition has been associated with increasing AD prevalence in Japan, Peru, Cuba and other low- and middle-income countries in both ecological and cross-sectional studies. A study of cognitively healthy individuals in Sweden showed that low consumption of meat and meat products was associated with better cognitive performance in clinical dementia screening tests and greater total brain volume after a 5-y follow-up period. Our previous review on meat consumption and cognitive disorders including dementia showed that most meat-related studies were embedded in complex dietary patterns with considerable heterogeneity, and the evidence of associations between risk of dementia and specific types or amounts of meat consumption was limited. A consistent association has been established between carriage of the apolipoprotein E (APOE) ε4 allele and elevated risk of dementia or AD. Previous stratified analyses by APOE ε4 status showed that unfavourable lifestyle factors (e.g., less healthy dietary pattern, less physical activity, smoking, and social isolation) were associated with higher risk of dementia in APOE ε4 noncarriers but not in carriers. The discrepancy between carriers and noncarriers indicates that APOE genotype may modify associations between lifestyle factors and dementia risks, and might be explained by a potential masking of weak associations from lifestyle factors by the strongly associated APOE ε4 allele. However, at present whether APOE ε4 allele carriage interacts with lifestyle factors, such as diet, influencing risk of dementia remains unclear. In the present study we examined the hypothesis that high consumption of meat increases the incidence of dementia in the general population, which may be more pronounced among APOE ε4 noncarriers.","Our findings suggest that consumption of processed meat may increase risk of incident dementia, and unprocessed red meat intake may be associated with lower risks, independent of APOE ε4 carriage. On the basis of the findings of this study, more specific public health guidance could be indicated differentiating between types of meat. However further research is recommended to confirm these results. Overall, the research adds to the growing body of evidence linking meat, especially processed meat consumption, to increased risk of a range of noncommunicable diseases.",https://twitter.com/foundmyfitness/status/1417907227294191616,
Very-low-carbohydrate diet enhances human T-cell immunity through immunometabolic reprogramming,A ketogenic diet may improve immunity and reduce excessive inflammation. Participants on a ketogenic diet for 3 weeks had enhanced activity of antibody-producing B cells and increased number and activity of regulatory T cells which prevent autoimmunity.,"Very-low-carbohydrate diet triggers the endogenous production of ketone bodies as alternative energy substrates. There are as yet unproven assumptions that ketone bodies positively affect human immunity. We have investigated this topic in an in vitro model using primary human T cells and in an immuno-nutritional intervention study enrolling healthy volunteers. We show that ketone bodies profoundly impact human T-cell responses. CD4+, CD8+, and regulatory T-cell capacity were markedly enhanced, and T memory cell formation was augmented. RNAseq and functional metabolic analyses revealed a fundamental immunometabolic reprogramming in response to ketones favoring mitochondrial oxidative metabolism. This confers superior respiratory reserve, cellular energy supply, and reactive oxygen species signaling. Our data suggest a very-low-carbohydrate diet as a clinical tool to improve human T-cell immunity. Rethinking the value of nutrition and dietary interventions in modern medicine is required.","Western diet is increasingly seen as being at the root of many diseases, such as metabolic syndrome, autoimmune disorders, and cancer, and thus is suspected to limit life expectancy during the 21st century (Olshansky et al, 2005; Christ & Latz, 2019). It impairs cellular immunity and evokes systemic low-grade inflammation not only by causing obesity but also by direct reprogramming of immune cells toward a proinflammatory phenotype (Hotamisligil et al, 1993; Visser et al, 1999; Weisberg et al, 2003; Mathis, 2013; Guo et al, 2015; de Torre-Minguela et al, 2017; Dror et al, 2017; Christ et al, 2018). Nutritional interventions may hold promise as a tool to prevent and even to treat disease. Unfortunately, most recommendations on food intake and dietary guidelines yet lack substantiated scientific background (Archer et al, 2018; Ioannidis, 2018; Archer & Lavie, 2019; Ludwig et al, 2019). Novel nutritional concepts promote a restriction of carbohydrates in favor of fat to ameliorate detrimental low-grade inflammation (Paoli et al, 2015; Bosco et al, 2018; Myette-Côté et al, 2018). However, large observational studies investigating this approach are highly controversial (Dehghan et al, 2018; Seidelmann et al, 2018), and molecular data in humans are scarce. In this regard, the high-fat low-carbohydrate ketogenic diet (KD) is one highly discussed approach (Bolla et al, 2019; Ruiz Herrero et al, 2020). Restriction of carbohydrate intake leads to the endogenous production of ketone bodies such as beta-hydroxybutyrate (BHB) as evolutionary conserved alternative metabolic substrates, which can be utilized via mitochondrial oxidative phosphorylation (Puchalska & Crawford, 2017). In animal models, BHB has been shown to dampen inappropriate innate immune responses via suppression of the NLRP3 inflammasome, thus ameliorating chronic low-grade inflammation and associated diseases (Youm et al, 2015; Goldberg et al, 2017; Newman et al, 2017). Human adaptive immunity, however, has not yet been addressed (Stubbs et al, 2020). Here, we present the first study investigating the influence of KD on human immune responses in vitro and in a cohort of healthy subjects. Our results reveal profound beneficial effects of ketone bodies on human T-cell immunity. BHB improves effector and regulatory T-cell function and primes human T memory cell differentiation both in vitro and in vivo. These functional changes are based on a fundamental immunometabolic reprogramming, resulting in enhanced mitochondrial oxidative metabolism, thus conferring an increased immunometabolic capacity to human T cells. We provide molecular evidence that ketone bodies promptly improve human T-cell metabolism and immunity. By complementing classical approaches of modern medicine, nutritional interventions offer new perspectives for prevention and therapy of numerous diseases.","Our study demonstrates that KD induces a fundamental immunometabolic reprogramming in human T cells associated with profound transcriptomic changes. This leads to a balanced global enhancement of T-cell immunity, comprising enhanced cytokine production and secretion, strengthened cell lysis capacity, amplified Treg differentiation, and pronounced Tmem cell formation. KD thus holds promise as a feasible and effective clinical tool for a large range of conditions intimately associated with immune disorders. This provides the basis to proceed into further medical translation. Consequently, a clinical phase II study investigating KD in sepsis patients is currently recruiting (Rahmel et al, 2020). These new immunological aspects of KD might contribute to the modern concept of metabolic therapy of cancer (Seyfried et al, 2017, 2020). KD not only targets the Warburg effect, but could also strengthen anti-tumor immunity (Ferrere et al, 2021). Moreover, the different effects of BHB-induced ROS elevation on tumor cells and T cells might even lead to additive beneficial effects: While oxidative stress compromises cancer cell viability, mild increase in ROS enhances T-cell immune capacity which in turn further restrains tumor growth. However, these issues need to be addressed in future clinical studies. In conclusion, our study changes the perspective on nutrition as a clinical tool and could help to redefine the role of dietary interventions in modern medicine.",https://twitter.com/foundmyfitness/status/1410330583859830786,
Effects of Aerobic Exercise Training on Systemic Biomarkers and Cognition in Late Middle-Aged Adults at Risk for Alzheimer’s Disease,Aerobic exercise for 26 weeks improved verbal learning and memory and increased a biomarker called CTSB which is secreted from muscle into circulation after exercise and is associated with memory and the growth of new brain cells in the hippocampus.,"Increasing evidence indicates that physical activity and exercise training may delay or prevent the onset of Alzheimer’s disease (AD). However, systemic biomarkers that can measure exercise effects on brain function and that link to relevant metabolic responses are lacking. To begin to address this issue, we utilized blood samples of 23 asymptomatic late middle-aged adults, with familial and genetic risk for AD (mean age 65 years old, 50% female) who underwent 26 weeks of supervised treadmill training. Systemic biomarkers implicated in learning and memory, including the myokine Cathepsin B (CTSB), brain-derived neurotrophic factor (BDNF), and klotho, as well as metabolomics were evaluated. Here we show that aerobic exercise training increases plasma CTSB and that changes in CTSB, but not BDNF or klotho, correlate with cognitive performance. BDNF levels decreased with exercise training. Klotho levels were unchanged by training, but closely associated with change in VO2peak. Metabolomic analysis revealed increased levels of polyunsaturated free fatty acids (PUFAs), reductions in ceramides, sphingo- and phospholipids, as well as changes in gut microbiome metabolites and redox homeostasis, with exercise. Multiple metabolites (~30%) correlated with changes in BDNF, but not CSTB or klotho. The positive association between CTSB and cognition, and the modulation of lipid metabolites implicated in dementia, support the beneficial effects of exercise training on brain function. Overall, our analyses indicate metabolic regulation of exercise-induced plasma BDNF changes and provide evidence that CTSB is a marker of cognitive changes in late middle-aged adults at risk for dementia.","Alzheimer’s disease (AD) is the most common neurodegenerative disease. The accumulation of amyloid plaques and neurofibrillary tangles result in a progressive loss of brain function that inevitably leads to mental and physical disability. The disease has genetic components, and individuals with a family member affected by AD or who carry the APOE ε4 allele are at increased risk. To date there are no effective treatment options for AD patients, and recent pharmacological trials have resulted in failure. Therefore, lifestyle interventions such as exercise training that may delay the onset of neurodegenerative conditions have become increasingly imperative. In rodents, running training enhances adult hippocampal neurogenesis, synaptic plasticity, and neurotrophin levels throughout the lifespan. In aging humans, aerobic exercise training increases gray and white matter volume, enhances blood flow, and improves memory function. However, human studies often utilize expensive and low-throughput brain imaging analyses that are not practical for large population-wide studies. Systemic biomarkers that can measure the effect of exercise interventions on AD-related outcomes quickly and at low-cost could be used to inform disease progression and for the development of novel therapeutic targets. Several exercise biomarkers have been proposed, including growth factor BDNF, myokine CTSB, and klotho. BDNF is an apparent candidate given that this protein is upregulated in the rodent hippocampus and cortex by running and is important for adult neurogenesis, synaptic plasticity, and memory function. However, the effects of exercise training on human peripheral BDNF levels have been equivocal. A more recent candidate marker is the myokine CTSB, a lysosomal cysteine protease. Adults with cognitive impairment have lower CTSB levels in serum and brain. Several studies indicate CTSB decreases β−amyloid, whereas others report that CTSB inhibitors may reduce β−amyloid. In young adults, four months of aerobic exercise training elevated plasma CTSB in association with improved fitness and memory function. Klotho is another circulating protein that can enhance cognition and synaptic function. Klotho is associated with resilience to neurodegenerative disease, possibly by supporting brain structures responsible for memory and learning. Emerging evidence suggests that its expression is upregulated by exercise. Finally, peripheral metabolomic changes associated with dementia have been reported, but have not been examined in conjunction with an exercise intervention. In the present study we aimed to determine whether metabolomic profiles related to brain health are beneficially altered following 26 weeks of aerobic exercise training in late middle-aged adults at risk for AD and how such alterations relate to systemic biomarkers such as BDNF, CTSB, and klotho. Therefore, we tested the hypotheses that circulating BDNF, CTSB, and klotho would increase following exercise training and correlate with cognition and metabolomic markers of brain health.","In conclusion, plasma CTSB levels were increased following this 26-week structured aerobic exercise training in adults at risk for AD, and change in CTSB was positively associated with cognitive function. Plasma BDNF levels decreased in conjunction with metabolomic changes. Serum klotho was unchanged but was associated with cardiorespiratory fitness. Multiple lipid metabolites relevant to AD were modified by exercise in a manner that may be neuroprotective. Our findings position CTSB, BDNF, and klotho as exercise biomarkers for evaluating the effect of lifestyle interventions on brain function.",https://twitter.com/foundmyfitness/status/1409984258324664322,
Quantitative mapping of human hair greying and reversal in relation to life stress,Stress can accelerate the graying of hair which was reversible for some people if the stress was removed before reaching a threshold before it turns permanently gray. Stress-induced changes in mitochondria play a role in how stress turns hair gray.,"Hair greying is a hallmark of aging generally believed to be irreversible and linked to psychological stress. Here, we develop an approach to profile hair pigmentation patterns (HPPs) along individual human hair shafts, producing quantifiable physical timescales of rapid greying transitions. Using this method, we show white/grey hairs that naturally regain pigmentation across sex, ethnicities, ages, and body regions, thereby quantitatively defining the reversibility of greying in humans. Molecularly, grey hairs upregulate proteins related to energy metabolism, mitochondria, and antioxidant defenses. Combining HPP profiling and proteomics on single hairs, we also report hair greying and reversal that can occur in parallel with psychological stressors. To generalize these observations, we develop a computational simulation, which suggests a threshold-based mechanism for the temporary reversibility of greying. Overall, this new method to quantitatively map recent life history in HPPs provides an opportunity to longitudinally examine the influence of recent life exposures on human biology.","Hair greying is a ubiquitous, visible, and early feature of human biological aging (O'Sullivan et al., 2021; Tobin, 2011). The time of onset of hair greying varies between individuals, as well as between individual hair follicles, based on genetic and other biobehavioral factors (Akin Belli et al., 2016; Bernard, 2012). But most people experience depigmentation of a progressively large number of hair shafts (HSs) from their third decade onward, known as achromotrichia or canities (Panhard et al., 2012). The color in pigmented HSs is provided by melanin granules, a mature form of melanosomes continuously supplied to the trichocytes of the growing hair shaft by melanocytes of the hair follicle pigmentary unit (HFPU) (Tobin, 2011). Age-related greying is thought to involve bulb and outer root sheath melanocyte stem cell (MSC) exhaustion (Commo et al., 2004; Nishimura et al., 2005), neuroendocrine alterations (Paus, 2011), and other factors, with oxidative damage to the HFPU likely being the dominant, initial driver (Arck et al., 2006; Paus, 2011; Trueb and Tobin, 2010). While loss of pigmentation is the most visible change among greying hairs, depigmented hairs also differ in other ways from their pigmented counterparts (Tobin, 2015), including in their growth rates (Nagl, 1995), HF cycle, and other biophysical properties (Van Neste and Tobin, 2004). Hair growth is an energetically demanding process (Flores et al., 2017) relying on aerobic metabolism in the HF (Williams et al., 1993), and melanosome maturation also involves the central organelle of energy metabolism, mitochondria (Basrur et al., 2003; Zhang et al., 2019). Moreover, mitochondria likely contribute to oxidative stress within the HF (Lemasters et al., 2017), providing converging evidence that white hairs may exhibit specific alterations in mitochondrial energy metabolism. Although hair greying is generally considered a progressive and irreversible age-related process, with the exclusion of alopecia areata (McBride and Bergfeld, 1990), cases of drug- and mineral deficiency-induced depigmentation or repigmentation of hair have been reported (Kavak et al., 2005; Kobayashi et al., 2014; Komagamine et al., 2013; Reynolds et al., 1989; Ricci et al., 2016; Sieve, 1941; Yoon et al., 2003) reflecting the influence of environmental inputs into HFPU function (Paus et al., 2014). Because most hairs are continually growing from a living hair follicle, sensitive to changing physiological conditions, into a hardened hair shaft external to the body that retains stable molecular traces of these conditions, the hair shaft represents a bioarchive of recent exposures (Kalliokoski et al., 2019). While spontaneous repigmentation can be pharmacologically induced, its natural occurrence in unmedicated individuals is rare and has only been reported in a few single-patient case studies (Comaish, 1972; Navarini and Trüeb, 2010; O'Sullivan et al., 2021; Tobin and Cargnello, 1993; Tobin and Paus, 2001). The reversal of hair greying has not been quantitatively examined in a cohort of healthy adults, in parallel with molecular factors and psychosocial exposures. The influence of psychological stress on hair pigmentation is a debated, but poorly documented, aspect of hair greying. In humans, psychological stress accelerates biological aging as measured by telomere length (Epel et al., 2004; Puterman et al., 2016). In mice, psychological stress and the stress mediator norepinephrine acutely causes depigmentation (Zhang et al., 2020). However, greying in both mice and humans has been shown to be a relatively irreversible phenomenon driven in part by a depletion of melanocyte stem cells, although some stem cells and transient amplifying cells do remain (Trueb and Tobin, 2010). In humans, recent evidence suggests hair growth and pigmentation changes in response to stress (Peters et al., 2017), but this relationship, along with reversal of greying, remain insufficiently understood. The paucity of quantitative data in humans is mostly due to the lack of sensitive methods to precisely correlate stressful psychobiological processes with hair pigmentation and greying events at the single-follicle level Here, we describe a digitization approach to map hair pigmentation patterns (HPPs) in single hairs undergoing greying and reversal transitions, examine proteomic features of depigmented white hairs, and illustrate the utility of the HPP approach to interrogate the association of life stress and hair greying in humans. Because previous literature suggests that rare repigmentation events are more likely to occur in the early stages of canities (Van Neste and Tobin, 2004), the current study focuses primarily on pigmentation events in young to middle-aged participants. Finally, we develop a computational model of hair greying to explore the potential mechanistic basis for stress-induced greying and reversibility on the human scalp hair population, which could potentially serve as a resource for the in silico modeling of macroscopic aging events in human tissues.","Our approach to quantify HPPs demonstrates rapid greying transitions and their natural transitory reversal within individual human hair follicles at a higher frequency and with different kinetics than had previously been appreciated. The literature generally assumes pigment production in the HFPU to be a continuous process for the entire duration of an anagen cycle, but here we document a complete switch-on/off phenomena during a single anagen cycle. The proteomic features of hair greying directly implicate multiple metabolic pathways that are both reversible in nature and sensitive to stress-related neuroendocrine factors. Therefore, this result provides a plausible biological basis for the rapid reversibility of greying and its association with psychological factors, and also supports the possibility that this process could be targeted pharmacologically.",https://twitter.com/foundmyfitness/status/1409608805869002754,
Potential reversal of epigenetic age using a diet and lifestyle intervention: a pilot randomized clinical trial,"Epigenetic age reversed by 3 yrs after an 8-week diet & lifestyle change including leafy greens, cruciferous, blueberries, animal protein, liver, eggs, no refined sugar, TRE, exercise & more (compared to control group)","Manipulations to slow biological aging and extend healthspan are of interest given the societal and healthcare costs of our aging population. Herein we report on a randomized controlled clinical trial conducted among 43 healthy adult males between the ages of 50-72. The 8-week treatment program included diet, sleep, exercise and relaxation guidance, and supplemental probiotics and phytonutrients. The control group received no intervention. Genome-wide DNA methylation analysis was conducted on saliva samples using the Illumina Methylation Epic Array and DNAmAge was calculated using the online Horvath DNAmAge clock (2013). The diet and lifestyle treatment was associated with a 3.23 years decrease in DNAmAge compared with controls (p=0.018). DNAmAge of those in the treatment group decreased by an average 1.96 years by the end of the program compared to the same individuals at the beginning with a strong trend towards significance (p=0.066). Changes in blood biomarkers were significant for mean serum 5-methyltetrahydrofolate (+15%, p=0.004) and mean triglycerides (-25%, p=0.009). To our knowledge, this is the first randomized controlled study to suggest that specific diet and lifestyle interventions may reverse Horvath DNAmAge (2013) epigenetic aging in healthy adult males. Larger-scale and longer duration clinical trials are needed to confirm these findings, as well as investigation in other human populations.","Advanced age is the largest risk factor for impaired mental and physical function and many non-communicable diseases including cancer, neurodegeneration, type 2 diabetes, and cardiovascular disease. The growing health-related economic and social challenges of our rapidly aging population are well recognized and affect individuals, their families, health systems and economies. Considering economics alone, delaying aging by 2.2 years (with associated extension of healthspan) could save $7 trillion over fifty years. This broad approach was identified to be a much better investment than disease-specific spending. Thus, if interventions can be identified that extend healthspan even modestly, benefits for public health and healthcare economics will be substantial. DNA methylation is the addition of a methyl group to cytosine residues at selective areas on a chromosome (e.g. CpG islands, shelf/shore, exons, open sea). Methylation constitutes the best-studied, and likely most resilient of many mechanisms controlling gene expression. Unique among epigenetic markers, DNA methylation can readily and cheaply be mapped from tissue samples. Of 20+ million methylation sites on the human genome, there are a few thousand at which methylation levels are tightly correlated with age. Currently, the best biochemical markers of an individual’s age are all based on patterns of methylation. This has led some researchers to propose that aging itself has its basis in epigenetic changes (including methylation changes) over time. As of this writing, the best-studied methylation-based clock is the multi-tissue DNAmAge clock. At the time this study design was approved, there were few viable alternatives. Horvath’s DNAmAge clock predicts all-cause mortality and multiple morbidities better than chronological age. Methylation clocks (including DNAmAge) are based on systematic methylation changes with age. DNAmAge clock specifically demonstrates about 60% of CpG sites losing methylation with age and 40% gaining methylation. This is distinct from stochastic changes, “methylation drift”, unpredictable changes which vary among individuals and cell-by-cell within individuals. Systematic methylation changes include hypermethylation in promotor regions of tumor suppressor genes (inhibiting expression) and hypomethylation promoting inflammatory cytokines (promoting expression). Saliva can be considered a good source of high-quality DNA, containing both white blood cells and buccal cells, and is a suitable tissue type to be assessed for the DNAmAge clock. The dietary recommendations employed as part of the treatment protocol for this study were based largely on biochemistry and generalized measures of health, because few dietary associations with the DNAmAge clock have yet been established. A modest, but significant, reduction in DNAmAge in individuals consuming a non-specific lean meat, fish and plant-based diet (as measured by blood carotenoids) has been observed. It is possible that changes of a greater magnitude require a more targeted approach. The dietary intervention used here was also plant-centered, but including a high intake of nutrients that are substrates or cofactors in methylation biosynthetic pathways (e.g. containing folate, betaine), ten-eleven translocation (TET) demethylase cofactors and modulators (e.g. alpha ketoglutarate, vitamin C and vitamin A) and polyphenolic modulators of DNA methyl transferases (DNMT) (e.g. curcumin, epigallocatechin gallate (EGCG), rosmarinic acid, quercetin, luteolin). It also included limited nutrient-dense animal proteins (e.g. liver, egg). The diet restricted carbohydrates and included mild intermittent fasting, both designed to lower glycemic cycling. The diet was supplemented daily with a fruit and vegetable powder, also rich in polyphenolic modulators of DNMT activity, and a probiotic providing 40 million CFU of Lactobacillus plantarum 299v. L. plantarum has been shown to be a folate producer in the presence of para aminobenzoic acid (PABA); it also has been demonstrated to alter gene expression. Lifestyle guidance in this study included a minimum of 30 minutes of exercise per day, at least 5 days per week at an intensity of 60-80 percent of maximum perceived exertion. Exercise is well-known to be broadly beneficial for almost every aspect of health and has been shown to extend mean lifespan in animal models. Exploration of the effect of exercise on the methylome has recently begun. For example, regular tai chi practice was associated with slowing of age-related DNA methylation losses in 500 women. In another study of 647 women, a lifelong history of exercise was associated with a similar endpoint. These results were not reported in terms of the Horvath clock, because it had not yet been developed. One systematic review of human studies found that regular, daily physical activity was associated with lower blood levels of homocysteine, which when elevated, suggests an insufficiency of methylation capacity. Excessive exercise may accelerate methylation aging, but this danger has only been observed in elite, competitive athletes. Twice-daily breathing exercises that elicit the Relaxation Response were prescribed for stress reduction. It was recently demonstrated that 60 days of relaxation practice designed to elicit the Relaxation Response, 20 minutes twice per day, could significantly reduce DNAmAge as measured by the Zbieć-Piekarska clock in their group of healthy participants (though not in their ‘patient’ group). Almost a quarter of the DNAmAge CpG sites (85/353) are located in glucocorticoid response elements, pointing to a likely relationship between stress and accelerated aging. Cumulative lifetime stress has been shown to be associated with accelerated aging of the methylome. Zannas et al. also reported that dexamethasone, a glucocorticoid agonist, can advance the DNAmAge clock and induce associated transcriptional changes. Dexamethasone-regulated genes showed enriched association of aging-related diseases, including coronary artery disease, arteriosclerosis and leukemias. Other findings include that PTSD contributes to accelerated methylation age; and that greater infant distress (lack of caregiver contact) is associated with an underdeveloped, younger epigenetic age. This study aimed to optimize sleep, with a recommendation for at least seven hours nightly. Seven hours is generally considered to be healthy, but the limited data on accelerated aging only relates to extremes of sleep deprivation. A (presumably transient) effect of sleep deprivation on genome-wide methylation patterns in blood has been demonstrated. Acceleration of the DNAmAge clock has been associated with insomnia in a sample of 2078 women. Carskadon et al found an association between poor quality / fewer hours of sleep with age acceleration in a small sample of 12 female college students. This multimodal (“systems”) intervention is reflective of a clinically-used approach that combines individual interventions, each of which carry evidence of favorable influence on the DNA methylome and of which several authors of this study have clinical experience of health benefits. Such interventions likely produce synergistic effects and reduce the possibility of negative effects from one disease-promoting input canceling out the benefits of another health-promoting input. Dietary and lifestyle interventions, as used here, target upstream influences that are generally considered safe, even over the long term. By design, an important endpoint of this study was to be Horvath’s DNAmAge clock, to see if it could be potentially slowed. This is to say we have tentatively accepted the hypothesis that the methylation pattern from which the DNAmAge clock is computed is a driver of aging (and the chronic diseases of aging), thus we expect that attempting to directly influence the DNA methylome using diet and lifestyle to set back DNAmAge will lead to a healthier, more “youthful” metabolism. To date, three non-controlled studies have demonstrated set back of DNAmAge. One small pilot study has been reported to have set back the DNAmAge clock over the course of 12 months by 1.5 (plus the one-year duration of the study) years in healthy men, using a combination of growth hormone, metformin, DHEA and two dietary supplements. Two additional studies have demonstrated age reduction from diet and/or dietary supplement interventions. A subgroup of Polish women from the NU-AGE cohort suggested a reduction in biological age of 1.47 years after 1 year of a Mediterranean diet plus 400IU vitamin D3, and a 16-week trial using 4000IU of vitamin D3 in overweight or obese African Americans with suboptimal D status demonstrated a 1.85-year reduction in biological age. Herein we report comparable initial results based on diet and lifestyle interventions employed for eight weeks (preceded by a one-week education period).","The increase in circulating folate demonstrates that dietary sources and folate-producing probiotics can be an effective method of nutrient repletion. The reduction in serum triglycerides might be expected with a diet that lowered carbohydrate intake and glycemic response, plus exercise. While we expected to see a decrease in homocysteine with an intervention that supplied additional dietary B vitamins and betaine, as well as exercise, the average starting homocysteine value of the treatment group was 10.9 umol/L, already within a range typically identified as “normal” (<15 umol/L).",https://twitter.com/foundmyfitness/status/1401992520788168705,
SARS-CoV-2 infection induces long-lived bone marrow plasma cells in humans,A new study found that some people that previously had mild COVID-19 illness develop antibody-producing cells in the bone marrow that can last a lifetime.Antibody-producing cells against SARS-CoV2 were also found in people 11 months after first symptoms.,"Long-lived bone marrow plasma cells (BMPCs) are a persistent and essential source of protective antibodies. Individuals who have recovered from COVID-19 have a substantially lower risk of reinfection with SARS-CoV-2. Nonetheless, it has been reported that levels of anti-SARS-CoV-2 serum antibodies decrease rapidly in the first few months after infection, raising concerns that long-lived BMPCs may not be generated and humoral immunity against SARS-CoV-2 may be short-lived. Here we show that in convalescent individuals who had experienced mild SARS-CoV-2 infections (n = 77), levels of serum anti-SARS-CoV-2 spike protein (S) antibodies declined rapidly in the first 4 months after infection and then more gradually over the following 7 months, remaining detectable at least 11 months after infection. Anti-S antibody titres correlated with the frequency of S-specific plasma cells in bone marrow aspirates from 18 individuals who had recovered from COVID-19 at 7 to 8 months after infection. S-specific BMPCs were not detected in aspirates from 11 healthy individuals with no history of SARS-CoV-2 infection. We show that S-binding BMPCs are quiescent, which suggests that they are part of a stable compartment. Consistently, circulating resting memory B cells directed against SARS-CoV-2 S were detected in the convalescent individuals. Overall, our results indicate that mild infection with SARS-CoV-2 induces robust antigen-specific, long-lived humoral immune memory in humans.","Reinfections by seasonal coronaviruses occur 6 to 12 months after the previous infection, indicating that protective immunity against these viruses may be short-lived14,15. Early reports documenting rapidly declining antibody titres in the first few months after infection in individuals who had recovered from COVID-19 suggested that protective immunity against SARS-CoV-2 might be similarly transient11,12,13. It was also suggested that infection with SARS-CoV-2 could fail to elicit a functional germinal centre response, which would interfere with the generation of long-lived plasma cells3,4,5,7,16. More recent reports analysing samples that were collected approximately 4 to 6 months after infection indicate that SARS-CoV-2 antibody titres decline more slowly than in the initial months after infection8,17,18,19,20,21. Durable serum antibody titres are maintained by long-lived plasma cells—non-replicating, antigen-specific plasma cells that are detected in the bone marrow long after the clearance of the antigen1,2,3,4,5,6,7. We sought to determine whether they were detectable in convalescent individuals approximately 7 months after SARS-CoV-2 infection.","This study sought to determine whether infection with SARS-CoV-2 induces antigen-specific long-lived BMPCs in humans. We detected SARS-CoV-2 S-specific BMPCs in bone marrow aspirates from 15 out of 19 convalescent individuals, and in none from the 11 control participants. The frequencies of anti-S IgG BMPCs modestly correlated with serum IgG titres at 7–8 months after infection. Phenotypic analysis by flow cytometry showed that S-binding BMPCs were quiescent, and their frequencies were largely consistent in 5 paired aspirates collected at 7 and 11 months after symptom onset. Notably, we detected no S-binding cells among plasmablasts in blood samples collected at the same time as the bone marrow aspirates by ELISpot or flow cytometry in any of the convalescent or control samples. Together, these data indicate that mild SARS-CoV-2 infection induces a long-lived BMPC response. In addition, we showed that S-binding memory B cells in the blood of individuals who had recovered from COVID-19 were present at similar frequencies to those directed against influenza virus HA. Overall, our results are consistent with SARS-CoV-2 infection eliciting a canonical T-cell-dependent B cell response, in which an early transient burst of extrafollicular plasmablasts generates a wave of serum antibodies that decline relatively quickly. This is followed by more stably maintained levels of serum antibodies that are supported by long-lived BMPCs.",https://twitter.com/foundmyfitness/status/1399798979190087682,"""Main"" section used as Introduction"
"Intermittent fasting enhances long-term memory consolidation, adult hippocampal neurogenesis, and expression of longevity gene Klotho",Alternate day fasting for 3 months increased the growth of new neurons in the brain and improved long-term memory in mice. It also increased the activity of the Klotho gene which has been identified as a longevity gene in humans.,"Daily calorie restriction (CR) and intermittent fasting (IF) enhance longevity and cognition but the effects and mechanisms that differentiate these two paradigms are unknown. We examined whether IF in the form of every-other-day feeding enhances cognition and adult hippocampal neurogenesis (AHN) when compared to a matched 10% daily CR intake and ad libitum conditions. After 3 months under IF, female C57BL6 mice exhibited improved long-term memory retention. IF increased the number of BrdU-labeled cells and neuroblasts in the hippocampus, and microarray analysis revealed that the longevity gene Klotho (Kl) was upregulated in the hippocampus by IF only. Furthermore, we found that downregulating Kl in human hippocampal progenitor cells led to decreased neurogenesis, whereas Kl overexpression increased neurogenesis. Finally, histological analysis of Kl knockout mice brains revealed that Kl is required for AHN, particularly in the dorsal hippocampus. These data suggest that IF is superior to 10% CR in enhancing memory and identifies Kl as a novel candidate molecule that regulates the effects of IF on cognition likely via AHN enhancement.","Calorie restriction (CR), typically defined as a 10–40% total reduction in daily calorie intake, and intermittent fasting (IF), typically involving every-other-day feeding, are two established dietary paradigms that extend life- and health-span across species. A recent study demonstrated that adoption of 30% CR or a single meal feeding strategy for 10 months enhanced longevity and health status in mice, regardless of whether their content was high or low in sugar. Adherence to CR and IF regimens also improves learning and memory in different models. Mechanistically, animal studies have shown that CR and IF induce a mild adaptive cellular stress response that promotes neuronal resilience to injury and pathology. Notably, CR and IF have been conflated in the literature with many studies reporting the beneficial effects of CR on measures of inflammation, neurodegeneration, brain plasticity, learning and motor performance, when in fact forms of IF were used in these studies to bring about overall reductions in calorie intake. However, one of the first studies to dissociate IF from CR demonstrated that mice in the IF paradigm did not reduce their overall food intake and maintained body weight. Despite no reduction in total calorie intake, IF still induced beneficial effects that matched those elicited by 40% CR, including reduced concentrations of serum insulin and glucose when compared to ad libitum (AL) intake. IF also improved neuronal survival following excitotoxic challenge in hippocampal regions CA1 and CA3 as induced by a stereotaxic injection of kainate in the dorsal hippocampus (DH) when compared to CR and AL mice. It is important to acknowledge, however, that the similar body weight between IF and CR groups was only achieved 20 weeks after the start of the dietary regimens. It is possible that effects on brain plasticity driven by short-term differences in overall body weight between groups may still be a factor that hinders the disentanglement of cellular and molecular pathways that are specific to IF and CR. Even with these potential limitations, the findings by Anson et al. indicate that IF could induce neuroprotection independent of overall calorie intake in the long term. Despite the positive effects of CR and IF in neurodegenerative and affective conditions, the specific behavioral contributions and mechanisms that differentiate both interventions remain largely unknown. Answering these questions is pivotal to adapting these regimens to human populations, given the challenges of adhering to a long-term CR regimen when compared to the improved adherence to variations of the IF paradigm. Here, we directly compared the effects of IF to a matched 10% daily CR regimen upon learning and memory in mice. A 10% energy restriction protocol was chosen for the CR group following the observation that IF mice overall consume 10% less calories on a weekly basis. IF improved long-term retention memory to a greater extent than CR and was associated with increased adult hippocampal neurogenesis (AHN) and upregulation of the longevity gene Klotho (Kl). The Kl gene produces a membrane-bound, single-pass protein (KL) that can be cleaved at the cell surface to produce a secreted form found in mammalian sera, urine and cerebrospinal fluid. Though KL is produced primarily in the kidney, it is also highly expressed in some brain areas, including the dentate gyrus (DG) of the hippocampus and in particular by its mature neurons. The function of Kl in the brain is still largely unknown but it has been proposed that Kl plays an important role in cognition because increased serum levels of KL were associated with increased cognitive ability in humans and rodents. Here, we confirm previous evidence suggesting that Kl is an important regulator of AHN and propose it as a novel molecular player through which IF may enhance cognitive performance. Finally, we highlight the potential for IF paradigms in the form of increased meal interval to help bring about improved cognitive performance in human populations","In conclusion, we showed that IF is more effective in improving long-term memory retention and generating more newborn neurons in the DG when compared to 10% CR. Moreover, we found that Kl, the longevity gene, is upregulated by IF only and that Kl is required for appropriate hippocampal neurogenesis in vitro and in vivo, especially in the DH. Our findings suggest that IF has the potential to be a potent cognitive enhancer, a finding that holds promise for use in humans. The search for the molecular pathways regulated by Kl in the hippocampus might also shed light on important pharmacological targets whose activation may mimic the beneficial effects of fasting on mental health.",https://twitter.com/foundmyfitness/status/1397603637459779585,"repeat, different TLDR"
A Scalable Suspension Platform for Generating High-Density Cultures of Universal Red Blood Cells from Human Induced Pluripotent Stem Cells,Are lab-generated red blood cells on the horizon? Type O red blood cells were produced from skin cells that were differentiated into induced pluripotent stem cells. These red blood cells had normal functions such as oxygen-carry capacity and more.,"Universal red blood cells (RBCs) differentiated from O-negative human induced pluripotent stem cells (hiPSCs) could find applications in transfusion medicine. Given that each transfusion unit of blood requires 2 trillion RBCs, efficient bioprocesses need to be developed for large-scale in vitro generation of RBCs. We have developed a scalable suspension agitation culture platform for differentiating hiPSC-microcarrier aggregates into functional RBCs and have demonstrated scalability of the process starting with 6 well plates and finally demonstrating in 500 mL spinner flasks. Differentiation of the best-performing hiPSCs generated 0.85 billion erythroblasts in 50 mL cultures with cell densities approaching 1.7 × 107 cells/mL. Functional (oxygen binding, hemoglobin characterization, membrane integrity, and fluctuations) and transcriptomics evaluations showed minimal differences between hiPSC-derived and adult-derived RBCs. The scalable agitation suspension culture differentiation process we describe here could find applications in future large-scale production of RBCs in controlled bioreactors.","O-negative rhesus factor D-negative (O-neg) blood, the universal donor blood type, is considered a limited and valuable source of red blood cells (RBCs) for emergency transfusion applications (Hirani et al., 2017). Anticipated supply shortages in the future due to an aging population and risks from emerging viruses and pathogens (Alter et al., 2007) have driven initiatives to develop alternate and ready sources of universal donor blood. Differentiation of RBCs from human induced pluripotent stem cells (hiPSCs) is one such approach actively being investigated. The unlimited proliferation potential of hiPSCs coupled with their potential to differentiate into hematopoietic lineages (Paes et al., 2017) has made these cells appealing as limitless sources of starting materials for generating universal RBCs. It has been postulated that as few as 10 hiPSC clones derived from patients with rare blood phenotypes would be sufficient to cover the necessary blood types for 99% of the population with recurrent transfusion needs (Peyrard et al., 2011). Each transfusion unit of blood consists of 2 trillion RBCs. In vitro derivation of such large numbers of RBCs requires overcoming a few unmet challenges. First is the lack of efficient bioprocesses that can be scaled up from laboratory to industrial scale for RBC manufacture. Although several groups have shown the potential for efficient differentiation of hiPSCs toward RBCs (Dorn et al., 2015; Mao et al., 2016; Olivier et al., 2016), most of these may not be favorable for clinical development, either due to the use of undefined or xenogenic components or due to the lack of scalability of the process. Other groups have also immortalized adult erythroblasts to produce RBCs (Trakarnsanga et al., 2017; Kurita et al., 2013; Hirose et al., 2013). Yet a second challenge that needs to be overcome is the lack of cost-effective means to achieve ultra-high-density cultures of RBCs. Given that each unit of blood requires 2 × 1012 RBCs, one would have to achieve cell densities of at least 1 × 108 cells/mL in order to generate the desired cell numbers in a minimal medium volume. Thus far, the highest reported cell density for RBC culture appears to be in the range of 1 × 107 cells/mL (Ying Wang et al., 2016). Development of a scalable process that can eventually be transferred to large-scale stirred bioreactors would require the entire process to be performed in continuous agitation suspension culture. We have previously described means to scale up the pluripotent expansion stage by culturing hiPSCs on Laminin-521 (LN-521)-coated microcarriers (MCs) (Lam et al., 2016; Sivalingam et al., 2018). We have also shown that hiPSC-MC aggregates in suspension culture can efficiently differentiate into T-Bra+ and KDR+ mesodermal cells (Sivalingam et al., 2018), demonstrating that hiPSC-MC aggregates could differentiate as embryoid bodies (EBs) in a scalable manner. The current study was undertaken to develop an agitation suspension culture bioprocess for differentiation of hiPSCs to erythroid cells with prospects of transferring the process to larger-scale controlled bioreactors for future manufacture of RBCs. Using process optimization, we show that hiPSC-MC aggregates can be efficiently differentiated into mature and functional RBCs. We demonstrate the scalability of the process starting from 6 well plates all the way to 500 mL spinner flasks. We show that it is possible to differentiate hiPSC-MC aggregates into high-density cultures of erythroid cells approaching concentrations of 1.7 × 107 cells/mL in spinner flasks. More importantly, we show that functional and transcriptomics evaluation revealed minimal differences between hiPSC-derived RBCs and adult derived RBCs. The scalable agitation suspension culture differentiation process we describe could serve as a platform for developing large-scale blood differentiation processes in controlled bioreactors.","In conclusion, we have developed a suspension agitation culture differentiation process for differentiating hiPSCs toward erythroblasts that can be volumetrically scaled up. hiPSC-derived erythroid cells were capable of undergoing enucleation, had expression of adult beta hemoglobin, albeit at reduced levels (in comparison with adult RBCs), were highly similar to adult-derived erythroid cells at the molecular level, and had similar membrane morphologies and dynamic membrane fluctuations/membrane deformability profiles. Further development would be necessary to adapt our differentiation processes into bioreactors for large-scale generation of high-density functional RBCs.",https://twitter.com/foundmyfitness/status/1347678741015588866,
Plasma Metabolome Profiling of Resistance Exercise and Endurance Exercise in Humans,Resistance exercise results in high lactate which crosses the blood-brain barrier and induces BDNF to improve learning/memory. Endurance training leads to high succinate which boosts energy expenditure in fat.,"The mechanisms by which exercise benefits human health remain incompletely understood. With the emergence of omics techniques, mapping of the molecular response to exercise is increasingly accessible. Here, we perform an untargeted metabolomics profiling of plasma from a randomized, within-subjects, crossover study of either endurance exercise or resistance exercise, two types of skeletal muscle activity that have differential effects on human physiology. A high-resolution time-series analyses reveal shared as well as exercise-mode-specific perturbations in a multitude of metabolic pathways. Moreover, the analyses reveal exercise-induced changes in metabolites that are recognized to act as signaling molecules. Thus, we provide a metabolomic signature of how dissimilar modes of exercise affect the organism in a time-resolved fashion.","With the increased awareness that exercise training can delay and, in some cases, prevent many chronic diseases, research into the biological mechanisms by which exercise improves human health has intensified (Booth et al., 2012). In particular, over the last two decades, there has been a heightened interest in unraveling exercise-induced hormonal-like factors (frequently referred to as “exerkines”) (Thyfault and Bergouignan, 2020), which could be important mediators of the systemic benefits of exercise through autocrine, paracrine, and/or endocrine properties. This work has largely focused on identifying circulating proteins and peptides. It is increasingly clear, however, that metabolites—beyond being fuel biomarkers consequential to phenotypic alterations—also directly modulate many physiological processes (Johnson et al., 2016). Thus, metabolites function as intracellular signal molecules and can regulate activity of enzymes, such as histone deacetylases (HDACs), and kinases, like the mammalian target of rapamycin (mTOR) (Wishart, 2019). In addition, similar to traditional neurotransmitters and hormones, metabolites may act as ligands for cell-surface receptors (Husted et al., 2017). With the recent emergence of untargeted metabolomics analyses, the technology for obtaining a wide-ranging overview of exercise-regulated metabolites is increasingly accessible. In 2010, a landmark paper detected more than 200 plasma metabolites in response to endurance exercise (EE) (Lewis et al., 2010). Subsequently, a series of studies successfully employed metabolomics approaches to identify exercise-induced metabolites such as β-aminoisobutyric acid (Roberts et al., 2014), kynurenic acid (Agudelo et al., 2014), γ-aminobutyric acid (Roberts et al., 2017), and 12,13-diHOME (Stanford et al., 2018), to name a few. Many early studies, however, have been challenged by a limited detection sensitivity and/or have been mostly focusing on EE (Schranner et al., 2020). In contrast, comprehensive omics analyses, including metabolomics, of resistance training are scarce, hindering sufficient comparative understanding of how the two types of exercise affect the organism. This is not ideal, as many athletic disciplines (e.g., basketball, soccer, tennis) consist of a combination of endurance-based and strength-/power-based contractile activation, but also because strength is an important predictor of mortality (Garcia-Hermoso et al., 2018). To begin to fill this knowledge gap and provide a comprehensive resource overview of temporal changes in metabolite pathways that are dependent on exercise mode, we performed untargeted and comparative metabolomics analysis of EE and resistance exercise (RE). Of note, a frequent post-exercise plasma sampling enabled an in-depth mapping of the plasma metabolome dynamics in response to dissimilar exercise interventions.","Taken together, our study provides a comprehensive, time-resolved, metabolomic profiling comparing the responses of acute bouts of RE and EE. The fact that different exercise modes produce dissimilar molecular profiles is well known, yet frequently disregarded. Our data clearly demonstrate that the molecular response to a given exercise intervention must be interpreted in the context of mode and the time course over which the biological changes occur. The translational potential of this work lies in delineating the mechanisms by which the changes in metabolites directly or indirectly govern the physiological adaptations to exercise. Ultimately, an enhanced knowledge of the biology of exercise might be harnessed to improve the treatment options for several chronic diseases.",https://twitter.com/foundmyfitness/status/1346537382296780800,
COVID-19 severity is predicted by earlier evidence of accelerated aging,"Biological age was a better predictor of COVID-19 severity than chronological age. ""Phenoage"" combines epigenetic age with measures of inflammation, metabolic, and immune function.","With no known treatments or vaccine, COVID-19 presents a major threat, particularly to older adults, who account for the majority of severe illness and deaths. The age-related susceptibility is partly explained by increased comorbidities including dementia and type II diabetes [1]. While it is unclear why these diseases predispose risk, we hypothesize that increased biological age, rather than chronological age, may be driving disease-related trends in COVID-19 severity with age. To test this hypothesis, we applied our previously validated biological age measure (PhenoAge) [2] composed of chronological age and nine clinical chemistry biomarkers to data of 347,751 participants from a large community cohort in the United Kingdom (UK Biobank), recruited between 2006 and 2010. Other data included disease diagnoses (to 2017), mortality data (to 2020), and the UK national COVID-19 test results (to May 31, 2020) [3]. Accelerated aging 10-14 years prior to the start of the COVID-19 pandemic was associated with test positivity (OR=1.15 per 5-year acceleration, 95% CI: 1.08 to 1.21, p=3.2×10−6) and all-cause mortality with test-confirmed COVID-19 (OR=1.25, per 5-year acceleration, 95% CI: 1.09 to 1.44, p=0.002) after adjustment for demographics including current chronological age and pre-existing diseases or conditions. The corresponding areas under the curves were 0.669 and 0.803, respectively. Biological aging, as captured by PhenoAge, is a better predictor of COVID-19 severity than chronological age, and may inform risk stratification initiatives, while also elucidating possible underlying mechanisms, particularly those related to inflammaging.","Coronavirus disease 2019 (COVID-19) represents one of the biggest threats to public health in nearly 100 years. While efforts are being undertaken to develop vaccines and antibody  tests for COVID-19, in the interim, there is a critical need for assessing risk stratification and to explore the use of geroscience-guided interventions seeking to improve outcomes by targeting biological aging. Accurately identifying those most at-risk of severe complications or death will facilitate treatment decisions and inform guidelines regarding shelter-in-place and social distancing policies. As such, a major priority is in developing biomarkers that prognostically inform on severity of COVID-19 disease progression.  The risk of fatality and/or severe complications due to COVID-19 infection is strongly age dependent. On March 18, 2020, the United States Center of Disease Control (CDC) projected that persons ages 85 and older have predicted mortality rates of 10-27%, compared to 3-11% for individuals ages 65-84 years, 1-3% for those 55-64 years, and <1% for those 20-54 years of age. All-in-all, those ages 85 and older have a mortality risk that is 100-fold higher than for those under the age of 50, and currently 8 out of 10 COVID-19 deaths in the United States are among adults age 65 or older. In addition to age, the CDC reports that morbidity prevalence—particularly history of diabetes, cardiovascular disease, chronic kidney disease, liver disease, and chronic obstructive pulmonary disease (COPD)—appears to exacerbate risk of death or symptomatic complications. Similar COVID-19 comorbidities were reported in other countries, e.g., UK, China, and Italy. Previous studies have predicted COVID-19 outcomes (pneumonia and mortality) using hospital inpatient data including demographics, signs and symptoms, clinical biomarkers, and imaging features. The performance in terms of C-statistic/index or area under the receiver operating characteristic (ROC) curve (AUC) was generally over 90% but subject to bias and overfitting. One study predicted hospital admission related to upper respiratory infections (pneumonia, influenza, acute bronchitis, etc.), proxy events of COVID-19, using over 500 diagnosis features from thousands of general population samples. The resulting AUCs were 70 to 80% but may not be generalized for COVID-19 as risk factors for COVID-19 and for other respiratory infections are not the same. In recent years, we have developed and widely validated several biomarkers of aging that strongly predict morbidity and mortality risk, in both short-term (1 year) and long-term (25+ years) follow-up. Based on these observed trends, we hypothesize that biological aging (rather than chronological age) is a strong determinant of symptom severity following COVID-19 infection. We aimed to assess the risk and predictive performance of accelerated aging for COVID19 severe infection using a biological age measure, named phenotypic age (PhenoAge). PhenoAge was trained using 42 biomarkers as inputs into a supervised machine learning model to predict allcause mortality.We applied this measure to biomarker data from 2006 to 2010 of participants from a large community cohort, United Kingdom Biobank (UKB). Combined with information on disease diagnoses updated to 2017, we tested whether PhenoAge was predictive of COVID-19 severity based on mortality data and COVID-19 test results recently linked from the UK National Health Service.","In conclusion, accelerated aging measured by PhenoAge was associated with both COVID-19 severity outcomes, with adjustment for demographics including current chronological age, and disease comorbidities. Accelerated PhenoAge in combination with demographics and disease comorbidities produced positive predicted values greater than the sample test positivity rate and all-cause mortality rate after COVID-19 infection. Accelerated aging by PhenoAge is largely characterized by inflammaging, suggested by the composition of biomarkers, the shared association with COVID-19 severity between PhenoAge and multiple diseases, and our prior gene enrichment analysis results of accelerated PhenoAge. Targeting the potential mechanisms underlying inflammaging may reduce COVID-19 severity. ",https://twitter.com/foundmyfitness/status/1334606271010406406,
"Total and added sugar intakes, sugar types, and cancer risk: results from the prospective NutriNet-Santé cohort","Total sugar intake is associated with a higher overall cancer risk independent of body fat. The associations were more pronounced for sucrose, non–fruit-derived sugars, and added and natural sugars present in sugary drinks.","Excessive sugar intake is now recognized as a key risk factor for obesity, type 2 diabetes, and cardiovascular diseases. In contrast, evidence on the sugar–cancer link is less consistent. Experimental data suggest that sugars could play a role in cancer etiology through obesity but also through inflammatory and oxidative mechanisms and insulin resistance, even in the absence of weight gain. The objective was to study the associations between total and added sugar intake and cancer risk (overall, breast, and prostate), taking into account sugar types and sources. In total, 101,279 participants aged >18 y (median age, 40.8 y) from the French NutriNet-Santé prospective cohort study (2009–2019) were included (median follow-up time, 5.9 y). Sugar intake was assessed using repeated and validated 24-h dietary records, designed to register participants’ usual consumption for >3500 food and beverage items. Associations between sugar intake and cancer risk were assessed by Cox proportional hazard models adjusted for known risk factors (sociodemographic, anthropometric, lifestyle, medical history, and nutritional factors). Total sugar intake was associated with higher overall cancer risk (n = 2503 cases; HR for quartile 4 compared with quartile 1: 1.17; 95% CI: 1.00, 1.37; Ptrend = 0.02). Breast cancer risks were increased (n = 783 cases; HRQ4vs.Q1 = 1.51; 95% CI: 1.14, 2.00; Ptrend = 0.0007). Results remained significant when weight gain during follow-up was adjusted for. In addition, significant associations with cancer risk were also observed for added sugars, free sugars, sucrose, sugars from milk-based desserts, dairy products, and sugary drinks (Ptrend ≤ 0.01). These results suggest that sugars may represent a modifiable risk factor for cancer prevention (breast in particular), contributing to the current debate on the implementation of sugar taxation, marketing regulation, and other sugar-related policies.","Despite public health recommendations and dietary guidelines of the WHO advising to limit “free sugars” [including both added and natural sugars present in fruit juices, honey, and syrups] up to 5% of calories, consumption remains excessive in most occidental countries. For instance, in the United States, 13% of total energy is brought by free sugars, and in France, sugars from sugary products contribute to 11% of energy intake and 20% of adults exceed 100 g of sugar per day [i.e., the official recommendation].While the associations of sugars on cardiometabolic outcomes have been extensively studied, associations with cancer risk have been much less investigated. Given the limited and inconsistent existing literature, the World Cancer Research Fund and American Institute for Cancer Research (WCRF/AICR) concluded, in their latest report, that the evidence base was not sufficient for a sugar and cancer association. Similarly, Makarem et al. recently performed a systematic review on sugars and related exposures and overall and site-specific cancers but concluded that, despite strong mechanistic plausibility, epidemiological data were still limited. Pancreatic and other gastrointestinal cancers have been the main cancer sites studied so far. In contrast, few studies have explored the associations between sugars and breast and prostate cancers, which are the most prevalent cancers in women and men, respectively, in several countries, and even fewer studies considered added sugars in relation with breast and prostate cancers. Literature on added sugars is even more limited due to methodologic limitations of dietary assessment tools in many cohort studies (i.e., FFQs), which do not adequately capture added sugar intake. To better understand the etiologic role of sugars in carcinogenesis and refine nutritional recommendations concerning sugar intakes, it is important to identify types and sources of sugars more specifically involved. Several studies have investigated associations between sugary food groups, considering the quantity eaten (in grams of food per day) and cancer risk, but none specifically studied sugar intake from these specific sugary food groups (in grams of sugars from food groups per day). Sugary beverages, which contribute to a substantial part of sugar intake in Western countries, have been the most investigated source, and several studies suggest an increased risk of various cancer sites. In particular, within the NutriNet-Santé cohort, we recently showed that sugary drinks were associated with higher overall and breast cancer risk. Some studies have identified associations for the different types of individual sugars and cancer risk, in particular, for fructose, glucose, and sucrose, but results were conflicting. Mechanistic plausibility supports sugar involvement in carcinogenesis. Excessive sugar intake is a known risk factor for adiposity, obesity, and cardiometabolic perturbations, which are, in turn, established risk factors for different cancer sites. It has also been suggested that sugars are associated with other risk factors for cancer, such as oxidative stress and inflammation or activation of the insulin pathway causing insulin resistance, even in the absence of weight gain. Thus, there is a knowledge gap for types of sugars and sources and their link with cancer risk. Existing evidence is conflicting, and much of it comes from US cohorts, which may be different from European populations. Given the policy and public health implications, it is important to establish a stronger evidence base. Our objective was to investigate the associations between dietary intakes of sugars (naturally present and added) and their food sources with overall cancer risk and risk of the most common nonskin cancers in men and women.","In conclusion, in this large population-based prospective cohort, higher sugar intake was associated with overall cancer and, more specifically, breast cancer risk, independently of weight gain and weight status. These findings need to be replicated in other large-scale prospective studies (notably involving more overweight/obese participants) and supported by experimental data to clarify underlying mechanisms. Nonetheless, they suggest that sugars may represent a modifiable risk factor for cancer prevention. Globally, the implementation of tax on sugary drinks and foods, as well as other sugar-related policies (e.g., the regulation of food advertising and marketing, the fixation of reference standards limiting sugar content according to product categories), is currently debated. In a context where sugar consumption is increasing in Western countries, and adding to its well-established cardiometabolic detrimental effects, these results contribute to building the evidence base suggesting that public health policies addressing sugar intake should also consider their role in cancer etiology.",https://twitter.com/foundmyfitness/status/1329512552603086848,
A network medicine approach to investigation and population-based validation of disease manifestations and drug repurposing for COVID-19,"A network medicine approach identified melatonin as a possible therapeutic for COVID-19. Melatonin use was also associated with a ~30% reduced risk of COVID-19 infection after adjusting for age, sex, race, smoking, and comorbidities.","The global coronavirus disease 2019 (COVID-19) pandemic, caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has led to unprecedented social and economic consequences. The risk of morbidity and mortality due to COVID-19 increases dramatically in the presence of coexisting medical conditions, while the underlying mechanisms remain unclear. Furthermore, there are no approved therapies for COVID-19. This study aims to identify SARS-CoV-2 pathogenesis, disease manifestations, and COVID-19 therapies using network medicine methodologies along with clinical and multi-omics observations. We incorporate SARS-CoV-2 virus–host protein–protein interactions, transcriptomics, and proteomics into the human interactome. Network proximity measurement revealed underlying pathogenesis for broad COVID-19-associated disease manifestations. Analyses of single-cell RNA sequencing data show that co-expression of ACE2 and TMPRSS2 is elevated in absorptive enterocytes from the inflamed ileal tissues of Crohn disease patients compared to uninflamed tissues, revealing shared pathobiology between COVID-19 and inflammatory bowel disease. Integrative analyses of metabolomics and transcriptomics (bulk and single-cell) data from asthma patients indicate that COVID-19 shares an intermediate inflammatory molecular profile with asthma (including IRAK3 and ADRB2). To prioritize potential treatments, we combined network-based prediction and a propensity score (PS) matching observational study of 26,779 individuals from a COVID-19 registry. We identified that melatonin usage (odds ratio [OR] = 0.72, 95% CI 0.56–0.91) is significantly associated with a 28% reduced likelihood of a positive laboratory test result for SARS-CoV-2 confirmed by reverse transcription–polymerase chain reaction assay. Using a PS matching user active comparator design, we determined that melatonin usage was associated with a reduced likelihood of SARS-CoV-2 positive test result compared to use of angiotensin II receptor blockers (OR = 0.70, 95% CI 0.54–0.92) or angiotensin-converting enzyme inhibitors (OR = 0.69, 95% CI 0.52–0.90). Importantly, melatonin usage (OR = 0.48, 95% CI 0.31–0.75) is associated with a 52% reduced likelihood of a positive laboratory test result for SARS-CoV-2 in African Americans after adjusting for age, sex, race, smoking history, and various disease comorbidities using PS matching. In summary, this study presents an integrative network medicine platform for predicting disease manifestations associated with COVID-19 and identifying melatonin for potential prevention and treatment of COVID-19.","The ongoing global coronavirus disease 2019 (COVID-19) pandemic has led to 38 million confirmed cases and 1 million deaths worldwide as of October 14, 2020. The United States alone has recorded nearly 8 million confirmed cases, with a death toll of more than 216,000. Several retrospective studies have reported the clinical characteristics of individuals with symptomatic COVID-19, and an emerging theme has been the significantly higher risk of morbidity and mortality among individuals with 1 or more comorbid health conditions, such as hypertension, asthma, diabetes mellitus, cardiovascular or cerebrovascular disease, chronic kidney disease, and malignancy. However, these retrospective clinical studies are limited by small sample sizes and unmeasured confounding factors, leaving the underlying patho-mechanisms largely unknown. More specifically, it is unclear whether associations of disease manifestations and COVID-19 severity are merely a reflection of poorer health in general, or a clue to shared pathobiological mechanisms. Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes COVID-19, is an enveloped virus that carries a single-stranded positive-sense RNA genome . SARS-CoV-2 is a newly discovered member of the coronavirus (CoV) family SARS-CoV-2 enters host cells via binding of its spike protein to the angiotensin converting enzyme 2 (ACE2) receptor on the surfaces of many cell types . This binding is primed by transmembrane protease serine 2 (TMPRSS2) and the host cell protease furin . Studies have shown that ACE2 and TMPRSS2 are highly co-expressed in alveolar type II (AT2) epithelial cells in the lung , nasal mucosa , bronchial secretory cells , and absorptive enterocytes in the ileum . Yet, much remains to be learned about how these critical human proteins involved in the infection and replication of SARS-CoV-2 are associated with various disease comorbidities and complications. Systematic identification of the host factors involved in the protein–protein interactions (PPIs) of SARS-CoV-2 and the human host will facilitate identification of drug targets and advance understanding of the complications and comorbidities resulting from COVID-19 . Studies using transcriptomics , proteomics , and interactomics (PPIs) methods  have contributed to a better understanding of the SARS-CoV-2–host interactome, which has enabled the investigation of the complications and comorbidities of SARS-CoV-2 and a facilitated search for effective treatment. Major efforts are underway to develop safe and effective drugs to treat COVID-19: Preventive and therapeutic strategies currently being explored include vaccination, SARS-CoV-2-specific antibodies, novel nucleoside analogs such as remdesivir, and repurposed drugs. Remdesivir, an agent originally developed for treatment of Ebola virus, was reported to shorten the time to recovery in adults who were hospitalized with COVID-19; yet, a 10-day course of remdesivir did not show a statistically significant difference in clinical status compared with standard care for patients with moderate COVID-19. Dexamethasone, an FDA-approved glucocorticoid receptor (GR) agonist, has been shown to reduce mortality by one-third in hospitalized COVID-19 patients requiring ventilation and by one-fifth in individuals requiring oxygen ; yet, dexamethasone did not reduce death in COVID-19 patients not receiving respiratory support . Many existing drugs are currently being or have been tested in clinical trials, such as the antimalarial drug hydroxychloroquine and protease inhibitor combination lopinavir/ritonavir; results from these trials have not yet shown significant clinical benefits for COVID-19 patients. We recently evaluated nearly 3,000 FDA-approved/investigational drugs using a network-based method and prioritized 16 drug candidates and 3 drug combinations for COVID-19 . Yet, the answer to the key question of why an approved drug originally documented for other diseases might be beneficial for COVID-19 remains unclear. One possible explanation is that COVID-19 shares common disease pathobiology or functional pathways elucidated by the human PPIs . Systematic identification of common disease pathobiological pathways shared by COVID-19 and other diseases would offer novel targets and therapies for COVID-19. In this study, we present an integrative network medicine platform that quantifies the association of COVID-19 with other diseases across 6 categories, including autoimmune, malignant cancer, cardiovascular, metabolic, neurological, and pulmonary. The rationale for these analyses rests on the notions that (1) the proteins that functionally associate with a disease (such as COVID-19) are localized in the corresponding subnetwork within the comprehensive human PPI network and (2) proteins that are associated with a specific disease may be directly targeted by the virus or are in the close vicinity of the target host proteins. We first performed network analysis followed by single-cell RNA sequencing (RNA-Seq) data analysis to identify the underlying pathobiological relationships between COVID-19 and its associated comorbidities. Additionally, we use our network medicine findings and patient data from a large COVID-19 patient registry database to identify and prioritize existing FDA-approved drugs as potential COVID-19 drug candidates.","In conclusion, our study provides a powerful, integrative network medicine strategy for advancing understanding of COVID-19-associated comorbidities and facilitating the identification of drug candidates for COVID-19. This approach also promises to address the translational gap between genomic studies and clinical outcomes, which poses a significant problem when rapid development of effective therapeutic interventions is critical during a pandemic. From a translational perspective, if broadly applied, the network medicine tools applied here could prove helpful in developing effective treatment strategies for other complex human diseases as well, including other emerging infectious diseases",https://twitter.com/foundmyfitness/status/1328098321357959169,
Cytotoxic T-cells mediate exercise-induced reductions in tumor growth,Exercise slowed the growth of cancer and reduced cancer mortality by increasing plasma lactate which boosted the activity of cytotoxic T-cells in mice. Exercise had the same effect on lactate and cytotoxic T-cell activity in humans.,"Exercise has a wide range of systemic effects. In animal models, repeated exertion reduces malignant tumor progression, and clinically, exercise can improve outcome for cancer patients. The etiology of the effects of exercise on tumor progression are unclear, as are the cellular actors involved. We show here that in mice, exercise-induced reduction in tumor growth is dependent on CD8+ T cells, and that metabolites produced in skeletal muscle and excreted into plasma at high levels during exertion in both mice and humans enhance the effector profile of CD8+ T-cells. We found that activated murine CD8+ T cells alter their central carbon metabolism in response to exertion in vivo, and that immune cells from trained mice are more potent antitumor effector cells when transferred into tumor-bearing untrained animals. These data demonstrate that CD8+ T cells are metabolically altered by exercise in a manner that acts to improve their antitumoral efficacy.","In humans, exercising cohorts have lower rates of cancer incidence (Moore et al., 2016) and better outcomes across a range of cancer diagnoses (Cormie et al., 2017; Friedenreich et al., 2016), proportionate to the degree and intensity of exercise. The mechanisms underlying these observations have remained elusive, although recent work has indicated a relationship between immune response and exercise-induced changes in malignant progression (Pedersen et al., 2016; Koelwyn et al., 2017). The metabolic demands of strenuous physical exertion generally induce significant changes in nutrient utilization, principally via central carbon metabolism (Brooks, 1998). These exercise-induced alterations in metabolism change the ratios of energy substrates utilized, and can shift intramuscular metabolite profiles. These shifts are reflected in systemic metabolite availability, which in turn modifies energy production throughout the body (Henderson et al., 2004; Lezi et al., 2013; El Hayek et al., 2019). It is clear that cytotoxic T cells play a crucial role in controlling tumor growth. By recognizing mutation-derived neoantigens, T cells can identify and eliminate malignant cells in a process known as immunosurveillance (Dunn et al., 2002; Dunn et al., 2004). Escape from immune control is a critical step toward progressive malignant growth in many cancers, and tumors achieve this in a number of ways, amongst them the dampening of antitumor T cell responses (Dunn et al., 2002; Dunn et al., 2004; Beatty and Gladney, 2015; Zappasodi et al., 2018). The activity of immune cells is tightly linked with their metabolism (O'Neill et al., 2016; Pearce and Pearce, 2013). Many aspects of immune cell energetics are likely sensitive to the metabolic changes induced by exercise (Henderson et al., 2004). Exercise is known to affect immune cell function, and an altered immune response has been suggested as a mechanism underlying effects of exercise on cancer risk and progression (Christensen et al., 2018). In this study, we investigate the association between exercise, tumor growth, and CD8+ T-cell function. To address this, we undertook studies of exercise-induced changes in tumor progression, and asked what metabolites are released in response to exercise; as well as whether metabolites produced by exercise can alter cytotoxic T-cell function. We found that exercise itself can modify cytotoxic T-cell metabolism, and that exercise-induced effects on tumor growth are dependent on cytotoxic T-cell activity.","In conclusion, we have shown that the antitumoral effects of exercise depend on CD8+ T-cells, and that intense exertion can alter the intrinsic metabolism and antitumoral effector function of cytotoxic T-cells. This indicates that, in the physiological setting, exercise-derived metabolites, whether systemically delivered or draining into an adjacent lymph node, could act to boost a nascent T cell response. This indicates that the adaptive immune system is a key component of exercise-induced suppression of tumorigenesis.",https://twitter.com/foundmyfitness/status/1327326722425962496,
Resveratrol and exercise combined to treat functional limitations in late life: A pilot randomized controlled trial,"A small pilot study found that exercise + resveratrol resulted in an increase of 449 meters in a 6-minute walk test and increased levels of citrate synthase, a common marker of mitochondrial volume in elderly people.","To evaluate the safety and feasibility of combining exercise (EX) and resveratrol to treat older adults with physical function limitations. Three-arm, two-site pilot randomized, controlled trial (RCT) for community-dwelling adults (N = 60), 71.8 ± 6.3 years of age with functional limitations. Participants were randomized to receive either 12 weeks of (1) EX + placebo [EX0], (2) EX + 500 mg/day resveratrol [EX500], or (3) EX + 1,000 mg/day resveratrol [EX1000]. EX consisted of two sessions a week for 12 weeks of center-based walking and whole-body resistance training. Safety was assessed through adverse events and feasibility through exercise session and supplement (placebo, or resveratrol) protocol adherence. Outcome measures included a battery of indices of physical function as well as skeletal muscle mitchondrial function. Data were adjusted for age and gender using the Intent-To-Treat approach. Adverse event frequency and type were similar between groups (n=8 EX0, n=12 EX500, and n=7 EX1000). Overall, 85% of participants met the supplement adherence via pill counts while 82% met the exercise session adherence. Adjusted within group mean differences (95% confidence interval) from week 0 to 12 for gait speed ranged from −0.04 (EX0: −0.1, 0.03) m/s to 0.04 (EX1000: −0.02, 0.11) and the six-minute walk test mean differences were 9.45 (EX0: −9.02, 27.7), 22.9 (EX500: 4.18, 41.6), and 33.1 (EX1000: 13.8, 52.4) meters. Unadjusted mean differences for citrate synthase were −0.80 (EX0: −15.45, 13.84), −1.38 (EX500: −12.16, 9.39), and 7.75 (EX1000: −4.68, 20.18) mU/mg. COX activity mean within group changes ranged from −0.05 (EX0) to 0.06 (EX500) k/sec/mg. Additional outcomes are detailed in the text. The pilot RCT indicated that combined EX + resveratrol was safe and feasible for older adults with functional limitations and may improve skeletal muscle mitochondrial function and mobility-related indices of physical function. A larger trial appears warranted and is needed to formally test these hypotheses.","Maintaining physical function during the aging process is critical to ward off disability and extend independence. However, despite the well-established relationship of decreased physical function with subsequent adverse health outcomes, few therapeutic strategies exist that are shown to improve these outcomes among older adults. To date, interventions incorporating physical exercise have demonstrated the most consistent results for improving physical function among older adults. Still, despite the general benefits of exercise, physiologic responses to exercise can be quite variable – thus adjuvant strategies may be useful to enhance the efficacy of exercise. One potential such adjuvant is the nutritional supplement resveratrol, a compound commonly found in red grapes. Resveratrol is a natural polyphenol with purported anti-oxidant, anti-inflammatory, and metabolic benefits. Preclinical studies have suggested greater potential benefits in combining exercise with resveratrol supplementation compared to exercise alone, but existing data from humans are less supportive. Still, the literature in humans remains sparse and novel interventions are needed to enhance the efficacy of exercise among older adults with physical limitations. Accordingly, the current pilot RCT expands on the limited literature to evaluate dose-dependent effects of resveratrol combined with physical exercise for older adults with functional limitations with an emphasis on safety and feasibility. As outlined previously, we designed a pilot, randomized controlled trial (RCT) to begin to test the hypothesis that resveratrol may potentiate the functional benefits of exercise among older adults at least partly by enhancing beneficial adaptations in skeletal muscle mitochondrial function in response to chronic exercise. The objective of this pilot trial was to collect data on the safety, feasibility, and potential efficacy of resveratrol supplementation combined with exercise training on indices of physical function and skeletal muscle mitochondrial function among older adults with functional limitations. The present manuscript provides findings from the pilot study designed to support or refute the need for a larger-scale trial necessary to fully test our central hypothesis. Notably, due to additional resources and tissue availability, were able to expand our initial battery of mitochondrial measures (mitochondrial DNA copy number, citrate synthase, cytochrome C oxidase) to include a more comprehensive evaluation of skeletal muscle mitochondrial function including indices of mitochondrial damage, fusion, fission, transcriptional regulation, as well as vascular inflammation and oxidative stress. These data provide important biologic insight, in addition to the clinical measures, to aid in evaluation of the potential need for a future, larger-scale trial in this area.","Aging of the worldwide population will have a massive burden of clinical and economic costs associated with age-related physical function. Moreover, declines in physical function occur with age and disablement. Research studies have demonstrated that exercise is a needed, yet insufficient, component of interventions to prevent age-related physical function. Therefore, it is critical to identify adjuvant therapies designed to optimize the efficacy of exercise which could ultimately have a positive impact on reducing functional limitations. Our pilot RCT has concluded that combined EX + resveratrol can be a safe and feasible for older adults with functional limitations. Moreover, EX + 1,000 mg/d resveratrol may have benefits for physical function and mitochondrial function. Ultimately, a fully powered trial is necessary to understand the effects of exercise and resveratrol combined for older adults with functional limitations.",https://twitter.com/foundmyfitness/status/1322611900815876097,
Enhanced physical and cognitive performance in active duty Airmen: evidence from a randomized multimodal physical fitness and nutritional intervention,"Either exercise alone or a nutritional supplement (protein, omega-3, vitamin D and other micronutrients) improved strength, endurance, mobility, stability, and cognitive function after 12 weeks in active duty Air Force airmen.","Achieving military mission objectives requires high levels of performance from Airmen who operate under extreme physical and cognitive demands. Thus, there is a critical need to establish scientific interventions to enhance physical fitness and cognitive performance—promoting the resilience of Airmen and aiding in mission success. We therefore conducted a comprehensive, 12-week randomized controlled trial in active-duty Air Force Airmen (n = 148) to compare the efficacy of a multimodal intervention comprised of high-intensity interval aerobic fitness and strength training paired with a novel nutritional supplement [comprised of β-hydroxy β-methylbutyrate (HMB), lutein, phospholipids, DHA and selected micronutrients including B12 and folic acid] to high-intensity interval aerobic fitness and strength training paired with a standard of care placebo beverage. The exercise intervention alone improved several dimensions of physical fitness [strength and endurance (+ 7.8%), power (+ 1.1%), mobility and stability (+ 18.3%), heart rate (− 1.3%) and lean muscle mass (+ 1.1%)] and cognitive function [(episodic memory (+ 19.9%), processing efficiency (+ 4.6%), executive function reaction time (− 5.8%) and fluid intelligence accuracy (+ 11.0%)]. Relative to exercise training alone, the multimodal fitness and nutritional intervention further improved working memory (+ 11.2%), fluid intelligence reaction time (− 6.2%), processing efficiency (+ 4.3%), heart rate (− 2.3%) and lean muscle mass (+ 1.6%). These findings establish the efficacy of a multimodal intervention that incorporates aerobic fitness and strength training with a novel nutritional supplement to enhance military performance objectives and to provide optimal exercise training and nutritional support for the modern warfighter.","Adaptive military operations are a hallmark of modern warfare and depend on the capacity for flexible, resilient behavior. Airmen face extreme physical and mental demands that require the capacity to overcome the negative physical and psychological effects of operating in challenging field environments characterized by “volatile, uncertain, complex, and ambiguous” events1. A primary aim of research in the nutritional sciences is therefore to establish and validate nutritional interventions for the modern warfighter that are designed to provide optimal nutritional support and supplementation for body and mind. There is growing interest within the United States military to understand the effects of nutritional factors on warfighter resilience examined with respect to both physical and cognitive performance.Challenging field environments can prevent warfighters from realizing their optimal physical and cognitive performance. Participants with higher levels of stress and anxiety in military survival training exercises have lower scores on the Army Physical Readiness Test. Acute stress in special operation soldiers may impair visuo-spatial capacity and working memory due to high dopamine and norepinephrine turnover in the prefrontal cortex, which is known to impair cognition and spatial working memor. Surveys among military populations indicate that most warfighters use multivitamins, protein supplements or sports bars/drinks with a perception that these supplements improve their performance. However, the efficacy of many of these supplements is not supported by randomized controlled trials. Caffeine benefits executive function in sleep-deprived individuals; but the effects are acute and have negative side effects in some individuals. Recent evidence further suggests that some supplements may contain dangerous or illegal substances that can have unknown consequences. Thus, an enduring aim of research in the psychological and brain sciences is to establish nutritional supplements to enhance physical fitness and cognitive performance in active-duty military populations. Accumulating evidence suggests that nutrients found in the Mediterranean diet promote brain and cognitive health. The Mediterranean diet includes olive oil as a source of monounsaturated fatty acids and polyphenols, fish and salmon that deliver phospholipids, omega-3 polyunsaturated fatty acids, lutein and vitamin D, and fruits and vegetables that provide antioxidants and vitamins B, C and E, carotenoids including lutein, folate, and polyphenols. Elements of the Mediterranean diet are known to promote healthy vasculature by way of reducing total cholesterol and low-density lipoproteins (i.e., “bad cholesterol”), and improving endothelial function. Broadly, these actions lower the risk of vascular comorbidities, dyslipidemia, hypertension, and coronary artery disease. Within the brain, these actions are linked to a reduction in white matter lesions and promotion of white matter microstructure. Furthermore, improvements in white matter brain structure are known to enhance cognitive performance (e.g., on measures of executive function and working memory). Thus, combinations of these nutrients in the form of a novel nutritional supplement have the potential to improve the protective vascular, metabolic, antioxidant, and anti-inflammatory mechanisms promoted by individual nutrients and therefore to enhance brain function and cognition.Physical activity interventions improve cognition and the brain in multiple animal models, including rodents, dogs and monkeys. Specifically, these models have demonstrated that exercise interventions result in (1) hippocampal neurogenesis; (2) increased synaptic activity in the brain; (3) growth of new blood vessels; (4) increased concentrations of brain derived neurotrophic factor (BDNF); (5) reduction of neurodegeneration; and (6) enhanced learning and memory. The results from these animal models provide the basis for human studies of physical activity, fitness, and exercise. A meta-analysis of exercise interventions in humans revealed a moderate effect size for exercise leading to better cognition. The meta-analysis showed that exercise improved a variety of cognitive tasks; but the largest effects were observed in tasks that engage the central executive network, including planning, problem solving, and working memory. Finally, the meta-analysis demonstrated that the combined effects of aerobic exercise, power and flexibility training had a greater cognitive benefit compared to only aerobic exercise training. Motivated by these parallel lines of research, the present study examined the efficacy of a multi-modal fitness and nutritional intervention to enhance cognitive performance and physical fitness in United States Air Force Airmen. Specifically, we investigated: (1) whether a unimodal exercise training protocol enhanced fitness and cognition and (2) whether a multimodal fitness plus nutritional intervention resulted in fitness and cognitive gains beyond those conferred by the unimodal intervention.","In conclusion, the current study supports the efficacy of a multimodal fitness and nutritional intervention to improve both physical fitness and cognition in Air Force Airmen. Moreover, these changes in the body and brain align with corresponding changes in blood-based biomarkers of nutrition. Our study also demonstrates a wider range of improvements and larger effect sizes due to multimodal training compared to unimodal fitness training alone. While the findings are based on a large sample of Air Force Airmen and demonstrate that physical fitness and cognitive enhancement is achievable, the multimodal lifestyle intervention documented in this study could easily be implemented in other real-world contexts to optimize human performance.",https://twitter.com/foundmyfitness/status/1320799175567499264,
Association of Coffee Intake With Survival in Patients With Advanced or Metastatic Colorectal Cancer,"Dose-dependant association between coffee consumption and colon cancer mortality. People with metastatic colorectal cancer who drank 2-3 cups per day were 18% less likely to die, and those who drank 4 cups per day were 36% less likely to die.","Several compounds found in coffee possess antioxidant, anti-inflammatory, and insulin-sensitizing effects, which may contribute to anticancer activity. Epidemiological studies have identified associations between increased coffee consumption and decreased recurrence and mortality of colorectal cancer. The association between coffee consumption and survival in patients with advanced or metastatic colorectal cancer is unknown. To evaluate the association of coffee consumption with disease progression and death in patients with advanced or metastatic colorectal cancer. This prospective observational cohort study included 1171 patients with previously untreated locally advanced or metastatic colorectal cancer who were enrolled in Cancer and Leukemia Group B (Alliance)/SWOG 80405, a completed phase 3 clinical trial comparing the addition of cetuximab and/or bevacizumab to standard chemotherapy. Patients reported dietary intake using a semiquantitative food frequency questionnaire at the time of enrollment. Data were collected from October 27, 2005, to January 18, 2018, and analyzed from May 1 to August 31, 2018. Consumption of total, decaffeinated, and caffeinated coffee measured in cups per day. Overall survival (OS) and progression-free survival (PFS).  Among the 1171 patients included in the analysis (694 men [59%]; median age, 59 [interquartile range, 51-67] years). The median follow-up time among living patients was 5.4 years (10th percentile, 1.3 years; IQR, 3.2-6.3 years). A total of 1092 patients (93%) had died or had disease progression. Increased consumption of coffee was associated with decreased risk of cancer progression (hazard ratio [HR] for 1-cup/d increment, 0.95; 95% CI, 0.91-1.00; P = .04 for trend) and death (HR for 1-cup/d increment, 0.93; 95% CI, 0.89-0.98; P = .004 for trend). Participants who consumed 2 to 3 cups of coffee per day had a multivariable HR for OS of 0.82 (95% CI, 0.67-1.00) and for PFS of 0.82 (95% CI, 0.68-0.99), compared with those who did not drink coffee. Participants who consumed at least 4 cups of coffee per day had a multivariable HR for OS of 0.64 (95% CI, 0.46-0.87) and for PFS of 0.78 (95% CI, 0.59-1.05). Significant associations were noted for both caffeinated and decaffeinated coffee. Coffee consumption may be associated with reduced risk of disease progression and death in patients with advanced or metastatic colorectal cancer. Further research is warranted to elucidate underlying biological mechanisms.","Despite advances in treatment, colorectal cancer (CRC) remains a deadly disease in the United States. Abundant experimental and epidemiological data support a link between dietary and other lifestyle factors and the incidence and mortality of this disease. One such factor that has garnered increasing interest is the consumption of coffee, which possesses antineoplastic properties in the laboratory and may play a role in CRC development and progression. The potential for coffee to slow the growth of cancer may be related to coffee’s ability to decrease blood insulin levels by sensitizing tissues to the effects of insulin, because insulin-resistant states have been associated with worse CRC outcomes. Alternatively, coffee’s anticancer effects may be related to biologically active constituents of coffee that have been shown to have antioxidant, anti-inflammatory, and antiangiogenic effects.Recent epidemiological studies found that higher coffee intake was associated with improved survival in patients with stage III CRC.However, the association between coffee consumption and survival of patients with metastatic CRC is unknown. A significant number of patients with CRC may ultimately develop metastatic disease, and treatment for this group is palliative, with a 5-year survival of 14%. Therefore, identifying novel treatment strategies to improve the outcomes of patients with metastatic CRC is of particular research and clinical importance. We conducted a prospective study evaluating the association of coffee consumption with overall survival (OS) and progression-free survival (PFS) in patients with advanced or metastatic CRC who were enrolled in a National Cancer Institute–sponsored, multi-institutional phase 3 randomized clinical trial of combined cytotoxic and biologic therapy. We hypothesized that increased coffee consumption is associated with longer survival in patients who were starting first-line chemotherapy for CRC.","This large cohort study detected an association between increased consumption of coffee and improved CRC outcomes. These findings are consistent with those of previous epidemiological studies, although this is the first such study, to our knowledge, to show a protective effect of coffee consumption in patients with advanced or metastatic CRC. Further research is needed to elucidate the specific mechanism driving these associations.",https://twitter.com/foundmyfitness/status/1310660431636094976,
Improved metabolic function and cognitive performance in middle-aged adults following a single dose of wild blueberry,A wild blueberry extract improved memory and executive function in middle-aged adults compared to a placebo powder with similar macronutrient content. Similar cognitive improvements were found in children and older adults given blueberry powder.,"Research has demonstrated cognitive benefits following acute polyphenol-rich berry consumption in children and young adults. Berry intake also has been associated with metabolic benefits. No study has yet examined cognitive performance in middle-aged adults. We investigated the relationships among cognitive and metabolic outcomes in middle-aged adults following wild blueberry (WBB) consumption. Thirty-five individuals aged 40–65 years participated in a randomized, double blind, cross-over study. Participants consumed a breakfast meal and 1-cup equivalent WBB drink or matched placebo beverage on two occasions. Participants completed cognitive tasks and had blood drawn before and at regular intervals for 8 h after each meal/treatment. Changes in episodic memory and executive function (EF) were assessed alongside plasma levels of glucose, insulin, and triglyceride. Analysis of the memory-related Auditory Verbal Learning Task (AVLT) word recognition measure revealed a decrease in performance over the test day after placebo intake, whereas performance after WBB was maintained. For the AVLT word rejection measure, participants identified more foils following WBB in comparison to placebo. Benefits were also observed for EF on the Go/No-Go task with fewer errors following WBB intake on cognitively demanding invalid No-Go trials in comparison to placebo. Furthermore, in comparison to placebo, response times were faster for the Go/No-Go task, specifically at 4 h and 8 h following WBB treatment. We also observed reduced post-meal glucose and insulin, but not triglyceride, concentrations in comparison to placebo over the first 2 h following ingestion. Though the addition of Age, BMI, glucose and insulin as covariates to the analysis reduced the significant effect of beverage for AVLT word rejection, metabolic outcomes did not interact with treatment to predict cognitive performance with the exception of one isolated trend.This study indicated acute cognitive benefits of WBB intake in cognitively healthy middle-aged individuals, particularly in the context of demanding tasks and cognitive fatigue. WBB improved glucose and insulin responses to a meal. Further research is required to elucidate the underlying mechanism by which WBB improves cognitive function.","There is a substantial body of evidence demonstrating an association between habitual consumption of foods high in flavonoids and cognitive benefits including delayed cognitive decline with ageing. In addition, evidence from controlled intervention trials corroborate these findings, showing that supplementation with flavonoid-rich foods produce improvements in cognitive performance. The majority of human berry trials have investigated the effects of supplementation for periods of several weeks , although recent data suggest that effects of other flavonoid-rich food, such as cocoa, on brain function and neurocognitive function can occur within hours of consumption. Likewise, in school-aged children, whole fruit blueberry powder delivered in a smoothie/juice drink was associated with improvements in executive function and memory performance 2–6 h following intake. In young adults, a smoothie of equal blueberry, strawberry, raspberry, and blackberry, was associated with improvements in executive function, again 2–6 h following intake. Furthermore, in older adults, global cognitive function was found to decline relative to baseline at 2 h following a control beverage whereas performance was maintained for a flavonoid-rich blueberry beverage. Importantly, these improvements in cognitive performance were demonstrated following berry intake at intervals similar to plasma peaks of blueberry anthocyanins and their metabolites 1–3 h after ingestion, as well as plasma peaks of different phenolic acid metabolites 2–3 h and 5 h post-consumption. Related to these timescales, reduced postprandial insulin (1–3 h) and attenuated postprandial inflammation have been shown to occur for up to 10 h in middle-aged, overweight and obese individuals consuming strawberries with a typical Western meal, and in a younger overweight group consuming strawberries 2 h before the meal, respectively. Similarly, red raspberry intake with a high carbohydrate breakfast meal reduced postprandial glycaemia and the concomitant insulin demand in overweight or obese individuals with pre-diabetes and insulin resistance across time frames that anthocyanin and phenolic metabolites were apparent in blood. Further, in a dose response study in individuals with obesity and insulin resistance, insulin and glucose responses after strawberry intake with a meal were associated with the main anthocyanin metabolite of strawberry, pelargonidin glucuronide .Collectively, there is evidence suggesting that cognitive benefits after a single administration of polyphenol-containing berry fruits occurs during a timeframe corresponding to both the pharmacokinetic profiles of berry (poly)phenols and biological effects associated with metabolic health. Notably, observational data suggest a strong link between metabolic syndrome and cognitive impairment, suggesting that dietary components and/or their metabolites that impact metabolic systems, also might impact cognitive function. However, these factors and their relationships have not been investigated in a clinical trial. Also, while evidence of immediate cognitive benefit following polyphenol intake has been observed in school-aged children, young adults, and older adults, there is limited research to date concerning such cognitive effects in middle-age, a period that is noteworthy because of the association of mid-life health conditions, particularly metabolic disturbance, with risk for late-life dementia. Therefore, the aim of this study was to examine relationships among cognitive performance and metabolic responses in middle-age adults following one-time intake of whole fruit wild blueberry powder.","Here we have presented the effects of a polyphenol-rich, wild blueberry intervention on acute cognitive function and metabolic outcomes in middle-aged adults. The findings provide further support for the efficacy of wild blueberry on improving cognitive outcomes within this age group, particularly where there is increased cognitive demand. Wild blueberry was also found to reduce glucose and insulin concentrations in response to a meal over the initial 120 min having implications for post-meal metabolic control. Although there was little evidence of a direct relationship on cognition, these data have importance for structuring meal plans to reduce the metabolic burden in individuals with glucose homeostasis concerns.",https://twitter.com/foundmyfitness/status/1308496865382469633,
Effect of Omega-3 Dosage on Cardiovascular Outcomes,"A meta-analysis of 40 clinical trials found that supplementation with fish oil is associated with a 35% reduced risk of fatal heart attacks, a 13% reduced risk of heart attacks, and a 9% reduced risk of fatal coronary heart disease.","To quantify the effect of eicosapentaenoic (EPA) and docosahexaenoic (DHA) acids on cardiovascular disease (CVD) prevention and the effect of dosage. This study is designed as a random effects meta-analysis and meta-regression of randomized control trials with EPA/DHA supplementation. This is an update and expanded analysis of a previously published meta-analysis which covers all randomized control trials with EPA/DHA interventions and cardiovascular outcomes published before August 2019. The outcomes included are myocardial infarction (MI), coronary heart disease (CHD) events, CVD events (a composite of MI, angina, stroke, heart failure, peripheral arterial disease, sudden death, and non-scheduled cardiovascular surgical interventions), CHD mortality and fatal MI. The strength of evidence was assessed using the Grading of Recommendations Assessment, Development, and Evaluation framework. A total of 40 studies with a combined 135,267 participants were included. Supplementation was associated with reduced risk of MI (relative risk [RR], 0.87; 95% CI, 0.80 to 0.96), high certainty number needed to treat (NNT) of 272; CHD events (RR, 0.90; 95% CI, 0.84 to 0.97), high certainty NNT of 192; fatal MI (RR, 0.65; 95% CI, 0.46 to 0.91]), moderate certainty NNT = 128; and CHD mortality (RR, 0.91; 95% CI, 0.85 to 0.98), low certainty NNT = 431, but not CVD events (RR, 0.95; 95% CI, 0.90 to 1.00). The effect is dose dependent for CVD events and MI. Cardiovascular disease remains the leading cause of death worldwide. Supplementation with EPA and DHA is an effective lifestyle strategy for CVD prevention, and the protective effect probably increases with dosage.","Despite significant advances in the prevention and treatment of cardiovascular diseases (CVDs), they remain the leading cause of mortality in the United States and most of the world. According to the Centers for Disease Control and Prevention, diseases of the heart accounted for 23.1% and cerebrovascular diseases for 5.2% of all deaths in the United States in 2017. Eicosapentaenoic (EPA) and docosahexaenoic (DHA) acids, the two main omega-3 long-chain polyunsaturated fatty acids of marine origin, have shown promise for the prevention of CVD outcomes in animal studies and epidemiologic studies as previously reviewed in detail. However, randomized control trials (RCTs) have found inconsistent results. Whether a study finds a significant protective effect is not purely a function of study size or quality. Three large studies whose primary outcome was the occurrence of CVD events were published in 2018, and they reached diverging conclusions. A Study of Cardiovascular Events in Diabetes (ASCEND)4 (n=15,480), a study on primary prevention in diabetics, found no reduction in CVD risk. Vitamin D and Omega-3 Trial (VITAL) (n=25,871), the first primary prevention study on healthy adults, found a non-statistically significant 7% reduction in the risk of CVD events, and an unexpectedly high 28% reduction in the risk of myocardial infarction (MI), a pre-specified secondary outcome. Finally, Reduction of Cardiovascular Events with Icosapent Ethyl-Intervention Trial (REDUCE-IT) (n=8179), a trial that studied the effect of Vascepa (icosapent ethyl), a highly concentrated ethyl ester form of EPA on patients with mostly borderline and mildly high triglycerides who were taking statins, found a statistically significant 25% reduction in the risk of the primary endpoint, as well as risk reductions of similar magnitudes in multiple secondary endpoints. The results of meta-analyses are equally inconsistent. Three recent analyses of the effect of EPA and DHA on multiple CVD outcomes reached entirely different conclusions. For example, for coronary heart disease (CHD) mortality, Abdelhamid et al7 finds low certainty of a possible protective effect; Rizos et al finds a protective effect when using the usual P value cutoff of .05, but dismisses it as uncertain after using a very conservative multiple hypothesis correction; and Maki et al9 finds a statistically significant 8.0% risk reduction. The limitations of these analyses form the basis and rationale for our more extensive evaluation of the evidence. The reasons for this variability among the results of RCTs are not well understood, and although a number of possible explanations have been proposed, there is a dearth of data to support them. The explanations proposed range from differences in results depending on the year of publication (earlier trials, presumably being more likely to find positive results),the natural variability to be expected in insufficiently powered trials, potential interference in the omega-3 mechanisms of action by modern CVD prevention and treatment (especially use of statins and statin doses), differences in omega-3 baseline status and treatment compliance, baseline risk and dosage (11), and whether the study intervention included EPA alone or both EPA and DHA.A recent meta-analysis12 used meta-regression to study the effect of dosage on CVD outcomes and suggested significant protective effects of omega-3 against CVD events, but they restricted the analysis to only the 13 largest RCTs to date. Previous meta-analyses have compared the effect of dosages greater than or less than 1000 mg/day, but reducing the existing dosage information in this manner using what is an arbitrary cutoff is not an efficient use of existing information, and does not allow for a proper quantification of the dose-effect relationship. In 2006, Mozaffarian and Rimm13 observed that for multiple CVD outcomes, a higher dosage was associated with increased protection, and quantified this effect, but their estimates were based on an unweighted combination of interventional and observational studies, and inclusion of the latter may have introduced confounders and biased the results. The current analysis builds on the work of Abdelhamid et al7 but differs in the choice of what trials to include, focusing only on studies for which the intervention is EPA/DHA supplementation, and not dietary advice. This addresses more directly the question of what the effect is of long-chain omega-3 supplementation on CVD outcomes. Trials on dietary advice are important for designing efficient public health recommendations, but their results are confounded by problems in compliance and by the differences in EPA/DHA content of common foods. Unlike previous research,12 our work uses the totality of available evidence in measuring the effect of dosage. The larger study number and wider range of dosages allows for more precise and more robust estimates of the dose-effect relationship. Finally, the current study is the first to use meta-regression to examine other often cited potential sources of heterogeneity in the results from existing research. Given the prevalence of CVD, and their human and financial costs, it is important to determine which lifestyle modifications (and under what conditions) may provide some protection against these various CVD conditions. The use of fish oils and other products containing long-chain omega-3 fatty acids is a popular patient strategy to reduce CVD risk, and a nuanced understanding of why some clinical trials yield positive results and others fail to do so is a fundamental step in the correct evaluation of the risks and benefits of this supplementation to adequately inform both clinicians and the public about the potential benefits or lack of efficacy. The current review focuses on determining whether supplementation with EPA and DHA results in reduced CVD risk, and in quantifying the relationship between dosage and other predictors and the risk of CVD outcomes.","The current study presents strong evidence that EPA+DHA supplementation is an effective strategy for the prevention of certain CVD outcomes, and that for CVD events and MI the protective effect appears to increase with dosage. Authoritative bodies issuing intake recommendations and health care providers need to consider taking these results into account. Considering the relatively low costs and side effect profiles of omega-3 supplementation and the low drug-drug interactions with other standard therapies used in primary and secondary CVD prevention, clinicians and patients should consider the potential benefits of omega-3 (EPA/DHA) supplementation, especially using 1000 to 2000 mg/day dosages, which are rarely obtained in most Westernized diets, even those including some routine fish consumption.",https://twitter.com/foundmyfitness/status/1308107300754743296,
Cross-sectional study of social behaviors in preschool children and exposure to flame retardants,"Do common flame retardants affect social behavior? Children with quantifiably higher exposure rates of flame retardants showed less responsible behavior, more aggression, defiance, hyperactivity, inattention, and bullying behaviors.","Children are exposed to flame retardants from the built environment. Brominated diphenyl ethers (BDE) and organophosphate-based flame retardants (OPFRs) are associated with poorer neurocognitive functioning in children. Less is known, however, about the association between these classes of compounds and children’s emotional and social behaviors. The objective of this study was to determine if flame retardant exposure was associated with measurable differences in social behaviors among children ages 3–5 years. We examined teacher-rated social behaviors measured using the Social Skills Improvement Rating Scale (SSIS) and personal exposure to flame retardants in children aged 3–5 years who attended preschool (n = 72). Silicone passive samplers worn for 7 days were used to assess personal exposure to 41 compounds using gas chromatography-mass spectrophotometer. These concentrations were then summed into total BDE and total OPFR exposure prior to natural log transformation. Separate generalized additive models were used to evaluate the relationship between seven subscales of the SSIS and lnΣBDE or lnΣOPFR adjusting for other age, sex, adverse social experiences, and family context. All children were exposed to a mixture of flame retardant compounds. We observed a dose dependent relationship between lnΣOPFR and two subscales where children with higher exposures were rated by their preschool teachers as having less responsible behavior (p = 0.07) and more externalizing behavior problems (p = 0.03). Additionally, children with higher lnΣBDE exposure were rated by teachers as less assertive (p = 0.007). We observed a cross-sectional association between children’s exposure to flame retardant compounds and teacher-rated social behaviors among preschool-aged children. Children with higher flame retardant exposures exhibited poorer social skills in three domains that play an important role in a child’s ability to succeed academically and socially.","Early childhood is a key developmental period for learning appropriate social behaviors. Individual differences in externalizing behaviors, such as hyperactivity, inattention, aggressive, and oppositional behaviors, that emerge during early childhood often persist throughout childhood. Furthermore, children who exhibit more externalizing behaviors tend to struggle more in both academic and social domains and are more likely to develop mental illness by adulthood . In contrast, young children who show more positive social behaviors, such as cooperation, assertiveness, and self-control, tend to have more success in school and show more positive school adjustment, motivation, and involvement in learning. Much of the prior research on the etiology of social behavior in early childhood has focused on children’s social experiences, at home and preschool, as well as children’s genetics. Yet there is concern that chemicals commonly found in children’s environments may adversely influence social and emotional behavioral development. For instance, epidemiological studies have reported that children with higher lead exposure have a greater probability of demonstrating negative social behaviors in toddlers, children, and young teens. Exposure to bisphenol A (BPA), an endocrine disrupting chemical, has also been linked to behavioral outcomes in young children. Another study reported an association between polychlorinated biphenyl (PCB) exposure during development and a greater risk of exhibiting attention-deficit and autistic-like behavior in childhood. There is also considerable interest in whether exposure to flame retardants can influence children’s behavioral development. Flame retardants, namely brominated flame retardants (BFRs) and organophosphate-based flame retardants (OPFRs) are widely used in furniture, building materials, plastics, and electronics to reduce their flammability in order to meet fire safety standards. Biomonitoring studies show that BFRs have increased in people over time and are almost an order of magnitude higher in the U.S. compared to European and Asian populations . Children also appear to have greater exposure to flame retardants as reflected by having much higher levels of BFRs in their blood compared to their mothers . Several prospective epidemiological studies report that in utero exposure or early life exposure to selected BDE congeners (e.g. BDE-28, -47, -99, -100, or -153) are associated with adverse neurological developmental, attention deficits, poorer behavioral regulation, or social competence in children. Very little is currently known about how exposure to OPFRs affect children’s neurodevelopment or social behaviors. Although data from experimental studies indicate that tris(2-chloroethyl)phosphate (TCEP) and tris(1,3-dichloropropyl)phosphate (TDCPP) affect neurodevelopmental responses in experimental systems. The current study expands prior research on the association between flame retardants and social behaviors by building on an existing study of preschool aged children. The objective of the study was to examine the association between two different classes of flame retardants (PBDEs and OPFRs) and preschool children’s social behaviors as assessed by the Social Skills Improvement System - Rating Scales (SSIS-RS) which is a clinically relevant assessment. The scores from SSIS-RS also capture normal variation in children’s behaviors that predicts success in academic and social domains. Furthermore, this study controlled for important psychosocial stressors that negatively affect behavior .","Children are exposed to different types of flame retardants from the built environment. These exposures have been associated with poorer attention and motor skills in children but less is known about how these compounds are related to children’s social skills. After controlling for social experiences and other factors, children with higher organophosphate flame retardant exposure were rated by their preschool teachers to show less responsible behavior and more externalizing behavior problems. Children with higher exposure to brominated flame retardants were rated by their preschool teachers as less assertive.",https://twitter.com/foundmyfitness/status/1306670957092765696,
Maternal dietary imbalance between omega-6 and omega-3 fatty acids triggers the offspring’s overeating in mice,"Pregnant mice fed a diet high in omega-6 fats and low in omega-3 fats had offspring that differed developmentally in their brain's reward centers, resulting in enhanced dopamine-seeking behaviors like excess food consumption.","The increasing prevalence of obesity and its effects on our society warrant intensifying basic animal research for understanding why habitual intake of highly palatable foods has increased due to recent global environmental changes. Here, we report that pregnant mice that consume a diet high in omega-6 (n-6) polyunsaturated fatty acids (PUFAs) and low in omega-3 (n-3) PUFAs (an n-6high/n-3low diet), whose n-6/n-3 ratio is approximately 120, induces hedonic consumption in the offspring by upregulating the midbrain dopaminergic system. We found that exposure to the n-6high/n-3low diet specifically increases the consumption of palatable foods via increased mesolimbic dopamine release. In addition, neurodevelopmental analyses revealed that this induced hedonic consumption is programmed during embryogenesis, as dopaminergic neurogenesis is increased during in utero access to the n-6high/n-3low diet. Our findings reveal that maternal consumption of PUFAs can have long-lasting effects on the offspring’s pattern for consuming highly palatable foods.","In nature, consuming a diet high in sugar and fat confers a clear advantage to the organism, as such foods provide readily accessible energy. The instinctive drive to consume such a diet, which has been acquired through evolution, has become an overshoot in our modern society in which highly palatable foods (i.e., hyper-caloric foods) abound. Frequent consumption of fast foods, which are often rich in calories but poor in essential nutrients, has been linked to a variety of health hazards including obesity. Given that no nation has yet succeeded in reducing the rising prevalence of obesity population during the past three decades, identifying the components that contribute to unhealthy eating habits is extremely urgent and has high societal relevance. The hedonic aspects of ingestive behavior are potentiated by mesolimbic dopaminergic neurons in the ventral tegmental area (VTA) in the midbrain, which innervate the nucleus accumbens (NAc) in the basal ganglia. In animal models, the consumption of palatable foods increases extracellular dopamine (DA) levels in the medial NAc, and pharmacologically inducing DA release in this brain region increases sucrose consumption. Interestingly, reward-predicting cues also activate mesolimbic dopaminergic neurons. Moreover, increasing extracellular DA in the medial NAc, which is more responsive to reward-predicting cues compared to the lateral NAc, increases the response to obtaining a predicted sucrose reward. On the other hand, increasing DA release in the medial NAc does not affect water consumption, and DA in the NAc does not play a role in the consumption of a standard (i.e., healthy) diet. Taken together, these data support the notion that inducing DA release in the medial NAc selectively potentiates the consumption of highly palatable foods.","In the wild, the drive to obtain foods containing high amounts of sugar and/or fat is beneficial for self-preservation and procreation, as it helps ensure both sufficient and efficient sources of energy. In humans, however, consuming an n-6high/n-3low diet can lead to maladaptive feeding practices, particularly in our modern society in which highly palatable foods are abundant. In an attempt to counteract the spread of obesity, a growing number of countries have adopted various health policies such as increasing taxes on unhealthy foods, improving nutrition labeling, and applying regulations to food advertising; despite these efforts, however, the global incidence of obesity has not declined. Our study suggests that the increased availability of n-6high/n-3low diets would hamper the efficacy of these national policies. To create a sustainable society, decreasing the maternal consumption of n-6high/n-3low foods might serve as a preemptive step toward reducing the risks of obesity in future generations. In this respect, our findings may have wide-reaching implications for solving an ongoing global health crisis.",https://twitter.com/foundmyfitness/status/1304159711177048064,
Effects of oily fish intake on cognitive and socioemotional function in healthy 8–9-year-old children: the FiSK Junior randomized trial,"Children given oily fish high in omega-3 fatty acids for 12 weeks had improved cognitive function, particularly attention and cognitive flexibility, and reduced socioemotional problems compared to children given poultry.","Long-chain n–3 PUFAs (n–3 LCPUFAs) accrete in the brain during childhood and affect brain development. Randomized trials in children show inconsistent effects of n–3 LCPUFAs on cognitive and socioemotional function, and few have investigated effects of fish per se. We aimed to investigate the effects of oily fish consumption on overall and domain-specific cognitive and socioemotional scores and explore sex differences. Healthy 8–9-y-old children (n = 199) were randomly allocated to receive ∼300 g/wk oily fish or poultry (control) for 12 ± 2 wk. At baseline and endpoint, we assessed attention, processing speed, executive functions, memory, emotions, and behavior with a large battery of tests and questionnaires and analyzed erythrocyte fatty acid composition. One hundred and ninety-seven (99%) children completed the trial. Children in the fish group consumed 375 (25th–75th percentile: 325–426) g/wk oily fish resulting in 2.3 (95% CI: 1.9, 2.6) fatty acid percentage points higher erythrocyte n–3 LCPUFA than in the poultry group. The overall cognitive performance score tended to improve by 0.17 (95% CI: −0.01, 0.35) points in children who received fish compared with poultry, supported by n–3 LCPUFA dose dependency. This was driven mainly by fewer errors [−1.9 (95% CI: −3.4, −0.3)] in an attention task and improved cognitive flexibility measured as faster reaction time [−51 ms (95% CI: −94, −7 ms)] in a complex relative to a simple task (“mixing cost”). The fish intervention furthermore reduced parent-rated Strength and Difficulties Questionnaire total difficulties by −0.89 (95% CI: −1.60, −0.18) points mainly due to a −0.63 (95% CI: −1.11, −0.16) points reduction in internalizing problems that was reflected in tendency to a decrease in the overall socioemotional problems score of −0.13 (95% CI: −0.26, 0.01) points. The overall effects were similar in boys and girls. Oily fish dose-dependently improved cognitive function, especially attention and cognitive flexibility, and reduced socioemotional problems. The results support the importance of n–3 LCPUFAs for optimal brain function and fish intake recommendations in children.","Nutrition plays an important role in brain development and cognitive function . Intake of n–3 long-chain PUFAs (n–3 LCPUFAs), especially DHA (22:6n–3), is considered particularly important for neuronal development during early infancy where a substantial amount of DHA is accreted in the brain This accretion continues throughout childhood, especially in the frontal cortex involved in cognitive functions such as executive functions, attention, and memory as well as in socioemotional functions such as emotional regulation, impulse control, and social behavior, which continue to develop until early adulthood. Intake of n–3 LCPUFAs could therefore be important for cognitive and socioemotional development throughout childhood and adolescence. Some randomized controlled trials (RCTs) have shown effects of n–3 LCPUFA supplementation on cognitive and socioemotional outcomes in schoolchildren , but other studies have not found any effects , and meta-analyses of RCTs have not shown effects of n–3 LCPUFAs on any cognitive domains in healthy children . Oily fish are the primary dietary source of n–3 LCPUFAs , but fish also contain other nutrients that are considered important for the brain, for example, vitamin D, iodine, zinc, and vitamin B-12 . Only a few RCTs have investigated the effect of fish per se. One trial in adolescents found that oily fish intake improved processing speed but reduced attention and did not affect mental health, and 2 trials in preschool children reported overall beneficial effects of oily fish consumption on cognitive performance but not mental health , and improvement in some tests of fluid intelligence but not in general intelligence. Furthermore, we previously showed that fish intake was associated with improved school performance but also with decreased attention in an RCT that compared fish-rich school meals with traditional Danish lunch packs in 8–11-y-old children.Most of the previous RCTs analyzed effects of fish or n–3 LCPUFAs without exploring potential sex differences, but few studies have reported sex-specific associations between n–3 LCPUFAs and cognitive outcomes. In the school meal intervention, we observed an increase in impulsivity only among boys, but their reading correctness also improved more than in girls . It is therefore important to focus on potential sex differences in the effects of n–3 LCPUFAs. The aim of this study was to investigate whether intake of oily fish compared with poultry affected cognitive function, focusing on measures of attention, processing speed, executive functions, and memory, as well as socioemotional function in healthy 8–9-y-old Danish children. Additionally, we examined whether the effects differed in boys and girls.","In conclusion, our findings support a dose-dependent beneficial effect of oily fish consumption on cognitive performance in healthy children and indicate improvements in attention, cognitive flexibility, and socioemotional problems. These results substantiate the importance of n–3 LCPUFAs for optimal brain function and recommendations of fish intake in children. Although a few sex-specific differences were indicated in attention and socioemotional measures, most effects were comparable in boys and girls. Future sufficiently powered studies are needed to understand the effects of fish consumption on specific cognitive and socioemotional domains and the potential influence of sex.",https://twitter.com/foundmyfitness/status/1296929813333078016,
Opposing effects of antibiotics and germ-free status on neuropeptide systems involved in social behaviour and pain regulation,High-dose antibiotic treatment in early life disrupted pathways in the brain that regulate social behavior and the response to pain including oxytocin and endorphins (animal study).,"Recent research has revealed that the community of microorganisms inhabiting the gut affects brain development, function and behaviour. In particular, disruption of the gut microbiome during critical developmental windows can have lasting effects on host physiology. Both antibiotic exposure and germ-free conditions impact the central nervous system and can alter multiple aspects of behaviour. Social impairments are typically displayed by antibiotic-treated and germ-free animals, yet there is a lack of understanding of the underlying neurobiological changes. Since the μ-opioid, oxytocin and vasopressin systems are key modulators of mammalian social behaviour, here we investigate the effect of experimentally manipulating the gut microbiome on the expression of these pathways. We show that social neuropeptide signalling is disrupted in germ-free and antibiotic-treated mice, which may contribute to the behavioural deficits observed in these animal models. The most notable finding is the reduction in neuroreceptor gene expression in the frontal cortex of mice administered an antibiotic cocktail post-weaning. Additionally, the changes observed in germ-free mice were generally in the opposite direction to the antibiotic-treated mice. Antibiotic treatment when young can impact brain signalling pathways underpinning social behaviour and pain regulation. Since antibiotic administration is common in childhood and adolescence, our findings highlight the potential adverse effects that antibiotic exposure during these key neurodevelopmental periods may have on the human brain, including the possible increased risk of neuropsychiatric conditions later in life. In addition, since antibiotics are often considered a more amenable alternative to germ-free conditions, our contrasting results for these two treatments suggest that they should be viewed as distinct models.","The regulation of behaviour and emotion is complex, influenced by both genes and the environment. The microbial environment within the gut, known as the gut microbiome, has recently been shown to affect various aspects of brain development and behaviour. The brain is particularly sensitive to perturbations throughout childhood and adolescence when its structure is undergoing rapid change. During this time, environmental disruptions may permanently impact brain function and increase susceptibility to neuropsychiatric conditions. These include changes to the gut microbiome which can affect neurodevelopment via gut–brain signalling. The microbial community of the gut may influence the functioning of the central nervous system through various mechanisms, including communication via the nervous, immune and endocrine systems.​​In addition, recent research provides strong support for the causal relationship between a dysbiotic gut microbiome and altered social behaviour . Offspring of mice which had been fed a high-fat diet exhibited autistic-like behaviours including social impairments, anxiety and repetitive behaviours, as well as fewer oxytocin-expressing neurones in the hypothalamus. Notably, offspring were found to have an altered gut microbial community but supplementation with the bacterial species Lactobacillus reuteri restored oxytocin levels and reversed social deficits in this mouse model of autism. Neuropeptides are postulated to play an important role in communication between the gut microbiome and the brain, especially since they interact with both the immune system and the vagus nerve. Interestingly, μ-opioid receptors are not only widely expressed in the brain but also in the gut, where they regulate the gut–brain neural circuitry involved in satiety . In fact, opioids, oxytocin and vasopressin can all influence gut physiology, such as motility . Certain Lactobacillus species can induce μ-opioid receptor expression in the gut via the nuclear factor-κB immune response, while antibiotic treatment has been found to reduce expression of this receptor in the gut. However, the interaction between the gut microbiome and the μ-opioid system in the central nervous system has not previously been investigated. Furthermore, there have been few studies on the relationship between the gut microbiome and brain neuropeptide systems, with research focusing on the expression of the peptides rather than their receptors. Here we examine the influence of antibiotic treatment and germ-free status on neuropeptide pathways implicated in social and emotional behaviour by measuring gene expression of both the peptides and their corresponding receptors. The experiments were conducted in mice since they are a naturally social mammalian species and therefore represent a suitable model organism. We adopt two different approaches, using mice treated with antibiotics post-weaning and germ-free mice, since both models are commonly used to ascertain the role of the microbiome in host development and behaviour.","The results of our study reveal that disruption of the gut microbiome in early life can significantly impact social neuropeptide signalling. The reduced activation of these pathways in mice treated with antibiotics post-weaning likely contributes to their social impairments. It is also relevant to note the opposing effects of antibiotic treatment and germ-free status on gene expression in the brain; an interesting observation that should be explored in further studies. Our findings indicate the possible effects that early-life exposure to antibiotics may have on pathways in the brain mediating social and emotional behaviour, with potential implications for the risk of developing neuropsychiatric conditions such as autism, depression and anxiety.",https://twitter.com/foundmyfitness/status/1294387092768792576,
Efficacy of Resveratrol Supplementation on Glucose and Lipid Metabolism: A Meta-Analysis and Systematic Review,Analysis of 25 clinical trials indicates that the red wine  molecule resveratrol has a dramatic impact on regulating lipid and glucose metabolism,"Lipids are ubiquitous metabolites with diverse functions. Excessive lipid accumulation can trigger lipid redistribution among metabolic organs such as adipose, liver and muscle, thus altering the lipid metabolism. It has been revealed that disturbed lipid metabolism would cause multiple disease complications and is highly correlated with human morbidity. Resveratrol (RSV), a phytoestrogen with antioxidant, can modulate insulin resistance and lipid profile. Recently, research on RSV supplementation to improve glucose and lipid metabolism has been controversial. A meta-analysis may provide a scientific reference for the relationship between lipid metabolism and RSV supplementation.
We searched the PubMed, Cochrane Library, Web of Science, and Embase databases from inception to October 2021 using relevant keywords. A comprehensive search for randomized controlled trials (RCTs) was performed. For calculating pooled effects, continuous data were pooled by mean difference (MD) and 95% confidence interval (CI). Adopting the method of inverse-variance with a random-effect, all related statistical analyses were performed using the Rev Man V.5.3 and STATA V.15 software.
A total of 25 articles were incorporated into the final meta-analysis after removal of duplicates by checking titles and abstracts and excluding non-relevant articles. The selected articles had a total of 1,171 participants, including 578 in the placebo group and 593 in the intervention group. According to the current meta-analysis, which demonstrated that there was a significant decrease in waist circumference (SMD = -0.36; 95% CI: -0.59, -0.14; P = 0.002; I 2 = 88%), hemoglobin A1c (-0.48; -0.69, -0.27; P ≤ 0.001; I 2 = 94%), total cholesterol (-0.15; -0.3, -0.01; P = 0.003; I 2 = 94%), low density lipoprotein cholesterol (-0.42; -0.57, -0.27; P ≤ 0.001; I 2 = 92%), high density lipoprotein cholesterol (0.16; -0.31, -0.02; P = 0.03; I 2 = 81%) following resveratrol administration.
These results suggest that RSV has a dramatic impact on regulating lipid and glucose metabolism, and the major clinical value of resveratrol intake is for obese and diabetic patients. We hope that this study could provide more options for clinicians using RSV. Furthermore, in the future, large-scale and well-designed trials will be warranted to confirm these results.","Lipids are ubiquitous metabolites with diverse functions.
Excessive lipid accumulation can trigger lipid redistribution
among metabolic organs such as adipose, liver and muscle, thus
altering the lipid metabolism (Zhao et al., 2020). Disturbed
lipid metabolism will cause multiple disease complications and
is highly correlated with human morbidity (Cao et al., 2020).
Even some reports have indicated that lipid redistribution was
tightly associated with progression of various cancers (Liu
et al., 2018), and these discoveries might be significant for
treatment of antileukemic and epigenetic effects (Grønningsæter
et al., 2019). Regulation of lipid metabolism is essential for
maintenance of whole-body metabolic and energy homeostasis
(Warne et al., 2011).
Resveratrol (RSV) is a phytoestrogen with antioxidant and
can modulate insulin resistance and lipid profile (Szkudelska and
Szkudelski, 2010). In previous studies, RSV has been suggested
to improve motor function, extension of life span and well
loss in weight in animal models (Alberdi et al., 2011; Wang
et al., 2014), such as such as diminishing the deposits of white
adipose tissue (WAT) and reducing total body fat (Alberdi et al.,
2011). However, it was reported that for obese men, high-dose
resveratrol (hRSV) used for four weeks had no effect on ectopic or
visceral fat content and lipid oxidation rates (Poulsen et al., 2013).
Also, RSV is a plant-derived nutritional supplement shown to
have antidiabetic properties in many animals models (Sahebkar,
2013; Sahebkar et al., 2015; Tabrizi et al., 2020). In summary, the
research on RSV supplementation improving glucose and lipid
metabolism remains controversial.
Systematic review and meta-analysis were performed to
summarize the published clinical trials to date, and we
tried to incorporate the evidence as a new model for
revaluating the effect of RSV on glucose and lipid metabolism
more comprehensively. The results of data-analysis further
define the relationship between lipid metabolism and RSV
supplementation, clarifying the contribution of RSV in lipidrelated components and elucidating the comparative causal role
of lipid-related components by RSV supplementation","Taken together, these results suggest that RSV has a dramatic
impact on regulating lipid and glucose metabolism, and the
major clinical value of resveratrol intake is for obese and
diabetic patients. The efficacy of resveratrol supplementation in
lipid metabolism was clarified in the results of the systematic
review. We hope that this study could provide more options for
clinicians using RSV.",https://pubmed.ncbi.nlm.nih.gov/35431994/,
"Nutrition, metabolism, and epigenetics: pathways of circadian reprogramming","NAD precursors, such as NMN, control the body's sleep-wake cycle, the timing & amplitude of which are disrupted by age & travel. Evidence suggests NAD precursors and keto diets make the clock tick stronger and may alleviate jet lag","Food intake profoundly affects systemic physiology. A large body of evidence has indicated a link between food intake and circadian rhythms, and ~24-h cycles are deemed essential for adapting internal homeostasis to the external environment. Circadian rhythms are controlled by the biological clock, a molecular system remarkably conserved throughout evolution. The circadian clock controls the cyclic expression of numerous genes, a regulatory program common to all mammalian cells, which may lead to various metabolic and physiological disturbances if hindered. Although the circadian clock regulates multiple metabolic pathways, metabolic states also provide feedback on the molecular clock. Therefore, a remarkable feature is reprogramming by nutritional challenges, such as a high-fat diet, fasting, ketogenic diet, and caloric restriction. In addition, various factors such as energy balance, histone modifications, and nuclear receptor activity are involved in the remodeling of the clock. Herein, we review the interaction of dietary components with the circadian system and illustrate the relationships linking the molecular clock to metabolism and critical roles in the remodeling process.","Every morning, after a night of sleep, we wake up, eat our regularly timed meals, go through our normal routines, sleep, and then repeat the same cycle. Various physiological functions, including sleep and being awake, body temperature, hormone secretion, locomotor activity, and appetite, are regulated by an autonomous, ~24-h mechanism, termed the circadian clock (Sahar & Sassone-Corsi, 2012). This endogenous timekeeper allows organisms to anticipate daily environmental fluctuations and time internal processes (Bass & Takahashi, 2010; Eckel-Mahan & Sassone-Corsi, 2013). Anatomically, the mammalian central clock or pacemaker is located in the suprachiasmatic nucleus (SCN) of the hypothalamus, with functions regulated by photic inputs from the retina in the form of light. Notably, light-induced resetting of the SCN clock depends on wavelength. Blue light (380–500 nm) potentially exerts more robust effects on mammalian circadian rhythms than green and yellow wavelengths (Lockley et al, 2003). A transformative discovery around the turn of the century revealed that in addition to the brain, the circadian clock functions in peripheral organs, including the liver and muscle (Schibler & Sassone-Corsi, 2002). These local or peripheral clocks are semi-autonomous elements of a larger system and are synchronized by the SCN clock, functioning as an “orchestra director,” via neural, hormonal (e.g., glucocorticoids [GCs], insulin, and melatonin), and behavioral inputs (Saini et al, 2011).

The molecular machinery underlying the circadian clock consists of a transcriptional/translational feedback loop: the core transcription factors, brain and muscle Arnt-like 1 (BMAL1) and circadian locomotor output cycles kaput (CLOCK), heterodimerize and drive the expression of core clock genes or output genes by binding to enhancer boxes (E-boxes) on target promoters (Crane & Young, 2014). As E-boxes are among the most common promoter elements in the genome, the clock can transcriptionally control a large array of genes. In addition, CLOCK:BMAL1 directly activates the transcription of Period (Per1, Per2, and Per3) and Cryptochrome (Cry1 and Cry2) genes, known to encode transcriptional repressors that dimerize and generate a tightly regulated negative portion of the feedback loop (Gekakis et al, 1998; Kume et al, 1999; Shearman et al, 2000; Lee et al, 2001; Padmanabhan et al, 2012; Kim et al, 2014). An additional level of circadian regulation involves the nuclear receptors, i.e., RAR-related orphan receptors (RORs) and REV-ERBα (Nr1d1), which activate and repress Bmal1 transcription, respectively (Reppert & Weaver, 2002; Everett & Lazar, 2014; Partch et al, 2014). Furthermore, virtually all clock proteins are reportedly regulated by post-translational modifications, including phosphorylation, acetylation, ubiquitination, and O-linked N-acetylglucosamine modification (O-GlcNAcylation) (Asher & Schibler, 2011; Kaasik et al, 2013; Li et al, 2013).

(A) Daytime: The core transcription factor BMAL1 heterodimerizes with CLOCK to form a CLOCK:BMAL1 complex. CLOCK acetylates BMAL1 and histone tails, leading to chromatin opening that promotes binding of CLOCK:BMAL1 to E-box elements in promoter regions of the core-clock genes and clock-controlled genes (Per, Cry, Rev-erv, Nampt). During the daytime AMPK and CK1ε contribute to phosphorylation and degradation of the negative regulators CRY and PER, respectively, thus relieving the negative feedback on CLOCK:BMAL1. The circadian activity of SIRT1, which regulates cyclic acetylation levels of BMAL1 and histones in nucleosomes associated with clock-controlled genes, is controlled by the rhythmic cellular levels of its cofactor NAD⁺. Nampt gene expression and cellular NAD⁺ levels oscillate and peak at night, leading to lower SIRT1 activity during the daytime. (B) Nighttime: PER and CRY protein accumulate in the cytosol during the night, heterodimerize and translocate to the nucleus to repress CLOCK:BMAL1 transcriptional activity. SIRT1 deacetylase activity during the nighttime is high, deacetylating BMAL1 and histone tails. SIRT1-mediated histone deacetylation induces packing of DNA (heterochromatin) and gene silencing. AMPK, 5′ AMP-activated protein kinase; BMAL1, brain and muscle ARNT-like 1; CK1ε, casein kinase 1 epsilon; CLOCK, circadian locomotor output cycles protein caput; CRY, cryptochrome; NAD⁺, oxidized nicotinamide adenine dinucleotide; Nampt, nicotinamide phosphoribosyltransferase; PER, period; SIRT1, sirtuin 1.
Numerous studies have highlighted how the clock system closely interacts with energy metabolism in peripheral organs (Bass, 2012; Eckel-Mahan et al, 2013). Although the SCN clock contributes to circadian variations in glucose homeostasis via glucose uptake and insulin release, peripheral clocks can also constitute another layer of regulation of these processes. For example, melatonin secreted from the pineal gland in an SCN clock-dependent manner directly regulates pancreatic insulin secretion (Peschke et al, 1997; Picinato et al, 2002). Pancreatic insulin secretion is regulated by multisynaptic projections from the SCN (Ueyama et al, 1999; Buijs et al, 2001). In addition to insulin-mediated signaling, it has been suggested that the SCN may regulate glucose uptake in peripheral organs through the nervous system (La Fleur, 2003). Moreover, the clock machinery controls the expression of numerous metabolic output genes in peripheral tissues (Dibner et al, 2010; Maury et al, 2010). Accordingly, genetic disruption of mouse clock components can induce metabolic diseases, including obesity, by attenuating rhythmic changes in hormone concentration and metabolic gene expression (Dibner et al, 2010; Maury et al, 2010). In humans, disruption of circadian rhythms owing to jet lag, time shift work, and irregular meal timing has been linked to metabolic diseases (Kettner et al, 2015; Morris et al, 2016). These observations suggest that the circadian clock controls several signaling pathways encompassing major components of metabolic homeostasis.

Accumulated evidence has suggested that the quality and timing of meals markedly alter circadian metabolism (Eckel-Mahan et al, 2013; Tognini et al, 2017). For example, a comparison of oscillating transcripts in different tissues revealed that approximately 15% of hepatic transcripts and 4% of muscle transcripts oscillate under ad libitum conditions of a normal chow diet (Hughes et al, 2010; Zhang et al, 2014b). A high-fat diet (HFD), wherein energy from fat exceeds 40%, can disrupt normally oscillating genes and induce de novo oscillations (Eckel-Mahan et al, 2013). Transcriptome analyses have revealed that a large fraction of the genome can be potentially controlled by the clock, and diet-induced remodeling exerts tissue-specific effects (Masri & Sassone-Corsi, 2010). Moreover, varying the macronutrient composition and specific nutrients (nobiletin, resveratrol, and caffeine) have been reported to influence peripheral clock gene expression (Sherman et al, 2011; Sun et al, 2015; He et al, 2016).

In this review, we describe the relationship between the circadian clock and metabolic homeostasis from the perspective of energy balance and epigenome regulation. We focus on the effect of dietary composition on the molecular clock and discuss how nutritional approaches may contribute to the prevention of metabolic diseases.","During past decades, a large array of nutritional challenge studies have provided insights into the reciprocal relationship between the circadian system and metabolic homeostasis. In addition to dietary composition, macronutrients can influence circadian rhythmicity. However, numerous outstanding questions remain unresolved.

Over the last decade, several circadian transcriptome analyses have been conducted to characterize the circadian and metabolic genes expressed in different organs and under several dietary conditions. Interestingly, the effects of diet differ depending on the organ, which is influenced by tissue-specific transcription factors. The interaction between organs via output metabolites has also been reported, although a comprehensive understanding is yet to be established. Future research should aim to determine the contribution of each peripheral clock to metabolic homeostasis in other organs.

Recent studies on various diets have revealed the effects of diet composition on clock genes and transcription factors. However, results across studies have been contradictory. As diet compositions vary substantially from study to study, a careful and comprehensive approach is required to elucidate the intricate mechanisms linking the clock and metabolism.

Most studies investigating clock changes and metabolic function changes attributed to dietary modifications have been performed in rodents. Available information regarding the impact of various dietary changes on the human clock system remains insufficient. Blood samples can be obtained using minimally invasive methods and are extremely valuable for extracting important biological information. For example, plasma levels of PC reflect the dietary carbohydrate–fat ratio in humans (Inoue et al, 2017). A recently published study has illustrated changes in metabolites in human serum in response to dietary changes over circadian time (Sato et al, 2018). Therefore, blood samples can be substantially useful for determining the effect of dietary composition on metabolic rhythms in humans. Further studies on chrono-nutritional aspects of the circadian clock can provide potential therapeutics for treating metabolic diseases.",https://www.embopress.org/doi/full/10.15252/embr.202152412,
Identification of touch neurons underlying dopaminergic pleasurable touch and sexual receptivity,Scientists have identified nerves in skin that induce pleasure when touched and have engineered mice that are sexually aroused by light,"Pleasurable touch during social behavior is the key to building familial bonds and
meaningful connections. One form of social touch occurs during sex. Although sexual
behavior is initiated in part by touch, and touch is ongoing throughout copulation, the
identity and role of sensory neurons that transduce sexual touch remain unknown. A
population of sensory neurons labeled by the G-protein coupled receptor Mrgprb4 detect
stroking touch in mice1,2. Here, we study the social relevance of this population by
genetically engineering mice to allow activation or ablation of Mrgprb4-lineage neurons
and reveal that these neurons are required for sexual receptivity and sufficient to activate
reward circuitry. Even in social isolation, optogenetic stimulation of Mrgprb4-lineage
neurons through the back skin is sufficient to induce a conditioned place preference and
a striking dorsoflexion resembling the lordotic copulatory posture in females. In the
absence of Mrgprb4-lineage neurons, female mice no longer find male mounts rewarding:
sexual receptivity is supplanted by aggression and a coincident decline in dopaminergic
release in the mesolimbic reward pathway. In addition to sexual behavior, Mrgprb4-
lineage neurons are also required for social postures induced by female-to-female back
touch. Together, these findings establish that Mrgprb4-lineage neurons are the first
neurons of a skin-to-brain circuit encoding the rewarding quality of social touch.","The pleasure of a partner’s caress or a child’s embrace begins with mechanical
signals transduced by neurons in our skin. The way our brain interprets these instances
of social touch is critical for our well-being. Despite the centrality of socially rewarding
touch in our daily lives, the neurons in the skin that detect social touch and shape the
valence of perception remain unknown. This gap in knowledge is critical, especially when
considering the nature of neurodevelopmental disorders like autism spectrum disorder,
where gentle touch and socially rewarding behaviors are aversive3-5.
A class of sensory neurons in humans that are linked to gentle stroking are termed
C-tactile afferents6,7, and there are putative populations of these neurons in the mouse8-
10. One such class of neurons in mice express the G-protein coupled receptor Mrgprb4
and share anatomical and physiological similarities with human C-tactile afferents1,2.
These hairy skin-innervating C-fibers respond to gentle stroking and produce a
conditioned place preference, suggesting their activation is rewarding1,2. Since the
Mrgprb4+ touch neurons fit the description of cells in the skin that might promote
ethologically relevant rewarding touch, we used a combination of mouse genetics, novel
behavioral paradigms, and in vivo brain imaging to connect the skin and brain by
dissecting the role of Mrgprb4+ neurons in socially rewarding behaviors.","Although affective social touch begins at the skin’s surface, the molecular identity
of sensory neurons in the skin that detect socially relevant signals and pass them to the
central nervous system has remained unknown. Moreover, because touch itself is highly
heterogeneous (i.e., discriminative touch to detect texture with our fingertips versus the
affiliative touch during a hug from a friend), sensory perception is likely generated by
different sets of neurons to provide specificity. Armed with this information, where
does one begin the search for touch neurons underlying social reward, including sexual
receptivity? Within the deep ocean of DRG neuron types, one class, termed C-low
threshold mechanoreceptors (C-LTMRs), or C-tactile afferents in humans, are implicated
in detecting gentle strokes across the skin’s surface. Although a limited number of papers
in the mouse identify molecular populations of C-LTMRs, including their neuroanatomy
and roles in somatosensation during baseline and chronic pain states, the role of
C-LTMRs in promoting social behaviors remains obscure.
Here, we demonstrate that Mrgprb4-lineage neurons are indeed critical for specific
social behaviors and for signaling social reward to the brain. Focal activation of Mrgprb4-
lineage neurons yields a striking dorsiflexion posture resembling mammalian lordosis,
representing the first acute behavioral response to optogenetic activation of social touch
neurons. Mrgprb4-lineage neurons are required for two touch-dependent social postures:
sexual behavior and crawling under same-sex conspecifics. However, their role in female
sexual behavior is not simply in the local lordotic reflex; rather, the neurons convey an
Figure 4: Mrgprb4-lineage neurons trigger dopamine release and are required for dopamine release during
sexual mounts. A) Schematic depicting the experimental setup. Female Mrgprb4Cre; RosaChR2/ChR2 mice with shaved
backs, which had been injected with GRABDA to the NAc two weeks prior to testing, were placed under plastic chambers
on a mesh platform. Pulsed blue (stimulating) or green (control) laser light (35mW, 10Hz sin wave) was shined to either
the back skin or skin surrounding the vagina while recording GRABDA signals. B) Average GRABDA delta F/F signals
(N=6-7) for green and blue light. C) Representative trace from a mouse with green light shined to the skin surrounding
the vagina, light stimulation begins T=0. D) Representative trace from the same mouse with blue light shined to the
skin surrounding the vagina. E) Average deltaF/F pre stim (-5-0) subtracted from average deltaF/F post stim (10-15s)
is significantly greater for blue light stimulation compared to the green light control, (*P<0.05, unpaired t-test). F)
Graphic depicting experimental setup. Female Mrgprb4Cre; RosaDTA mice or littermate RosaDTA controls, which had been
injected with GRABDA to the NAc over two weeks prior to testing were paired with males for a mating assay. Females
were ovariectomized and hormone primed to be in a state of behavioral estrus for testing. GRABDA signals were
recorded for the entire pairing and analyzed surrounding three events: mount, anogenital sniff to female, and back
contact to female. G) Average deltaF/F traces surrounding mount onset (T=0) for Mrgprb4Cre; RosaDTA females (purple)
or littermate RosaDTA controls (gray) (N=4-5). H) Representative average GRABDA signal across seven mounts in one
pairing for a RosaDTA control, mount onset at T=0. I) Representative average GRABDA signal across three mounts in
one pairing for a Mrgprb4Cre; RosaDTA female. J) The average GRABDA signal following mount onset is reduced in
Mrgprb4Cre; RosaDTA females compared to RosaDTA controls (*P<0.05, One way ANOVA).
affective sensation that reinforces sexual receptivity, and without them, male advances
during sexually receptive hormonal states become aversive. This finding suggests that
Mrgprb4-lineage neurons contribute to the perceived valence of sexual encounters in
females by encoding the rewarding aspect of male sexual touch.
Lastly, we use fiber photometry to functionally link Mrgprb4-lineage neurons in the
skin to reward circuitry in the brain. Transdermal optogenetic activation of Mrgprb4-
lineage neurons in the skin surrounding female genitalia is sufficient to induce dopamine
release in the NAc, representing the first demonstration of molecularly defined
somatosensory neurons triggering activation of a brain reward center. Moreover,
Mrgprb4-lineage neurons are required for this same dopamine release during male
mounts. These experiments have revealed the sufficiency of peripheral inputs to regulate
social reward independent of context and other sensory cues. Since ventral tegmental
dopaminergic neurons that project to the NAc are themselves functionally
heterogeneous50,51, it is possible that some of these neurons might be tuned for encoding
rewarding touch. It will also be interesting to determine if Mrgprb4-lineage neurons are
involved in other behaviors that integrate social touch, such as maternal care. Moreover,
the approaches outlined here could be leveraged to determine the functional roles of other
molecularly defined touch neurons. We believe this study not only draws new attention
to the importance of elucidating skin-to-brain circuits, but also unveils the therapeutic
potential of peripheral manipulations for enhancing intact or impaired social reward
systems, including sexual receptivity, or simulating social reward during periods of
isolation.",https://www.biorxiv.org/content/10.1101/2021.09.22.461355v1,
,"A landmark identical twin study indicates how a child is raised (e.g. type & amount of food, lack of exercise) influences their rate of aging & how healthy they will be for the rest of their life.","Previous findings for the genetic and environmental contributions to DNA methylation variation were for limited age ranges only. We investigated the lifespan contributions and their implications for human health for the first time. 1,720 monozygotic twin (MZ) pairs and 1,107 dizygotic twin (DZ) pairs aged 0-92 years were included. Familial correlations (i.e., correlations between twins) for 353,681 methylation sites were estimated and modelled as a function of twin pair cohabitation history. The methylome average familial correlation was around zero at birth (MZ pair: -0.01; DZ pair: -0.04), increased with the time of twins living together during childhood at rates of 0.16 (95%CI: 0.12-0.20) for MZ pairs and 0.13 (95%CI: 0.07-0.20) for DZ pairs per decade, and decreased with the time of living apart during adulthood at rates of 0.026 (95%CI: 0.019-0.033) for MZ pairs and 0.027 (95%CI: 0.011-0.043) for DZ pairs per decade. Neither the increasing nor decreasing rate differed by zygosity (both P>0.1), consistent with cohabitation environment shared by twins, rather than genetic factors, influencing the methylation familial correlation changes. Familial correlations for 6.6% (23,386/353,681) sites changed with twin pair cohabitation history. These sites were enriched for high heritability, proximal promoters, and epigenetic/genetic associations with various early-life factors and late-life health conditions. Early life strongly influences DNA methylation variation across the lifespan, and the effects are stronger for heritable sites and sites biologically relevant to the regulation of gene expression. Early life could affect late-life health through influencing DNA methylation.","Epigenetic modifications regulate gene expression without changing the underlying DNA sequence, and have been proposed to play a critical role in the aetiology of complex human traits and diseases. DNA methylation, one of the most studied epigenetic modifications, has been found to be associated with several human traits and diseases, such as smoking and obesity. Understanding the causes of DNA methylation variation could inform the determinants and biological mechanisms that affect human health through influencing DNA methylation. Twin studies are classic designs for understanding the contributions of (unmeasured) genetic and environmental causes to the variation of human traits, including DNA methylation. Although twin studies might not necessarily measure any genetic or environmental factors, they infer the genetic and environmental contributions to variation by comparing the twin resemblance in the trait of interest between monozygotic (MZ) and dizygotic (DZ) twin pairs. Under the null hypothesis that genetic factors do not influence variation in the trait, MZ pairs will have the same resemblance as DZ pairs. If only additive genetic factors influence familial correlation in the trait (i.e., there is no effect of having a shared environment), the MZ pair correlation will be twice the DZ pair correlation. Under this latter assumption, the heritability of the trait (expressed as a percentage) can be estimated as 100 times the minimum of: (i) twice the difference between the MZ and DZ pair correlations and (ii) the MZ pair correlation.
Twin studies have considered a measure of the overall proportion of DNA methylation variation explained by genetic factors, which is defined as the average of the heritability estimates across the measured sites. The average heritability of the sites measured by the HumanMethylation27 and HumanMethylation450 BeadChip arrays has been found to range from 3% to 20% and vary with the age of twins, being highest in adolescence and young adulthood and lowest at birth and in middle age. There is, however, evidence that environmental factors shared between twins explain a substantial proportion of methylation variation at age of 18 years and middle age. These studies investigated specific and limited age ranges; therefore, they could only provide evidence for the investigated age range, but not for the whole lifespan.
We previously pooled DNA methylation data from studies in which the age of twins covered the whole lifespan to solve the issues that studies of limited age ranges found different estimates of the genetic and environmental contributions to the variation in methylation-based measures. We found evidence that the variation in both genome-wide average methylation and epigenetic age are consistent with being influenced by environmental factors shared by twins when they cohabit, and that these effects can persist across the whole lifespan. Such findings are not possible to be found by studies focusing on limited age ranges.
To address the issue that different genetic and environmental contributions to methylation variation have been found in different age ranges, here we investigated the determinants of methylation variation across the lifespan by combining and analysing data for 5, twins from nine twin studies. We also considered the implications of our findings for human health by leveraging published epigenetic and genetic associations with human diseases and traits.","In conclusion, our findings are consistent with early
life critically determining DNA methylation variation
across the lifespan. The effects persist during the whole
lifespan, and are stronger for methylation sites affected
by genetic factors and sites biologically relevant to gene
expression regulation. The variability of a substantial
number of DNA methylation sites change with cohabitation and these sites are enriched for genetic and epigenetic associations with a variety of early-life factors
and late-life health conditions, implying that early life
could affect late-life health through influencing DNA
methylation.",https://www.thelancet.com/journals/ebiom/article/PIIS2352-3964(22)00111-6/fulltext,
Circulating linoleic acid at the time of myocardial infarction and risk of primary ventricular fibrillation,Walnuts are an excellent source of an omega-3 called alpha-linoleic acid (ALA). People who have higher blood levels of ALA during a heart attack are less likely to experience the muscle misfires that cause the heart to stop beating,"Primary ventricular fibrillation (PVF) is a major driver of cardiac arrest in the acute phase of ST-segment elevation myocardial infarction (STEMI). Enrichment of cardiomyocyte plasma membranes with dietary polyunsaturated fatty acids (PUFA) reduces vulnerability to PVF experimentally, but clinical data are scarce. PUFA status in serum phospholipids is a valid surrogate biomarker of PUFA status in cardiomyocytes within a wide range of dietary PUFA. In this nested case–control study (n = 58 cases of STEMI-driven PVF, n = 116 control non-PVF STEMI patients matched for age, sex, smoking status, dyslipidemia, diabetes mellitus and hypertension) we determined fatty acids in serum phospholipids by gas-chromatography, and assessed differences between cases and controls, applying the Benjamini–Hochberg procedure on nominal P-values to control the false discovery rate (FDR). Significant differences between cases and controls were restricted to linoleic acid (LA), with PVF patients showing a lower level (nominal P = 0.002; FDR-corrected P = 0.027). In a conditional logistic regression model, each one standard deviation increase in the proportion of LA was related to a 42% lower prevalence of PVF (odds ratio = 0.58; 95% confidence interval, 0.37, 0.90; P = 0.02). The association lasted after the inclusion of confounders. Thus, regular consumption of LA-rich foods (nuts, oils from seeds) may protect against ischemia-driven malignant arrhythmias.","Coronary artery disease (CAD) remains a major health challenge and a top cause of global mortality. Though advances in drugs and device-based therapies have largely reduced complications and improved outcomes for those who experience a myocardial infarction (MI), the rate of primary ventricular fibrillation (PVF) has remained stable over time. PVF is a major trigger of out-of-hospital cardiac arrest leading to sudden cardiac death, and patients developing ventricular fibrillation during acute MI are at higher risk of in-hospital mortality5. Therefore, novel strategies are needed to prevent and manage acute ischemic PVF.

Experimental research has been instrumental in understanding the arrhythmogenic mechanisms leading to PVF. In this regard, many metabolic and electrophysiological cardiac changes observed in the acute ischemic phase underlie disturbances in voltage-gated channels6. A large body of evidence indicates that polyunsaturated fatty acids (PUFA) acylated in the phospholipids constituting the lipid bilayers of cell membranes modulate the activity of voltage-gated channels by either altering the biophysical membrane properties (indirect effect) or binding to the protein upon cleavage from cell membrane phospholipids (direct effect). The finding that dietary fats are readily incorporated into the cell membranes of cardiomyocytes put for the hypothesis that sustained dietary PUFA intake and the ensuing enrichment in cardiac phospholipids might reduce myocardial vulnerability to early PVF in MI9.

PUFA include mainly omega-3 (n-3) and omega-6 (n-6) fatty acids. n-3 PUFA, particularly those of marine origin (eicosapentaenoic acid [C20:5n-3, EPA] and docosahexaenoic acid [C22:6n-3, DHA]), have been shown to possess an array of cardioprotective effects. Regarding n-6 PUFA, in particular linoleic acid (C18:2n-6, LA), although sustained intake of LA has been mechanistically linked to an increased low-density lipoprotein oxidation and to the transformation to arachidonic acid (C20:4n-6, AA—a precursor of proinflammatory lipid mediators), there is increasing evidence of the cardioprotective benefits of LA intake within the range advocated by the American Heart Association11. Experimental research uncovered a long time ago that replacement of saturated animal fat in the diet with either LA-rich or n-3-rich oils reduced the incidence and severity of arrhythmias occurring in ischemia9. However, this notion barely translated into clinical research. We hypothesized that in patients with ST-elevation myocardial infarction (STEMI), cardiac enrichment in specific fatty acids resulting from the consumption of fat-rich foods during the weeks prior to the event would influence the vulnerability of the myocardium to develop PVF. To address this issue, at hospital admission for STEMI, we performed lipidomics in 58 patients who developed PVF and in 116 non-PVF controls matched for cardiovascular risk factors, searching for associations between fatty acid species and incident PVF. Given that routine myocardial biopsy is not safe in the acute phase of STEMI, we determined fatty acidsin serum phospholipids, the status of which changes in parallel with heart phospholipids within a wide range of dietary fats.","In this case–control, prospective study enrolling a large cohort of STEMI-driven PVF patients and matched
non-PVF STEMI controls, we determined the fatty acid status of serum phospholipids at hospital admission for
STEMI. Tese objective lipidomic biomarkers mirror not only dietary intake during the previous weeks, but also
the fatty acid status in inner membranes, including cardiomyocytes. We found that a lower prevalence of PVF
was associated with increasing levels of LA, an essential n-6 fatty acid naturally found in nuts (such as walnuts,
pine nuts, pistachios and almonds) and seed oils (such as canola, corn, safower, soybean, and sunfower oils).
Our fndings are clinically relevant in two ways. First, we provide novel clinical evidence of a modifable lifestyle (dietary) factor that is related to a lower risk of PVF, an endpoint that is a major contributor to short-term
mortality in STEMI survivors. PVF is difcult to predict, and occurs at persistently high rates despite advances
in the era of PPCI. Terefore, identifying any easily accessible and safe strategy to reduce the risk for PVF in
STEMI patients is needed. Second, because LA in serum phospholipids refects dietary levels of this essential
fatty acid we suggest that intake of this n-6 PUFA may protect against PVF. Tis observation was repeatedly reported in animal models11 and may be mechanistically explained by changes in membrane cells14 upon
incorporation of dietary LA into the phospholipids, as occurs with other dietary PUFA12. Notably, because LA
can be transformed into AA, a substrate of many proinfammatory lipid mediators, dietary LA is widely perceived to promote infammation, contributing to cardiovascular disease. Although LA can be converted to a
few infammatory and vasoconstrictory lipid mediators15, there is a growing body of evidence for the potential
cardio-metabolic benefts of LA, including a recent landmark study pooling data from 30 prospective cohorts,
which reported that higher in vivo circulating and tissue levels of LA were associated with lower risk of major
cardiovascular events. Our data contribute to countering the demand to remove LA from the diet based on its
proposed harmful efects. Indeed, seed oils that used to be rich sources of LA, such as safower and sunfower oils,
have now been hybridized to substantially remove LA. According to a recent modeling experiment, this shif may
be placing the population (children in particular) at increased risk of a defciency in essential fatty acids, including LA. Studies such as ours may help build a frmer evidence base for the benefts of LA, which will hopefully
slow (or even reverse) these well-intentioned but, in our view, misguided eforts to remove LA from the diet.
Interestingly, we failed to fnd signifcant associations for marine-derived n-3 fatty acids. Tough a large body
of observational evidence exists on the benefts of dietary EPA and DHA against sudden cardiac death, the issue
of whether dietary EPA and DHA may protect against PVF in the setting of STEMI remains unsettled19, with
randomized controlled trials conducted in patients with chronic myocardial scars wearing implantable cardioverter defbrillators who already have a history of ventricular arrhythmia20–22. Further research is warranted to
clarify efects of dietary n-3 PUFA on PVF.
Tis study had several limitations. First, its observational nature precludes establishing causality between
circulating LA (or dietary LA intake) and prevention of PVF in the setting of STEMI. Such a link could only
be confrmed by a randomized controlled trial of dietary supplementation with LA before the occurrence of
STEMI—which practically speaking would be challenging to design. Second, we used the fatty acid profle of
serum phospholipids. Although the use of this lipidomic-based objective biomarker of long-term dietary fatty
acid intake circumvents the disadvantages of self-reported dietary data (i.e., food diaries and food frequency
questionnaires), the fatty acid profle of serum phospholipids does not refect long-term intake as accurately as
the fatty acids in adipose tissue or red blood cells. Tird, dietary LA in Mediterranean populations is largely
supplied by nuts and seeds23, which contain many cardioprotective bioactive agents in addition to PUFA24.
Terefore, we cannot rule out the parent foods themselves being the actual protective agent and LA just a marker
of their consumption. Finally, because some potential health-related confounding variables (i.e., socioeconomic
status, education, adherence to Mediterranean diet) were not available, we could not exclude the possibility that
uncaptured environmental factors may have infuenced or caused the observed association.
In conclusion, we identifed an association of elevated LA in serum phospholipids at the time of STEMI with a
lower risk of PVF. Tus, sustained consumption of sources rich in LA may reduce the risk of ischemia-driven cardiac arrest. Our results, which concur with experimental data and suggested membrane-based benefts ascribed to
dietary LA, contribute to dispel the notion that the entire n-6 family of fatty acids promote cardiovascular disease.",https://pubmed.ncbi.nlm.nih.gov/35288655/,
Restoring nuclear entry of Sirtuin 2 in oligodendrocyte progenitor cells promotes remyelination during ageing,"As the brain ages, white matter volume shrinks as nerve cell sheaths are lost. Replenishing NAD levels via NMN supplementation activates SIRT2 to restore remyelination in aged mice, helping the aged brain function more youthfully","The age-dependent decline in remyelination potential of the central nervous system during ageing is associated with a declined differentiation capacity of oligodendrocyte progenitor cells (OPCs). The molecular players that can enhance OPC differentiation or rejuvenate OPCs are unclear. Here we show that, in mouse OPCs, nuclear entry of SIRT2 is impaired and NAD+ levels are reduced during ageing. When we supplement β-nicotinamide mononucleotide (β-NMN), an NAD+ precursor, nuclear entry of SIRT2 in OPCs, OPC differentiation, and remyelination were rescued in aged animals. We show that the effects on myelination are mediated via the NAD+-SIRT2-H3K18Ac-ID4 axis, and SIRT2 is required for rejuvenating OPCs. Our results show that SIRT2 and NAD+ levels rescue the aged OPC differentiation potential to levels comparable to young age, providing potential targets to enhance remyelination during ageing.","The increasingly ageing global population has ignited great interest in identifying rejuvenating molecules that may delay ageing and enhance the regeneration of the aged central nervous system (CNS). The ensheathment of axons by myelin gives structure to saltatory conduction and provides metabolic support to maintain both axonal functional integrity and long-term survival. Therefore, proper myelination is not only a prerequisite for, but also a consequence of, normal CNS activity. As the human brain ages, white matter volume shrinks more prominently than grey matter and some myelin sheaths exhibit myelin ageing which in turn drives CNS ageing. The reasons for myelin ageing remain unclear. However, the decreased remyelination capacity of oligodendrocyte progenitor cell (OPC) is one of the popularly proposed mechanisms.

Demyelination is the key pathological feature of the autoimmune inflammatory diseases of CNS such as multiple sclerosis (MS) and an early pathological hallmark of neurodegenerative diseases. Consequently, demyelination may cause devastating irreversible axonal degeneration. Remyelination by oligodendrocytes differentiated from OPCs occurs throughout life. Unfortunately, with ageing, the efficiency of remyelination declines mainly due to the reduced capacity of OPC differentiation. Such decline leads to disability in MS, whose course usually spans several decades and progresses with ageing. It is essentially untreatable when MS develops into a progressive phase. Identifying new molecular targets in OPC to rejuvenate the aged OPC therefore holds much promise for this unmet medical need.

As the central player for myelination and remyelination, OPCs are distributed throughout the CNS. The ageing of OPCs is orchestrated by epigenetic mechanisms likely involving sirtuins, the nicotinamide adenine dinucleotide (NAD+)-dependent histone deacetylases. Among the seven members of sirtuins, it remains elusive which members of the sirtuin family are expressed in the OPC. Sirtuin 2 (SIRT2) is the only member which is predominantly localised in the cytoplasm, and its role in the CNS remains largely unclear. So far, although mature oligodendrocytes express SIRT2, it is not clear whether OPCs in the CNS express SIRT2, or what role SIRT2 plays in remyelination in the aged CNS.

Enhancing remyelination in the aged CNS of animal models has proven to be an effective strategy to rejuvenate aged OPCs, and recent studies have demonstrated that a youthful systemic environment, fasting, or metformin18 can restore remyelination efficiency in the aged CNS, indicating that the aged OPC can be rejuvenated. Emerging evidence suggests that the metabolomic profile defines an OPC’s cell fate and shapes its ability to differentiate. However, so far it is not clear how the endogenous metabolic fingerprints of OPCs are affected by ageing.

Herein, we report that the depletion of SIRT2 and its nuclear localisation together with declined NAD+ are features of aged OPCs. Importantly, supplementation of NAD+ by β-nicotinamide mononucleotide (β-NMN) induces re-expression and restores nuclear entry of SIRT2 in OPCs, and rejuvenates aged OPCs by promoting them differentiating into mature oligodendrocytes and eventually enhances new myelin generation in the aged CNS. Mechanistically, we reveal that SIRT2 is necessary for the effect of NAD+ on OPCs. This study identifies SIRT2 as a molecular target for OPCs, and by restoring the nuclear entry of SIRT2 in OPCs, β-NMN delays myelin ageing in the normal CNS and enhances remyelination in the demyelinated aged CNS.","The present study identifies SIRT2 as a molecular target for OPC. Werner et al. and Li et al. reported that SIRT2 is expressed throughout the oligodendrocyte lineage cells, and exclusively in mature oligodendrocytes in the adult CNS. The transcriptome database provided by Cahoy et al. and Zhang et al. also revealed that the mRNA of SIRT2 is enriched in oligodendrocytes lineage cells. In this work, we showed that the sirt2 mRNA in cultured OPCs was overwhelmingly dominant over any of the other six members within the sirtuin family, and ranked the second highest among various cell types in the brain, just next to mature oligodendrocytes. Consistent with previous results, we demonstrated that SIRT2 was indeed exclusively localised to the cytoplasm of mature oligodendrocytes. However, surprisingly, we further revealed that the SIRT2 protein was transiently expressed in the nucleus of OPCs during myelin development while depleted in OPCs of the adult CNS, indicating that SIRT2 is a specific target for oligodendrocyte lineage cells. The switch of SIRT2 localisation from OPCs during development to mature oligodendrocytes in the adult CNS is reserved to mammals and, in particular, primates. Interestingly, in the adult CNS, demyelination induced the re-expression of SIRT2 in the nuclei of OPCs, recapitulating a developmental program. However, such recapitulation was impaired in the aged CNS, corelating with impaired remyelination in the aged CNS.

Given that nuclear SIRT2 expression occurred during myelin postnatal development, and this program was recapitulated within the demyelination lesion in the adult CNS, we next explored the function of SIRT2 in OPCs. To clearly investigate the role of SIRT2 specifically in OPCs in vivo, we used NG2-CreERT; Sirt2flox/flox mice, and i.p. injected tamoxifen to exclusively delete SIRT2 in OPCs in a temporally controllable manner. Our results showed that the conditional knockout of SIRT2 in OPCs reduced the frequency of remyelinated axons, decreased the thickness of newly formed myelin, and reduced the frequency of normal-looking newly formed myelin. These results provide direct evidence that SIRT2 in OPCs indeed plays a critical role in remyelination, and have established a causality between the loss of SIRT2 in OPCs and impaired remyelination. Intriguingly, the autoantibody to SIRT2 is prevalent in the cerebrospinal fluid of secondary progressive MS patients48, whose pathological feature is failed remyelination. In addition, SIRT2 protein levels diminished within the lesion in both the EAE model and in MS patients.

Next, we demonstrate that elevating NAD+ with β-NMN delays myelin ageing and enhances myelin repair in the aged CNS. Using G3 Terc−/− mice, a premature ageing OPC model established in this work, we revealed that NAD+ was one of the top depleted metabolites in the aged OPCs. The present study provides a comparison of the metabolic fingerprints of OPCs between young and old, laying a foundation for future studies concerning the metabolism of aged OPCs. Since NAD+ is a regulator of SIRT2 activity, we supplemented β-NMN, the immediate precursor of NAD+51, in a dose comparable to the clinical trial in which the long-term clinical safety of β-NMN has been proven, and found that NAD+ repletion restored the nuclear entry of SIRT2 in the aged OPCs and delayed myelin ageing. Furthermore, we demonstrated that NAD+ supplementation enhances remyelination efficiency both ultrastructurally and functionally in the aged CNS in both a preventive and therapeutic way; these results highlight the potential for this work to go towards clinical translational studies, bringing hope to potential treatments for the progressive phase of MS, a so far unmet therapeutic challenge.

To elucidate the molecular mechanism underlying the effect of NAD+ on OPCs, by using multiple techniques, we unveiled that nuclear SIRT2 deacetylates H3K18, thus making chromatin condensed and repressed. This leads to the decreased transcription of ID4 and further promoting OPC differentiation by disinhibiting the transcription of myelin genes such as MBP. Importantly, SIRT2 is necessary for the effect of NAD+ on OPCs. Interestingly, the localisation switch of SIRT2 is similar to that of ID4, which also switches from the nuclei of OPCs to the cytoplasm of mature oligodendrocytes shown in the previous study. The current work reveals that the effects of β-NMN are mediated through NAD+-SIRT2-H3K18Ac-ID4 sequentially, and that SIRT2 is necessary for NAD+ to rejuvenate the aged OPCs. Nevertheless, the process by which NAD+ causes nuclear entry of SIRT2 in OPC awaits further study. For translational purposes, oral administration is desirable in view of the effects of oral administration of β-NMN on OPCs, other neural functions, and other systems need to be systematically examined.

Recent studies have shown that in young mice, supplementing unstable NAD+ decreases the EAE score, correlating with beneficial Th1/Th17 immune responses or helpful CD4+ T cell differentiation, and supplying niacin, an indirect NAD+ precursor, modulates macrophages19, suggesting that NAD+ has versatile targets. Our work clarifies that β-NMN rejuvenates aged OPCs, the central player for myelination and remyelination12, by restoring the nuclear entry of SIRT2.

Together, we report that the nuclear entry of SIRT2 is depleted and NAD+ is substantially reduced in the aged OPCs. Supplementing β-NMN restores the nuclear entry of SIRT2 and rejuvenates aged OPCs by promoting their differentiation into mature oligodendrocytes and eventually enhances new myelin generation in the aged CNS. Mechanistically, the effects of β-NMN are mediated through NAD+-SIRT2-H3K18Ac-ID4 sequentially, and SIRT2 is necessary. The present study identifies SIRT2 as a molecular target for rejuvenating the aged OPCs, and by restoring nuclear entry of SIRT2 in the aged OPCs, β-NMN delays myelin ageing in normal and enhances remyelination in demyelinated aged CNS, paving the way for future clinical translation study.",https://www.nature.com/articles/s41467-022-28844-1#Abs1,
Improving the accuracy of epigenetic biological age clocks using combinatorial DNA methylation site selection methods,"Novel feature selection methods for construction of accurate
2 epigenetic clocks. Chemicals called methyls are added and subtracted to our DNA over time. Our method uses a combination of machine learning computations to figure out which DNA methylation sites (and how many) are the best to measure in order to estimate someone’s epigenetic (biological) age","Epigenetic clocks allow the accurate prediction of age based on the methylation status of specific CpG sites in a variety of tissues. These predictive models can be used to distinguish the biological age of an organism from its chronological age, and are a powerful tool to measure the effectiveness of aging interventions. There is a growing need for methods to efficiently construct epigenetic clocks. The most common approach is to create clocks using elastic net regression modelling of all measured CpG sites, without first identifying specific features or CpGs of interest. The addition of feature selection approaches provides the opportunity to reduce the cost and time of clock development by decreasing the number of CpG sites included in clocks. Here, we apply both classic feature selection methods and novel combinatorial methods to the development of epigenetic clocks. We perform feature selection on the human whole blood methylation dataset of ~470,000 CpG features published by Hannum and colleagues (2015). We develop clocks to predict age, using a variety of feature selection approaches, and all clocks have R2 correlation scores of greater than 0.73. The most predictive clock uses 35 CpG sites for a R2 correlation score of 0.87. The five most frequent sites across all clocks are also modelled to build a clock with a R2 correlation score of 0.83. These two clocks are validated on two external datasets where they perpetuity. It is made available under aCC-BY 4.0 International license.
preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in maintain excellent predictive accuracy and outperform Hannum et al’s model in accuracy of age prediction despite using significantly less CpGs. We also identify the associated gene regulatory regions of these CpG sites, which may be possible targets for future aging studies. These novel feature selection algorithms will lower the number of sites needed to be sequenced to build clocks and allow conventionally expensive aging epigenetic studies to cost a fraction of what it would normally.","Epigenetic clocks allow for the prediction and observation of biological aging (Bocklandt, 2011). By profiling the methylation levels at specific sites in DNA, it is possible to accurately predict the age of organisms and tissues (Horvath 2013). This is often referred to as epigenetic or DNA methylation (DNAm) age. CpG sites are areas of repetitive DNA bases where a guanine follows a cytosine, which can be modified via DNA methylation and demethylation to alter the structure of chromatin and gene expression in a cell (Moore et al., 2021). Epigenetic clocks can now predict age across multiple species and tissue types (Thompson et al., 2018), and even predict mortality (Lu, Quach et al 2019). With the increased use of DNA methylation clocks to determine biological age and screen for interventions that slow or reverse aging the demand for more robust, accurate clocks is growing.
The first epigenetic clocks were created by Bocklandt et al (Bocklandt et al, 2011) and quickly followed by the Hannum and Horvath labs in 2013 (Hannum et al, 2013; Horvath 2013). The Hannum clock, based on methylation analysis of DNA from peripheral blood mononuclear cells, was developed using elastic net regression modelling. 71 markers were selected from over 470,000 CpG sites to derive an age prediction accuracy of four years (Hanuum et al. 2013). Horvath’s clock encompasses multiple tissue types and includes 353 CpG sites that strongly predict age (Horvath 2013). Recently, there has been a focus on creating clocks with fewer CpG sites to enable epigenetic age profiling without the use of costly microarrays or expensive reduced-representation bisulfite sequencing (Ito el al. 2018, Park JL, et al. 2016, Zbieć-Piekarska et al. 2014, Spólnicka, M. et al. 2017). Alghanim et al.'s clock, built on blood methylation data, only uses CpG sites from three gene regions to explain 84-85% of age variance (Alghanim et al. 2017), and Weidner’s clock based on only 3 CpG sites, is able to predict age with an error of less than five years (Weidner et al., 2014). Few epigenetic clock studies employ a discrete step to find optimal features for building clocks. Feature selection is commonly used in situations where the number of features far outnumber the number of samples (Guyon et al. 2003). Given the vast number of CpG sites in the genome and the relatively low number of samples in most studies, feature selection methods will improve the efficiency of clock building. Currently, the most common approach for clock building is to use a ‘correlation-with-age’ method, where CpGs that have a non-zero coefficient in ElasticNet Regression analyses are given more predictive power in the model (Horvath 2013, Hannum et al. 2013). Some clocks utilize more advanced feature selection methods such as Boruta (Renner et al., 2013), recursive feature selection (Wang et al., 2018; Darst, Malecki and Engelman, 2018; Meng, Murrelle and Li, 2008) or neural networks (Spólnicka et al. 2017). These algorithms select even fewer CpG features whilst still accurately predicting age. However the number of clocks being built with these tools is minimal and there is more room to optimise feature selection methods and parameters.
Here, we use several feature selection approaches to construct accurate epigenetic clocks with low numbers of CpG sites on the publicly available Hannum dataset (GSE40279), and evaluate their accuracy and generalizability on other datasets: GSE52588 (Horvath et al, 2015), GSE137688 (McEwen 2019), GSE85311 (Martens et al, 2020). We use a combination of modified standard methods that are readily available in python packages as well as the development of our own novel selection methods. We combine methods and use them in tandem to form new methods of feature selection, and optimise the development of epigenetic clocks to predict age.","Overall we show feature selection methods can select CpG sites that are highly predictive of age, allowing for less features needed to build an accurate epigenetic clock. Many different types of feature selection methods are able to attain a reasonably high correlation score of around 0.75-0.85 whilst using a low number of CpG features. The rudimentary base code that outlines most of the feature selection ideas in this paper is publicly available and we hope that feature selection becomes a standard discrete step in future epigenetic clock studies. The corresponding genes of the most common CpG sites in these clocks are possible future targets for aging studies. Two of our clocks, both trained on the original Hannum dataset, also performed well on two external datasets. The models, in fact, performed higher on validation datasets than the training dataset, and outperformed Hannum’s original clock that uses 71 features. This validates both the feature selection methods' ability to reliably select good CpGs and the construction of our clocks. These clocks are thus able to be used by others reliably to serve as predictors of chronological age. We also applied these models to a dataset of a different sample type; buccal epithelial cells. Although the r2 scores were only moderate for this dataset, the mean and median absolute errors were the lowest we observed. This suggests an interesting future potential for buccal/saliva methylation samples, as they are much more accessible and less expensive to obtain. In addition to the validation of the clocks, we also tested whether the identified CpGs of two of these methods could be used to make accurate clocks using the Horvath down syndrome dataset (Horvath et al, 2015, GSE52588). These clocks still achieved high 0.91-0.92 r2 scores (Table 4). This suggests that these features and their ability to predict age are not dataset specific and can universally be used across other methylation datasets. We identified five CpGs and their corresponding genes that were of particular interest, as they were most commonly identified across all feature selection methods in our study (Table 3). Four of these CpG sites, and particularly ELOVL2, have been previously identified as strong predictors of age. ELOVL2, C1orf132, FHL2 and CCDC102B are included in an online seven CpG site epigenetic clock from the University of Santiago de Compostela (Mathgene, 2021). Zbieć-Piekarska et al constructed a linear regression model using only ELOVL2’s CpG site (cg16867657) to predict age (Zbieć-Piekarska et al., 2015) and obtained a high degree of accuracy in blood samples from humans. By manipulating the expression of ELOVL2 and observing age-related changes in the eyes of mice, Chen et al suggest that the gene is a molecular regulator of aging in the retina. Spólnicka and colleagues used ELOVL2 to accurately detect age differences from 3 disease groups (Spólnicka et al., 2018), and also highlight C1orf132 and FHL2 as key genes from which CpG sites are used for their epigenetic clock. CCDC102B also has links to aging and age-related degenerative diseases (Hosoda et al., 2018, Xia et al., 2018). Ito and colleagues developed a clock using only the CpG sites associated with CCDC102B and ELOVL2 (Ito et al., 2018) and are able to predict age with an r2 of 0.75. Additionally, Fleckhaus et al.'s study develops a clock using 8 target regions, four of which are ELOVL2, FHL2, CCDC102B and C1orf132 (Fleckhaus etc al, 2020). These papers show that our feature selection methods are able to select the most age-predictive CpG sites, consistently with other studies.
OTUD7A is the fifth gene of interest that we identified with our methods, but is the least documented. One study has previously identified that high methylation rates of CpG sites associated with OTUD7A are correlated with age (Tharakan et al., 2020), and Yin et al. identified it as a potential regulator for neurodevelopmental disorders (Yin et al.,2018). The role of OTUD7A in aging, if any, is not well-known and should be explored further. We also applied a neural network method for feature selection in this study, but found it was not as powerful in terms of predictive accuracy as the other feature selection methods. However, this method did select many CpG features that were missed by our other conventional and novel methods. As neural network architecture becomes more advanced in its ability to read in larger datasets, the features it selects may eventually rival the accuracy of other methods. The features identified with neural networks may also give rise to new sets of CpG sites and genes worthy of study in aging.
The feature selection methods we introduce here overcome the common computational issues of stock selection methods and select a low-number of CpG sites whilst still yielding predictions of age that have high accuracy. These methods can be applied to a range of future studies developing epigenetic clocks including across new tissue types, or by examining a limited subset of CpGs in mutual overlap between bulk methylation and single cell datasets (Trapp et al, 2021). Parallelized, highly cost-reduced methods targeting specific CpG regions (Griffin et al. 2021) are another prime example. Lastly, these methods are not limited to the identification of CpG sites as features, and this pipeline could be used to identify features for biomarkers or clocks developed from a range of datasets (eg. metabolomics, microbiome, clinical data), and to predict a variety of age and health outcomes.",https://www.biorxiv.org/content/10.1101/2022.02.21.481326v1,
Taiji-reprogram: a framework to uncover cell-type specific regulators and predict cellular reprogramming cocktails,We demonstrate the superior power of this approach on predicting reprogramming cocktails compared to the existing popular methods.,"Cellular reprogramming is a promising technology to develop disease models and cell-based therapies. Identification of the key regulators defining the cell type specificity is pivotal to devising reprogramming cocktails for successful cell conversion but remains a great challenge. Here, we present a systems biology approach called Taiji-reprogram to efficiently uncover transcription factor (TF) combinations for conversion between 154 diverse cell types or tissues. This method integrates the transcriptomic and epigenomic data to construct cell-type specific genetic networks and assess the global importance of TFs in the network. Comparative analysis across cell types revealed TFs that are specifically important in a particular cell type and often tightly associated with cell-type specific functions. A systematic search of TFs with differential importance in the source and target cell types uncovered TF combinations for desired cell conversion. We have shown that Taiji-reprogram outperformed the existing methods to better recover the TFs in the experimentally validated reprogramming cocktails. This work not only provides a comprehensive catalog of TFs defining cell specialization but also suggests TF combinations for direct cell conversion.","Transcription factors (TFs) play pivotal roles during development, cell type specification and aging. Identification of the key TFs in each cell type would facilitate understanding the regulatory mechanisms that decide cell fate and cell identity, and provide clues and intervention strategies for cell fate determination. The best known example for cellular reprogramming is the generation of induced pluripotent stem cells (iPSCs) from somatic cells by the introduction of four TFs (Oct4, Sox2, Klf4 and c-Myc), which demonstrated that using TFs as reprogramming factors can induce drastic cell conversion. Subsequently, transdifferentiation between different pairs of terminally differentiated cell types without going through a pluripotent state has been achieved.

Cellular reprogramming opens new doors towards understanding the mechanisms underlying development as well as developing new cell therapy. A major roadblock toward achieving cell conversion is to develop effective reprogramming cocktails. The main challenges include how to identify key regulators in the source and target cell types/tissues that can serve as the candidate reprogramming factors and how to efficiently consider the exponentially increasing combinations given a set of candidate TFs. Furthermore, as epigenetic state is crucial in deciding cell state and cell type specificity, how to consider the epigenomes of the source and target cell types/tissues is also pivotal to developing reprogramming cocktails.

Efforts have been devoted to addressing these challenges. High expression level is useful to select important TFs in a particular cell type while its limitation is also well acknowledged as the activity of a TF can be regulated through post-translational modifications and other non-transcriptional mechanisms. Methods such as Schacht et al. and Arrieta-Ortiz et al. have been developed to consider the target genes of a TF whose expression levels reflect the TF’s regulatory activity. Other methods including CellNet, Mogrify and PANDA predict the key TFs by reconstructing genetic network based on expression or protein–protein interaction data and how the combinations of TFs would regulate the differentially expressed genes in the source and target cell types in cellular conversion. While such an approach has helped to identify key TFs that are not found by only considering their own expressions, it does not fully consider the regulatory effect of a TF propagating through the genetic network, i.e. a TF impacts not only its direct targets but also their descendants and the feedback from the descendants to the TF is also crucial to affect the phenotypic outcome. Therefore, considering the complexity of the genetic network and assessing the global importance of a TF in the network is critical to identify the key regulators for cell type/tissue specification.

Previously we developed a systems biology method called Taiji that integrates transcriptomic and epigenomic data to construct a cell-type specific genetic network, based on which the global importance of each TF is evaluated by a personalized PageRank algorithm. We have shown that Taiji is robust and resistant to noise that is unavoidable in constructing genetic networks. The effectiveness of identifying key regulators by Taiji has been confirmed using simulated data and experimental validations.

Here, we leverage the power of Taiji to develop a systematic approach called Taiji-reprogram for efficiently developing reprogramming cocktails. Taking advantage of the vast amount of epigenomic data generated by the ENCODE and the NIH Epigenomics Roadmap projects, we have applied Taiji to identify key TFs in diverse cell types and tissues. Because the epigenomic data are highly cell-type specific, the genetic network constructed by Taiji captures the regulatory interactions specifically present in a particular cell type and the key regulators with the most global importance in the network are expected to be tightly associated with cell-type specific functions. Using the PageRank scores of the top TFs, we can efficiently evaluate the TF combinations and find the most promising cocktails for a defined reprogramming task. We showed the superior performance of our approach in comparison with the existing methods on identifying reprogramming factors in the experimentally achieved cocktails.","We present here a comprehensive identification of TFs that are key regulators of deciding cell specificity in diverse human cell types and tissues using a systems biology approach called Taiji. Taiji integrates gene expression and epigenomic data (open chromatin or histone modification) to construct a genetic network. Compared to using protein–protein interaction networks that are not cell-type specific in other methods, Taiji analyzes expression and epigenomic data in the cell type or tissue under consideration, which better represents cell-type/tissue-specific regulatory interactions. Importantly, the PageRank score of a TF naturally considers the impacts of its upstream regulators and downstream regulatees in the network including the feedback from its regulatees. Therefore, the PageRank score represents the global importance of a TF in the genetic network, i.e. the top ranked TFs by PageRank score are master regulators and any perturbation on them would have significant impact on the network.

By comparing the key TFs found in each cell type/tissue, we successfully identified lineage-specific and cell-type/tissue-specific regulators including many well-known key regulators. In particular, the previously unknown TFs were uncovered to be responsible for tissue development and differentiation in the human embryonic and postnatal stages, which can serve as a valuable reference for future mechanistic studies to better understand the regulatory mechanisms of development. Furthermore, given the fast advancement of mapping human tissues using RNA-seq, ATAC-seq and other epigenetic assays, our analysis is readily applicable to these data and can provide a powerful way to integrate gene expression and epigenomic data at the systems level. Importantly, Taiji is applicable to individual data sets and thus expansion to include additional data is straightforward. Comparative analysis on more diverse cell types and tissues would also better define cell/tissue-specific regulatory roles of TFs and further our understanding of the mechanisms underlying cell/tissue specification.

By leveraging the TF PageRank scores’ measurement of global importance in individual cell-types, we developed a systematic approach to identifying TF cocktails that can convert one cell type to another. This new approach is straightforward and computationally efficient. By considering the TFs that have the most differential PageRank scores in the target and source cells, we find the candidate regulators whose perturbation would likely facilitate cell conversion. Given the PageRanks scores, we can easily consider a large number of TF combinations to select the most promising reprogramming cocktails. In particular, because the PageRank scores reflect the global importance of the TFs, we can directly use the product of PageRank scores of the TFs in a cocktail to rank the candidate TF combinations. This way, we avoid the heuristic process to select a set of differentially expressed genes between the target and source cells to score the reprogramming TF combinations. We demonstrated the superior power of this approach on predicting reprogramming cocktails compared to the existing popular methods. The functional analysis of novel TF candidates in reprogramming cocktails indicates the potential roles in the conversion between different cell types, which can guide further experimental investigations. As the ongoing efforts such as Human Cell Atlas aim to measure transcriptome and epigenome in all the cell types of the human body, our approach will be readily applicable to identify key regulators defining the cell types and develop reprogramming cocktails for cell conversion, which will greatly facilitate devising disease models and new cell-based therapeutics.",https://academic.oup.com/nargab/article/3/4/lqab100/6423166?login=false,
Omega-3 polyunsaturated fatty acids suppress the inflammatory responses of lipopolysaccharide-stimulated mouse microglia by activating SIRT1 pathways,"Ingestion of omega-3s raises NAD+ levels in blood vessels and in brain cells, turning on cell defenses ","Obesity and diabetes are known risk factors for dementia, and it is speculated that chronic neuroinflammation contributes to this increased risk. Microglia are brain-resident immune cells modulating the neuroinflammatory state. Eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA), the major ω-3 polyunsaturated fatty acids (PUFAs) of fish oil, exhibit various effects, which include shifting microglia to the anti-inflammatory phenotype. To identify the molecular mechanisms involved, we examined the impact of EPA, DHA, and EPA + DHA on the lipopolysaccharide (LPS)-induced cytokine profiles and the associated signaling pathways in the mouse microglial line MG6. Both EPA and DHA suppressed the production of the pro-inflammatory cytokines TNF-α and IL-6 by LPS-stimulated MG6 cells, and this was also observed in LPS-stimulated BV-2 cells, the other microglial line. Moreover, the EPA + DHA mixture activated SIRT1 signaling by enhancing mRNA level of nicotinamide phosphoribosyltransferase (NAMPT), cellular NAD+ level, SIRT1 protein deacetylase activity, and SIRT1 mRNA levels in LPS-stimulated MG6. EPA + DHA also inhibited phosphorylation of the stress-associated transcription factor NF-κB subunit p65 at Ser536, which is known to enhance NF-κB nuclear translocation and transcriptional activity, including cytokine gene activation. Further, EPA + DHA increased the LC3-II/LC3-I ratio, an indicator of autophagy. Suppression of TNF-α and IL-6 production, inhibition of p65 phosphorylation, and autophagy induction were abrogated by a SIRT1 inhibitor. On the other hand, NAMPT inhibition reversed TNF-α suppression but not IL-6 suppression. Accordingly, these ω-3 PUFAs may suppress neuroinflammation through SIRT1-mediated inhibition of the microglial NF-κB stress response and ensue pro-inflammatory cytokine release, which is implicated in NAMPT-related and –unrelated pathways.","Obesity and diabetes are strong risk factors for both cardiovascular diseases and age-related dementia. Further, with the rates of both diabetes and obesity increasing globally, understanding the mechanisms underlying their associated neuropathologies is critical. It has been reported that obesity and diabetes affect the functions of microglia, the resident mononuclear phagocytes in the brain. Microglia release multiple pro- and anti-inflammatory immunoregulators, and a shift in the balance to the pro-inflammatory state contributes to neuroinflammation and concomitant neurodegeneration. Therefore, it is suggested that microglial dysfunction in obesity and diabetes may contribute to cognitive impairment, although the mechanistic details remain unclear.

Accumulating evidence has demonstrated the benefits of fish oil or its components against cardiovascular diseases, particularly eicosapentaenoic acid (EPA, C20:5n-3) and docosahexaenoic acid (DHA, C22:6n-3), the major ω-3 polyunsaturated fatty acid (PUFA) constituents of fish oil. We previously reported that EPA exhibited anti-inflammatory/anti-atherosclerotic effects, including a reduction of inflammation and arterial stiffness in patients with obesity, diabetes, metabolic syndrome, or a combination. We further showed that EPA shifted the balance of peripheral blood monocytes from pro-inflammatory (M1)-like to anti-inflammatory (M2)-like. In addition to these beneficial effects on cardiovascular diseases, it has been reported that ω-3 PUFAs including EPA and DHA are present in the brain and may protect against cognitive dysfunction. Indeed, we showed that ω-3 PUFAs reduced cerebral reactive oxygen species and prevented age-related cognitive dysfunction in a rat model of metabolic syndrome. In addition, a recent study demonstrated that ω-3 PUFAs promoted a phenotypic shift of microglia from M1 to M2 and reduced cognitive deficits in an animal model of demyelination induced by the copper chelator cuprizone. Therefore, it is possible that ω-3 PUFAs may shift microglia as well as peripheral monocytes away from the pro-inflammatory state in obesity and diabetes, thereby protecting against age-related diseases and cognitive decline.

Molecular pathways for the anti-inflammatory effects of ω-3 PUFAs have been studied much more extensively in peripheral macrophages (Mϕ) than in microglia. These mechanisms include activation and overexpression of AMP-activated protein kinase (AMPK) and the protein deacetylase sirtuin1 (SIRT1), with subsequent suppression of the stress-responsive transcription factor NF-κB via subunit p65 deacetylation [17], [18]. However, while there are multiple functional similarities between microglia and Mϕ, microglia exhibit unique properties such as a proliferation capacity as well as distinct cell surface markers and gene expression profiles [19], [20], [21]. Accordingly, it is equally important to describe the physiological effects of ω-3 PUFAs on microglia, particularly to understand the mechanisms of ω-3 PUFA-associated neuroprotection. In this study, we examined the impact of ω-3 PUFAs on the inflammatory response of microglia, focusing on the possible contributions of SIRT1 signaling.","In conclusion, this is the first study to demonstrate multiple temporally-regulated anti-inflammatory effects of the ω-3 PUFAs EPA and DHA on activated microglia. Our findings suggest that these PUFAs may suppress the chronic neuroinflammation associated with obesity and diabetes by reducing IL-6 and TNF-α release from activated microglia. In turn, suppression of neuroinflammation may reduce age-related neural cell damage and concomitant cognitive decline. Further studies to identify and characterize the molecular targets of ω-3 PUFAs that are involved in activation of SIRT1, elevation of NAMPT expression, and autophagy induction, could facilitate the development of novel medications to effectively suppress microglial-mediated neuroinflammation, thereby preserving neuronal function and cognition in various age-related neurodegenerative diseases.",https://www.sciencedirect.com/science/article/abs/pii/S1388198117300343?via%3Dihub,
Resveratrol ameliorates myocardial fibrosis by regulating Sirt1/Smad3 deacetylation pathway in rat model with dilated cardiomyopathy,Resveratrol reduces heart fibrosis and improves heart function via Sirt1 in rats at 10 and 50 mg/kg/day,"The aim of this study was to investigate the effects of Resveratrol (RSV) in rats with dilated cardiomyopathy (DCM).
Porcine cardiac myosin was used to set up rat model with DCM. RSV (10 mg/kg in RSV-L group and 50 mg/kg in RSV-H group) or vehicle was administered to rats with DCM once daily from the 28th day till the 90th day after the first immunization. Cardiac function of rats was evaluated by echocardiographic analysis. The deposition of fibrous tissues in the hearts was evaluated by Masson and picrosirius red staining. The mRNA levels of collagen type I (Col I), collagen type III (Col III) and silence information regulator 1 (Sirt1) were measured by quantitative real-time polymerase chain reaction (qRT-PCR). The interaction of Sirt1 with Smad3 was revealed by coimmunoprecipitation.
The heart weight, heart weight/body weight ratio, left ventricular end diastolic diameter (LVEDD) and left ventricular end systolic diameter (LVESD) were significantly increased in rats with DCM, and attenuated by RSV. RSV also positively decreased fibrosis, and the expression of Col I and Col III in the myocardium. The Sirt1 mRNA was significantly decreased in myosin-immunized hearts and was positively increased by RSV. The Sirt1 combined with Smad3 directly. Acetylation of Smad3 (Ac-Smad3) was significantly increased in DCM and was markedly decreased by RSV.

Conclusion
RSV effectively ameliorated myocardial fibrosis and improved cardiac function by regulating Sirt1/Smad3 deacetylation pathway in rat model with DCM.","Dilated cardiomyopathy (DCM) is mainly characterized by complex remodeling of one or both ventricles with an associated increase in mass, volume and the architecture of the myocardium fibres, resulting in left ventricular systolic dysfunction. It is the most common non-ischemic cardiomyopathy throughout the world, with an estimated prevalence of 1:2500–1:250 in the general population. DCM can be caused by many risk factors, such as hypertension, inflammation, infection, valve disease, metabolic and toxic effects medications. It also commonly has an underlying genetic variation which accounts for 30–48% of cases with DCM. Affected individuals are at risk of heart failure, sudden cardiac death and other life-threatening risks. In DCM, myocardial fibrosis, which plays a vital role in the genesis of ventricular arrhythmias, is known as an important pathophysiological process. Myocardial fibrosis can also be used to predict ventricular arrhythmias and sudden cardiac death in patients with nonischemic DCM.

The acetylation of Smad3 (Ac-Smad3) level was high in the rats with cardiac fibrosis  and renal fibrosis, while it was low in the normal myocardium and nephridial tissue of rats. Ac-Smad3 can regulate Smad3 DNA binding activity and transcriptional activity of specific profibrotic genes. So, increasing Ac-Smad3 level by transforming growth factor-beta 1 (TGF-β1) promote the occurrence and development of tissue fibrosis. Accordingly, collagen lattice contraction were impaired in Smad3−/− fibroblasts, and collagen deposition in the infarcted heart was reduced in Smad3 null mice, which reflects decreasing Ac-Smad3 level in Smad3−/− fibroblasts or Smad3 null mice will prevent or attenuate the tissue fibrosis. Therefore, the Ac-Smad3 level may play an important role in the tissue fibrosis. Recently, studies found that Ac-Smad3 can be targeted by resveratrol (RSV), geniposide, metformin, carnosic acid and nicotinamide riboside  to ameliorate the tissue fibrosis. The Ac-Smad3 level was mainly adjusted by the activation of acetyltransferase or histone deacetylase. There are two solutions to reduce the level of Ac-Smad3. One is to reduce the activity of acetyltransferase, such as lysine acetyltransferase 5 can be suppressed by metformin to decreased the Ac-Smad3 level, the other is to enhance the activation of histone deacetylase, such as the activation of histone deacetylase silence information regulator 1 (Sirt1) can be increased by resveratrol (RSV), geniposide, carnosic acid and nicotinamide riboside to decreased the Ac-Smad3 level.

As a natural non-flavonoid polyphenol, resveratrol (RSV) found in grapes and other plants, is a natural activator for Sirt1. Li J, et al. found that RSV can inhibit renal fibrosis by the activation of Sirt1, which mediated the deacetylation of Smad3 and suppressed the TGF-β1–induced fibrotic response. There is increasing evidence that RSV also has the effect of cardiovascular protection and promoting the left ventricular function to recover. Furthermore, clinical research also showed that RSV improved diastolic function in the patients with coronary artery disease. However, the effects and mechanisms of RSV on myocardial fibrosis in DCM are still unclear.

In this study, in order to investigate the effects and mechanisms of RSV on myocardial fibrosis, the rat model of DCM was used and the myocardial fibrosis with or without RSV intervention was assessed.",Rresveratrol effectively ameliorated myocardial fibrosis and improved cardiac function by regulating Sirt1/Smad3 deacetylation pathway in the rat model with dilated cardiomyopathy. Resveratrol may be a therapeutic modality for ameliorating myocardial fibrosis and improving cardiac function for patients with dilated cardiomyopathy.,https://bmccardiovascdisord.biomedcentral.com/articles/10.1186/s12872-021-02401-y#Sec18,
"A Low Carbohydrate, High Protein Diet Slows Tumor Growth and Prevents Cancer Initiation","Most cancers require high glucose to proliferate and survive. In 🐭, a low sugar, high protein diet reduced tumor size & death rates, indicating that lowering blood sugar levels & increasing protein may be a way to prevent & treat cancer","Since cancer cells depend on glucose more than normal cells, we compared the effects of low carbohydrate (CHO) diets to a Western diet on the growth rate of tumors in mice. To avoid caloric restriction–induced effects, we designed the low CHO diets isocaloric with the Western diet by increasing protein rather than fat levels because of the reported tumor-promoting effects of high fat and the immune-stimulating effects of high protein. We found that both murine and human carcinomas grew slower in mice on diets containing low amylose CHO and high protein compared with a Western diet characterized by relatively high CHO and low protein. There was no weight difference between the tumor-bearing mice on the low CHO or Western diets.

Additionally, the low CHO-fed mice exhibited lower blood glucose, insulin, and lactate levels. Additive antitumor effects with the low CHO diets were observed with the mTOR inhibitor CCI-779 and especially with the COX-2 inhibitor Celebrex, a potent anti-inflammatory drug. Strikingly, in a genetically engineered mouse model of HER-2/neu–induced mammary cancer, tumor penetrance in mice on a Western diet was nearly 50% by the age of 1 year whereas no tumors were detected in mice on the low CHO diet. This difference was associated with weight gains in mice on the Western diet not observed in mice on the low CHO diet. Moreover, whereas only 1 mouse on the Western diet achieved a normal life span, due to cancer-associated deaths, more than 50% of the mice on the low CHO diet reached or exceeded the normal life span. Taken together, our findings offer a compelling preclinical illustration of the ability of a low CHO diet in not only restricting weight gain but also cancer development and progression.","More than 80 years ago, Otto Warburg found that most cancer cells, unlike normal cells, rely more on glycolysis than oxidative phosphorylation (OXPHOS) to meet their energy needs, even under normoxic conditions. He postulated that this “aerobic glycolysis” was due to irreversible defects in mitochondrial respiration. However, whereas some studies have linked mitochondrial mutations to cancer, a causal role for these mutations seems to be relatively rare , and, in most cases, glycolysis in tumors seems reversible. Importantly, because glycolysis is far less efficient at generating ATP, most cancer cells require higher levels of glucose than normal cells to proliferate and survive, and this is why the glucose analog, 18fluorodeoxyglucose, is capable of detecting the majority of human tumors via positron emission tomography.

The current consensus to explain why tumor cells prefer aerobic glycolysis is that even though it is far less efficient than OXPHOS at generating ATP, yielding only 2 ATPs/glucose rather than 34 ATPs/glucose, it does not catabolize glucose completely to CO2 for ATP but instead uses the carbon chains as building blocks for nucleic acid (i.e., ribose), protein (i.e., alanine, etc.), and lipid (i.e., citrate) syntheses, all of which are essential for cell proliferation. This likely explains why increased glycolysis is not exclusive to solid tumors, but also occurs in leukemias and some rapidly growing normal cells, such as clonally expanding T cells. Also, glycolysis, via its pentose phosphate pathway offshoot, provides NADPH, which generates glutathione, an important intracellular reducing agent that prevents intracellular reactive oxygen species–induced death in cancer cells (10). In addition, glycolysis leads to the secretion of lactic acid, which can decrease the extracellular pH from 7.4 to 6.0 within a poorly perfused tumor, and this promotes metastasis by inducing normal cell death, angiogenesis, extracellular matrix degradation, and the inhibition of tumor antigen-specific immune responses. So, as long as cancer cells can obtain high levels of glucose, a high glycolytic rate provides sufficient ATP, even under hypoxic conditions, for tumor cell survival and proliferation.

Thus, we investigated whether low carbohydrate (CHO), high protein diets could sufficiently decrease blood glucose (BG) in mice to slow tumor growth. To prevent caloric restriction (CR)-induced effects, we had isocaloric diets prepared, in which we compensated for the low CHO content by raising protein levels. We chose high protein rather than high fat because of the reported tumor-promoting effects of high fat (13, 14) and the established benefits of amino acid supplementation (15, 16).","To exploit the fact that cancer cells rely more heavily on glycolysis than normal cells, we designed low CHO, high protein diets to see if we could limit BG and tumor growth. In designing our diets, we wanted to avoid NCKDs because of the difficulty in achieving long-term compliance with no CHO diets in potential future human studies (27) and because Masko and colleagues recently reported that a 10% or 20% CHO diet slows tumor growth as effectively as NCKDs (27). Following early studies with 8% CHO diets, using 10% and 15% CHO, high protein diets in which 70% of the CHO was in the form of amylose, we found that, compared with a Western diet, they were indeed capable of reducing BG, insulin, and lactate levels and, importantly, in slowing the growth of implanted murine and human tumors, with little or no effects on mouse weight.

We assessed the effects of our low CHO diets in both murine tumor-bearing immunocompetent mice and human tumor-bearing immunocompromised mice, because immune status has been shown to influence tumor growth (7), but found that low CHO diets slowed tumor growth to a similar extent without any difference in tumor-associated immune cell composition between the low CHO and 5058 groups (data not shown). Of note, Venkateswaran and colleagues recently found that CHO reduction (from 45% to 10%) slowed the growth of LNCaP xenografts and attributed this to reduced insulin like growth factor I (IGF-I) levels (28). Interestingly, we detected no changes in IGF-I levels in mice on our low CHO diets, unless there was CR (e.g., with our 8% CHO diet; data not shown). Our findings suggest that although IGF-I reduction may be a relevant mechanism in some models, low CHO diets may also slow tumor growth in an IGF-I–independent manner.

Given the antitumor effects of ketones and ketosis (29), we measured plasma β-hydroxybutyrate and found that tumor-bearing mice on our 10% diet as well as NOP mice on our 15% CHO diet for many months had β-hydroxybutyrate levels similar to mice fed with Western diet (<5 mg/dL; Supplementary Fig. S1), and substantially less than those reported for mice on NCKDs (∼15 mg/dL; refs. 27, 30). This is consistent with very recent studies showing that ketosis requires high dietary fat (31) and suggests that ketosis does not contribute to the slower tumor growth we observe with our low CHO diets.

We also found that our low CHO diets were additive with the tumor suppressive effects of CCI-779 and Celebrex. Related to this, while it has been shown that COX-2 is overexpressed in many human cancers, and that Celebrex may be beneficial in preventing/slowing colon, breast (32), and prostate cancers (33, 34) by blocking both omega-6 fatty acid-induced inflammation(33) and tumor-induced angiogenesis (35), high-dose Celebrex has cardiovascular side effects (36). As our low CHO diets show additive effects with Celebrex, it might allow for a lower, safer dose of Celebrex, without loss of therapeutic efficacy (37).

Although our work strongly suggests that cancer can be treated and/or prevented by limiting BG, some caution must be exercised in extrapolating our results to humans. This is because, while fasting BG levels have been shown to be significantly reduced in cancer patients on a low CHO diet (38), they may not be reduced as much as in mice (39). On the contrary, substantial postprandial reductions in BG have been reported in humans on low CHO diets (24, 38, 40, 41). Given that postprandial BG in humans is elevated for up to 2 hours after a meal and we typically eat 3 or more meals a day, it is very likely that a low CHO diet will significantly reduce the daily area-under-the-curve BG exposure. In keeping with this, it has been reported that low GI meals greatly reduce the BG area-under-the-curve compared with high GI meals in humans (40) and that reducing the CHO content of meals in mild diabetics from 55% to 20% reduces the BG area-under-the-curve by 36%, which is similar to what we see with our mice (41). Also, our low CHO studies with human HCT-116 cells in Rag2M mice and low CHO studies with other human tumors (42) suggest that there are no inherent differences between human and mouse cancer cells in their response to BG levels. Consistent with the notion that reducing BG in humans can be beneficial, there is a wealth of epidemiologic evidence showing a clear association between BG and/or insulin levels (which are determined by BG levels) and the incidence of human cancers (43–49). Thus, although our studies were conducted, out of necessity, with mice, the fact that human BG can be significantly reduced with low CHO diets and the association of many cancers with high BG levels suggest that our findings are very likely relevant to human cancers as well, particularly in cancers that have been associated with higher baseline BG and/or insulin levels, such as pancreatic (43, 44), breast (45), colorectal (46), endometrial (47, 48), and esophageal cancers (49).

In addition to these cancers, a low CHO diet may also be beneficial in early-stage prostate cancer, even though it is not typically detectable by PET (50). This is because the metastases of these tumors kill the patients and, given the pivotal role of lactate in promoting metastasis (11), our low CHO diets could significantly reduce metastasis by reducing tumor-associated lactate levels. In fact, we have preliminary data suggesting that a low CHO diet plus low dose Celebrex profoundly reduces the lung metastasis of orthotopically implanted 4T1 tumor cells (manuscript in preparation).

In terms of macronutrient composition, even though high protein has been shown to promote satiety (19)—thus reducing obesity, BG, and insulin levels—and enhance both antitumor immunity, through amino acid supplementation, and life span (15, 16, 51), we were concerned, based on the literature (52–54), that high protein levels might cause kidney damage. More recent data, however, suggest that this may only occur in individuals with existing chronic kidney disease (52, 55) and that in normal people, the increase in glomerular filtration rate and kidney cellularity that occur with long-term high protein consumption may be a normal response (52). Consistent with this, we found that while the 5 long-lived NOP mice on our 15% CHO diet had larger than normal kidneys (data not shown), only 1 had elevated urinary albumin. Moreover, because they lived beyond the normal life span of C57BL/6 mice on a Western diet, we can infer that the overall health of the mice was not adversely affected. In humans, most epidemiologic studies examining high protein diets and cancer progression have been confounded by not taking into account protein source, fat content, and red meat consumption. This is important because high fat increases cancer risk (56) and plant protein seems to decrease whereas animal protein increases cancer mortality (57). Interestingly, colonic cancer-inducing damage caused by red meats may be avoided with high amylose, low CHO diets (58). These studies suggest that macronutrient sources and combinations are very important and that testing them through highly controlled studies, such as those achieved with mice, represents a powerful approach to this question.

Our study, herein, shows that a high amylose containing low CHO, high protein diet reduces BG, insulin, and glycolysis, slows tumor growth, reduces tumor incidence, and works additively with existing therapies without weight loss or kidney failure. Such a diet, therefore, has the potential of being both a novel cancer prophylactic and treatment, warranting further investigation of its applicability in the clinic, especially in combination with existing therapies.",https://aacrjournals.org/cancerres/article/71/13/4484/567705/A-Low-Carbohydrate-High-Protein-Diet-Slows-Tumor,
Cell type-specific aging clocks to quantify aging and rejuvenation in regenerative regions of the brain,First high-resolution aging clocks from single cells in mouse brain showing exercise & young blood restore youthful patterns of gene control.,"Aging manifests as progressive dysfunction culminating in death. The diversity of cell types is a challenge to the precise quantification of aging and its reversal. Here we develop a suite of ‘aging clocks’ based on single cell transcriptomic data to characterize cell type-specific aging and rejuvenation strategies. The subventricular zone (SVZ) neurogenic region contains many cell types and provides an excellent system to study cell-level tissue aging and regeneration. We generated 21,458 single-cell transcriptomes from the neurogenic regions of 28 mice, tiling ages from young to old. With these data, we trained a suite of single cell-based regression models (aging clocks) to predict both chronological age (passage of time) and biological age (fitness, in this case the proliferative capacity of the neurogenic region). Both types of clocks perform well on independent cohorts of mice. Genes underlying the single cell-based aging clocks are mostly cell-type specific, but also include a few shared genes in the interferon and lipid metabolism pathways. We used these single cell-based aging clocks to measure transcriptomic rejuvenation, by generating single cell RNA-seq datasets of SVZ neurogenic regions for two interventions – heterochronic parabiosis (young blood) and exercise. Interestingly, the use of aging clocks reveals that both heterochronic parabiosis and exercise reverse transcriptomic aging in the niche, but in different ways across cell types and genes. This study represents the first development of high-resolution aging clocks from single cell transcriptomic data and demonstrates their application to quantify transcriptomic rejuvenation.

","Aging is the progressive deterioration of cellular and organismal function. Age-dependent decline is linked in large part to the passage of time and therefore the chronological age of an individual. But such chronological decline is not inexorable. At the same chronological age, some individuals have better organismal and tissue fitness (biological age) than others. Furthermore, aging trajectories can be slowed, and some aspects of aging can be reversed by specific interventions, including dietary restriction, exercise, senolytics (compounds that kill senescent cells), reprogramming factors, and young blood factors. As aging is the primary risk factor for many diseases, particularly neurodegenerative diseases, a better understanding of aging and ‘rejuvenation’ strategies could yield large benefits for a wide-range of diseases.

Aging is complex and difficult to quantify. One quantification approach is to use machine learning to build age-prediction models – aging clocks – which can serve as integrative aging biomarkers. Such clocks should also accelerate our understanding of existing interventions and help identify new strategies to counter aging and age-related diseases. Machine learning models trained on high dimensional datasets (e.g. DNA methylation, transcriptomics, proteomics) have been shown to predict chronological age with remarkable accuracy. For example, regression-based aging clocks trained on DNA methylation profiles from multiple tissues (‘epigenetic aging clocks’) or blood plasma protein profiles have striking performance to predict chronological age in humans. Aging clocks directly optimized to predict biological age have also been developed by regressing functional phenotypes or time remaining until death. Beneficial health interventions such as diet and exercise and genetic manipulations result in younger predictions from epigenetic aging clocks trained on chronological age. Hence, epigenetic aging clocks, despite being trained on chronological age, also capture dimensions of biological age.

So far, molecular aging clocks have largely relied on datasets built using bulk tissue input or purified cell populations. Bulk tissue profiles (and even purified populations) average the molecular profiles from many cells, integrating tissue composition changes and cell type-specific responses. Hence, the cell type-specific contributions to aging and rejuvenation detected by these clocks remain unclear. While single cell DNA methylation and transcriptomic have started to be used to classify age, cell-specific transcriptomic aging clocks have not yet been generated. Thus, it remains to be determined if different cell types’ clocks ‘tick’ at different rates, which cell types predict age most accurately, and how specific cell types respond to different interventions. The rapid advance of single cell RNA-sequencing technologies provides a unique opportunity to explore these unaddressed questions and identify new molecular aging clocks to study interventions to counter aging and age-related diseases.","Here we show that single cell RNA-seq data allow the generation of quantitative aging clocks that can be trained on chronological age or on aspect of tissue fitness – i.e. proliferative fraction of stem cells in the neurogenic region. To our knowledge, these are the first quantitative aging clocks based on single cell RNA-seq. We also generate three datasets that represent valuable stand-alone resources: a high temporal resolution single cell RNA-seq aging dataset of a neurogenic niche and the first single cell RNA-seq datasets for a neurogenic niche following heterochronic parabiosis and voluntary exercise. These datasets will be helpful to identify additional cellular and molecular changes during aging and rejuvenation.

Our clock accuracy (e.g. R = 0.92 in microglia) approaches that of bulk DNA methylation and proteomics13,17,31 while preserving cell-type-specificity and avoiding biased sorting procedures. Single-cell DNA methylation and proteomic methods have suffered from sparsity and scaling challenges, though there is rapid innovation to address these issues35,86. While the methods described here preserve cell-type specificity without relying on cell sorting, ‘pure’ single-cell trained clocks were not as effective as our BootstrapCell and EnsembleCell approaches (see Fig. 1). Thus, using small pools of 15 single cell transcriptomes can mitigate some technological (e.g. gene dropouts) or biological (e.g. transcriptional bursting) challenges inherent to single cell RNA-seq datasets. Nevertheless, increased gene variability (transcriptional noise) is itself a feature of aging87–91, and it will be important to model this feature in the next-generation aging clocks.

A limitation of the application of chronological-age-trained clocks is that interventions that stimulate age-associated compensatory pathways (e.g. stress responses) will reflect as age-acceleration despite their functional benefit to the cell, the tissue, or the organism. Thus, there is a need for a better understanding of genes contributing to the aging clocks and their function as well as continued development of functional and phenotypic-trained models. Here, we built clocks based on a functional phenotype of the regenerative region (neural stem cell proliferative capacity), but more comprehensive phenotyping approaches will be important to pursue. Overall, functional-aging clocks are likely to prove instrumental to understanding the biology of aging and rapidly evaluating interventions necessary to extend healthy lifespan.

The observation that heterochronic parabiosis and exercise can ‘turn back’ the single cell-based aging clocks provides a proof-of-concept that these aging clocks, even when trained on chronological age, can record aspects of aging biology. This is in line with other aging clocks built on bulk datasets13,22–25,32,33. Our results also highlight cell type specificity for aging and rejuvenation interventions, with some cell types being more responsive than others (e.g. NSCs, oligodendrocytes). This is unique to single cell-based clocks and will allow a better understanding of cell heterogeneity in tissue aging and rejuvenation. Our data also reveal different potential for rejuvenation strategies, at least at the transcriptional level, and different cellular and molecular mechanisms of action. For example, young blood (heterochronic parabiosis) has a stronger rejuvenating effect than exercise, especially in aNSC-NPCs. These results raise the exciting possibility that aging clocks can serve to rapidly test the efficacy of rejuvenation interventions and to support combining specific interventions to counter aging and age-related diseases.",https://www.biorxiv.org/content/10.1101/2022.01.10.475747v2.full,
Effect of Salt Substitution on Cardiovascular Events and Death,Massive 21 thousand person study of 600 Chinese villages over 4.7 years finds that a salt substitute (sodium+potassium chloride vs sodium) reduces the risks of stroke and heart attack (49.09 vs. 56.29 / 1000 person years),"Salt substitutes with reduced sodium levels and increased potassium levels have
been shown to lower blood pressure, but their effects on cardiovascular and safety
outcomes are uncertain.

We conducted an open-label, cluster-randomized trial involving persons from 600
villages in rural China. The participants had a history of stroke or were 60 years
of age or older and had high blood pressure. The villages were randomly assigned
in a 1:1 ratio to the intervention group, in which the participants used a salt substitute (75% sodium chloride and 25% potassium chloride by mass), or to the
control group, in which the participants continued to use regular salt (100% sodium chloride). The primary outcome was stroke, the secondary outcomes were
major adverse cardiovascular events and death from any cause, and the safety outcome was clinical hyperkalemia.

A total of 20,995 persons were enrolled in the trial. The mean age of the participants was 65.4 years, and 49.5% were female, 72.6% had a history of stroke, and
88.4% a history of hypertension. The mean duration of follow-up was 4.74 years.
The rate of stroke was lower with the salt substitute than with regular salt (29.14
events vs. 33.65 events per 1000 person-years; rate ratio, 0.86; 95% confidence interval [CI], 0.77 to 0.96; P=0.006), as were the rates of major cardiovascular events
(49.09 events vs. 56.29 events per 1000 person-years; rate ratio, 0.87; 95% CI, 0.80
to 0.94; P<0.001) and death (39.28 events vs. 44.61 events per 1000 person-years;
rate ratio, 0.88; 95% CI, 0.82 to 0.95; P<0.001). The rate of serious adverse events
attributed to hyperkalemia was not significantly higher with the salt substitute
than with regular salt (3.35 events vs. 3.30 events per 1000 person-years; rate ratio,
1.04; 95% CI, 0.80 to 1.37; P=0.76).
CONCLUSIONS
Among persons who had a history of stroke or were 60 years of age or older and
had high blood pressure, the rates of stroke, major cardiovascular events, and
death from any cause were lower with the salt substitute than with regular salt.
(Funded by the National Health and Medical Research Council of Australia; SSaSS
ClinicalTrials.gov number, NCT02092090.)","Elevated dietary sodium consumption, as well as low levels of dietary potassium intake, is associated with high blood
pressure and an increased risk of cardiovascular
disease and premature death. Randomized trials of dietary sodium reduction,
 as well as trials of dietary potassium supplementation, have
shown clear blood-pressure–lowering effects. Salt
substitutes, which replace part of the sodium
chloride in regular salt with potassium chloride,
combine these effects in a single product. Salt
substitutes are available in many countries worldwide and have proven blood-pressure–lowering
effects in diverse populations. However, in the
absence of well-powered, randomized, controlled
trials, there are uncertainties about the effects of
salt substitutes on serious disease outcomes such
as stroke, acute coronary syndrome, and death.
In addition, concern about the theoretical risks
of hyperkalemia and associated sudden death
from the use of salt substitutes in patients with
serious kidney disease has adversely influenced
perceptions of clinicians and the general population. The Salt Substitute and Stroke Study
(SSaSS) was designed to define the overall balance of benefits and risks of salt substitute as
compared with regular salt on stroke, cardiovascular events, death, and clinical hyperkalemia.","In the present trial, the participants who received the salt substitute had significantly lower
rates of stroke, major adverse cardiovascular
events, and death from any cause than those
who received regular salt. The observed benefits
from the salt substitute were broadly consistent
across participant subgroups and prespecified
exploratory outcome analyses of stroke, other
vascular events, and death. Use of the salt substitute was not associated with any apparent serious adverse effects.
The sizes of the effects on outcomes of stroke
observed in our trial are in keeping with our
power estimates at trial inception and are in accordance with a mechanism of action mediated
through blood-pressure reduction. The observed
changes in 24-hour urinary sodium and potassium excretion are of a magnitude consistent
with the measured decline in systolic blood
pressure. Sodium reduction and potassium supplementation have been shown to independently
lower blood pressure and to have synergistic
effects.4,5,13,14 Although a systematic review and
meta-analysis of other studies showed larger effects of salt substitution on blood pressure, all
of these studies were of shorter duration than
the current trial.15 Incomplete adherence to the
use of the salt substitute, consumption of regular salt outside the home, and some use of a salt
substitute in the control group probably attenuated the magnitude of the treatment effects in
the current trial.
The absence of any evident increased risk of
clinical hyperkalemia addresses concerns about
the potential harms from the use of salt substitutes.6
 Persons at risk of hyperkalemia were excluded if they reported serious kidney disease or
the use of medicines that might substantially
elevate blood potassium levels. There was no
biochemical prescreening of kidney function or
potassium levels, and there was concomitant use
of drugs blocking the renin–angiotensin system
in approximately a quarter of participants. However, we searched systematically for all serious
adverse events potentially attributable to hyperkalemia and did not identify any increased risk
of this outcome. We also found no increased risk
of sudden death that might be caused by hyperkalemia-induced arrhythmic events. Our data
also provide reassurance about the efficacy and
safety of sodium-intake reduction for the prevention of cardiovascular events and death. There
was no apparent evidence of an increased risk of
any cardiovascular or other adverse outcome
among the participants, who had a mean baseline
24-hour sodium excretion level of 187 mmol per
day (equivalent to 10.75 g of salt and 4.3 g of
sodium), which is only slightly above the average
global intake.16
In the absence of well-powered, randomized,
controlled trials, previous attempts to estimate
the effects of sodium reduction or potassium
supplementation on cardiovascular disease have
relied mainly on observational data, which are
subject to various biases and confounding.17,18 In
contrast, the current trial used a robust, randomized design, was large, and had a long duration
with many outcome events.
This trial has several notable limitations. Potassium was not measured serially, and elevated
potassium levels might have been missed in
some participants. Only one preparation of salt
substitute was used, and thus graded decreases
in sodium intake, which might have induced
graded responses, were not evaluated. Although
the delivery of the intervention was not concealed, the risk of bias was minimized with the
use of objective primary and secondary end
points and standardized methods for the identification of outcome events, including ascertainment through systematic searches of routinely
collected health data and verification by an independent end-point adjudication committee, the
members of which were unaware of the trialgroup assignments. Information that could be
used for adjudication of outcome events was
limited, and definitive assignment of causation
was difficult in many cases. We investigated the
potential effect of misclassification of end points
on effect estimates but found no evidence that
certainty of adjudications had any substantive implications for the primary conclusions of the trial.
We note that the magnitude of protection
observed in this trial is similar to that assumed
in a recent modeling study that estimated that
365,000 strokes, 461,000 premature deaths, and
1,204,000 vascular events could be averted each year by the population-wide use of a salt substitute in China.19 Large benefits might also be
achieved in other countries in Asia, Africa, and
Latin America in which salt intake is above recommended levels.20 Furthermore, because it is
primarily persons in low-income and disadvantaged populations who add large quantities of
salt to their diet during food preparation and
cooking,21 salt substitution — a practical, lowcost intervention (about $1.62 per kilogram of
salt substitute vs. $1.08 per kilogram of regular
salt in China) — may have the potential to reduce health inequities related to cardiovascular
disease.
In this trial comparing a salt substitute with
regular salt among persons who had a history of
stroke or were 60 years of age or older and had
high blood pressure, the rates of stroke, major
cardiovascular events, and death from any cause
were lower with the salt substitute, which had no
apparent serious adverse effects.

",https://www.nejm.org/doi/10.1056/NEJMoa2105675,
A High Protein Diet Is More Effective in Improving Insulin Resistance and Glycemic Variability Compared to a Mediterranean Diet—A Cross-Over Controlled Inpatient Dietary Study,A 21 day crossover study of 16 obese women finds eating high amounts of fish & cheese is better than a Mediterranean diet (30% vs 20% protein) at controlling fasting insulin and glucose levels ,"The optimal dietary pattern to improve metabolic function remains elusive. In a 21-day randomized controlled inpatient crossover feeding trial of 20 insulin-resistant obese women, we assessed the extent to which two isocaloric dietary interventions—Mediterranean (M) and high protein (HP)—improved metabolic parameters. Obese women were assigned to one of the following dietary sequences: M–HP or HP–M. Cardiometabolic parameters, body weight, glucose monitoring and gut microbiome composition were assessed. Sixteen women completed the study. Compared to the M diet, the HP diet was more effective in (i) reducing insulin resistance (insulin: Beta (95% CI) = −6.98 (−12.30, −1.65) µIU/mL, p = 0.01; HOMA-IR: −1.78 (95% CI: −3.03, −0.52), p = 9 × 10−3); and (ii) improving glycemic variability (−3.13 (−4.60, −1.67) mg/dL, p = 4 × 10−4), a risk factor for T2D development. We then identified a panel of 10 microbial genera predictive of the difference in glycemic variability between the two diets. These include the genera Coprococcus and Lachnoclostridium, previously associated with glucose homeostasis and insulin resistance. Our results suggest that morbidly obese women with insulin resistance can achieve better control of insulin resistance and glycemic variability on a high HP diet compared to an M diet.","An obesity pandemic is gripping the globe, with higher demand and availability for energy-dense foods, accompanied by increasingly sedentary lifestyles. This is a major public health concern, as obesity often confers an increased risk of developing a wide range of complex and life-changing diseases, including cardiovascular and cerebrovascular disease, type II diabetes and cancers. Therefore, the development and implementation of effective and affordable measures to combat obesity is of utmost importance. As well as encouraging increased physical activity, many efforts to reduce obesity and its associated disorders have focused on the impact of diet and nutrition [8]. In particular, the Mediterranean (M) diet, a diet characterized by high levels of polyphenols, mono- and polyunsaturated fatty acids (MUFAs and PUFAs), antioxidants, and fiber, as well as low levels of salt, sugar and saturated fatty acids, has been associated with improved health outcomes. Greater adherence to the M diet has been associated with reduced risk of cardiovascular disease, which also supports weight loss. A high-protein (HP) diet, comprising low carbohydrate, high fat and high protein intake, has also been suggested as a potential dietary intervention for obesity prevention with HP diets corresponding to greater weight loss compared to similar isocaloric diets with standard protein content. A HP diet has also been shown to lead to a greater weight loss compared to a high-carbohydrate diet, along with an improvement in insulin parameters, highlighting its power to lower the risk of type 2 diabetes and cardiovascular diseases. Over the short term, a HP diet has been suggested to more effectively aid weight loss in contrast to a low-fat diet, and has been shown to change body composition in overweight or obese men. The mechanisms supporting the HP diets’ effects on weight loss efficacy is theorized to be related to increased satiety and it has been suggested that this enhances an individual’s metabolic rate. Recent evidence suggests that the benefits of any dietary intervention are intrinsically linked to an individual’s metabolic profile. The exact role of the gut microbiome in nutrient metabolism is still unclear, but various studies have linked microbial diversity and specific bacteria to a propensity for obesity, as well as to the metabolism of dietary compounds found in the M diet, including omega-3 fatty acids and polyphenols. Here, we aimed to explore the differential effects on metabolic parameters elicited by the M and HP diets. As these effects are reported to be exacerbated in obese individuals with impaired metabolic response, we conducted a 21-day randomized crossover controlled dietary trial in 20 insulin-resistant women with obesity.","In conclusion, we find that the HP diet is more effective in reducing insulin resistance and in improving glycemic variability in morbidly obese women with pre-diabetes and have identified a panel of 10 microbes underlying the difference in glycemic variability between the two diets. Further investigation is required to elucidate the links between dietary interventions, the microbiome and clinical outcomes, as well as to identify measures that are predictive of individual response to intervention. Continued investigation of these interactions will contribute to the development of stratified intervention and prevention strategies for obesity and its associated health problems.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8707429/,
The flavonoid procyanidin C1 has senotherapeutic activity and increases lifespan in mice,A chemical from grape seeds can reduce senescent cells and increase healthy lifespan in mice. ,"Ageing-associated functional decline of organs and increased risk for age-related chronic pathologies is driven in part by the accumulation of senescent cells, which develop the senescence-associated secretory phenotype (SASP). Here we show that procyanidin C1 (PCC1), a polyphenolic component of grape seed extract (GSE), increases the healthspan and lifespan of mice through its action on senescent cells. By screening a library of natural products, we find that GSE, and PCC1 as one of its active components, have specific effects on senescent cells. At low concentrations, PCC1 appears to inhibit SASP formation, whereas it selectively kills senescent cells at higher concentrations, possibly by promoting production of reactive oxygen species and mitochondrial dysfunction. In rodent models, PCC1 depletes senescent cells in a treatment-damaged tumour microenvironment and enhances therapeutic efficacy when co-administered with chemotherapy. Intermittent administration of PCC1 to either irradiated, senescent cell-implanted or naturally aged old mice alleviates physical dysfunction and prolongs survival. We identify PCC1 as a natural senotherapeutic agent with in vivo activity and high potential for further development as a clinical intervention to delay, alleviate or prevent age-related pathologies.","Ageing is one of the biggest risk factor for chronic disorders, including cardiovascular diseases, metabolic disorders, neurodegenerative pathologies and diverse malignancies, which together account for the bulk of morbidity, mortality and health costs globally1. Considerable progress has been made over recent years to develop specific agents to treat individual age-related conditions, such as type 2 diabetes, osteoporosis, skeletal fragility and vascular dysfunction. However, the combined effect of these drugs in controlling morbidity and mortality of chronic diseases has been modest, and these diseases tend to occur in synchrony as multimorbidities, with prevalence increasing exponentially after 70 years of age2.

Several major factors affecting healthspan and lifespan have been identified through studies across a range of species and defined as ageing mechanisms that can be categorized into nine hallmarks3. Of these fundamental ageing mechanisms, cellular senescence has received substantial attention, as it represents a druggable process that prevents or delays multiple ageing comorbidities4. First reported in the 1960s, cellular senescence refers to a cellular state involving essentially irreversible replicative arrest, profound chromatin changes, apoptosis resistance and increased protein synthesis, frequently culminating in overproduction of pro-inflammatory cytokines, a feature termed the SASP, which is thought to drive ageing phenotypes and various age-related pathologies5. Ablation of senescent cells positive for the senescence marker p16INK4A mitigates tissue degeneration and extends animal healthspan, supporting the contention that senescent cells play a causative role in organismal ageing6,7.

Success in preclinical studies has inspired the initiation of proof-of-concept clinical trials involving senolytics for several human diseases with the potential to decrease the burden of in vivo senescent cells through selective pharmacological elimination8,9,10. Since the first discovery in 2015 (ref. 11), a handful of synthetic or small-molecule senolytic agents are now known. Targeting strategies are mainly based on the resistance mechanism of senescent cells to apoptosis, which appears to depend on senescence-associated anti-apoptotic pathways that allow senescent cell survival for extended periods12,13. Intermittent administration of senolytics holds the potential to reduce the risk of patients developing adverse conditions, minimize off-target effects of drugs and prevent development of drug resistance of senescent cells, which do not divide, a characteristic that sets them apart from cancer cells, as cancer cells frequently acquire advantageous mutations providing resistance against anticancer therapies. However, most reported senolytics are dependent on cell lineage or cell type or, alternatively, exhibit substantial cytotoxicity in vivo, thus limiting their potential use for clinical purposes.

In this study, we screened a natural product medicinal library composed of anti-ageing agents and identified several candidates including GSE. Further analysis revealed that PCC1, a B type trimer epicatechin component of GSE flavonoids, plays a major role in inhibiting SASP expression at low concentrations and killing senescent cells at higher concentrations, the latter through inducing apoptosis. Preclinical data suggested that, in combination with classic chemotherapy, PCC1 can significantly reduce tumour size and prolong survival in several mouse models. Thus, PCC1 represents a new class of phytochemical senolytics isolated from natural sources that delay ageing and ameliorate age-related disorders and warrants further exploration as a potential geroprotective agent in clinical medicine.","Ageing is an essentially inevitable process that progressively causes functional decline in nearly all organisms. Cellular senescence, a state of permanent growth arrest, has recently emerged as both a hallmark and a driver of ageing. Senescent cells accumulate in aged tissues over time and contribute to an increasing list of pathologies. Clearance of senescent cells from progeroid or naturally aged mice extends healthspan, increases lifespan and restrains age-related disorders including but not limited to atherosclerosis, osteoarthritis and neurodegenerative diseases. Recent advances in age-related studies prompted a search for drugs that can selectively target senescent cells, particularly a new class of geroprotective agents termed senolytics or, less aggressively, senomorphics. To date, a handful of senolytics have been reported, including dasatinib and quercetin, fisetin, piperlongumine, heat-shock protein (HSP)90 inhibitors and BCL-2 family inhibitors such as ABT-263 (navitoclax) and ABT-737. Among them, BCL-2 inhibitors are the most widely used senolytics, although originally developed as therapies for lymphoma. ABT-737 targets BCL-2, BCL-xL and BCL-w but with low solubility and oral bioavailability. More effective for in vivo use, ABT-263 mainly inhibits BCL-2 and BCL-xL, whereas it frequently causes thrombocytopaenia. Given the marked side effects of some senolytic compounds, there is a need to identify new compounds with senolytic activity but reduced cytotoxicity. In this study, we screened a PDMA-based drug library composed mainly of natural products with an aim to identify new agent(s) that can widely target senescent cells with optimal in vivo efficacy and safety. As a result, we identified PCC1, a phytochemical agent derived from natural sources, as a broad-spectrum senolytic compound. As a special advantage, PCC1 can alternatively act as a senomorphic agent to minimize SASP expression when used at low concentrations. Such an advantageous feature of PCC1 indeed largely resembles that of GSE, which can generate both senomorphic and senolytic effects.

Genetic and pharmacological strategies demonstrated an array of benefits of eliminating senescent cells to delay ageing and control diseases. Cellular senescence can be triggered by a variety of stimuli ranging from oncogenic activation, genotoxic stress, to inflammatory response and replicative exhaustion. Several compounds are identified as broad-spectrum senolytics, while others are selective against only a certain type of senescent cell. Differences in specificity imply individual choices of senolytics, which mainly depend on their intended clinical use. A recent study revealed ouabain, a natural compound belonging to the cardiac glycoside family, as a senolytic agent that can be used for both senescent cell elimination and cancer therapy, the latter implemented through a dual mechanism of action63. In this work, we discovered PCC1 as another new, natural and potent senolytic, which selectively and specifically induces apoptosis of senescent cells but with limited cytotoxicity to proliferating cells64. Of note, at lower concentrations, PCC1 inhibits SASP expression, a property shared by some plant-derived flavonoids such as apigenin and kaempferol, which can act as senomorphics to limit the impact of senescent cells on age-related conditions65,66. Although few studies have disclosed such a dual mechanism of natural agents in targeting senescent cells, the recently synthesized quercetin surface functionalized Fe3O4 nanoparticles exhibited both senolytic and senomorphic potential in human fibroblasts by enhancing AMP-activated protein kinase (AMPK) activity67.

The mechanism by which PCC1 achieves senolytic effects appears complex and requires further study. Our data suggest that PCC1 impairs the functional integrity of mitochondria, compromising Δψm, leading to increased production of free radicals such as ROS and causing cytochrome c release in senescent cells but not in proliferating cells. A possible reason for this specificity is that senescent cells tend to develop a depolarized plasma membrane and have increased concentrations of H+ (ref. 64), a feature that might make them more susceptible to the action of PCC1. Of note, these alterations are accompanied by upregulated expression of pro-apoptotic factors, specifically NOXA and PUMA, events that also critically promote senescent cell apoptosis. Within the family of procyanidins, members of which are known to derive from the polymerisation of flavan-3-ol molecules and exist as oligomers or polymers28, PCC1 seems to be functionally unique. Our experimental data imply a noticeable difference between PCC1 (a trimer) and other procyanidins (most of which are indeed monomers or dimers, such as PCB2). Since we did not comprehensively assay procyanidin family members, whether the number of monomers in the molecule determines its anti-senescence potential remains an open but intriguing question, and the underlying mechanisms deserve continued studies in the future.

Cellular senescence per se is a highly heterogeneous process that depends on different cell origins and environmental stimuli68. One of the key features of PCC1 is its ability to efficiently clear senescent cells in a wide spectrum of cell types and stressors, including replication, oncogenes, irradiation and chemotherapy. In this study, we compared PCC1 with other reported senolytics for effects on human stromal cells, fibroblasts, HUVECs and MSCs, major cell types in the tissue microenvironment. As reported, ABT-263 eliminates senescent human embryonic fibroblasts (HEFs) and HUVECs but has little effect on human pre-adipocytes12,18. The combined use of dasatinib and quercetin can deplete all three types of senescent cells in a dose-dependent manner but is toxic to proliferating cells11,69,70. Fisetin, another natural flavonoid reported as a senolytic agent, displays modest effects on senescent HEFs and pre-adipocytes only at high concentrations20,21. By contrast, PCC1 has the potential to overcome these limitations, including cell type dependency, high toxicity in nonsenescent cells and low efficiency against senescent cells. Although, when used alone, quercetin (another flavonoid in GSE) per se displayed cytotoxicity against senescent stromal cells, its efficacy is generally lower than that of PCC1 (compare Fig. 2a,c and Supplementary Figs. 3n and 4n). Together, PCC1 has a superior senolytic activity with high specificity and efficiency for a wider range of cell types than many reported senolytics such as ABT-263, dasatinib, quercetin and fisetin and can target senescent cells generated by several major types of senescence inducers.

We found that PCC1 exerts apoptosis-inducing effect on senescent cells under in vivo conditions. PCC1 eliminated therapy-induced senescent cells effectively and reduced senescence markers in solid organs, highlighting its effectiveness in vivo. In this study, we also treated naturally aged mice with PCC1 and tested its effects on senescent cells, chronic inflammation and physical function. First, PCC1 treatment depleted senescent cells in multiple tissues and decreased SASP-associated signatures as shown by GSEA analysis. Second, PCC1 could suppress expression of SASP-associated genes in aged livers and kidneys and reduce chronic low-grade inflammation in the blood. Third, PCC1 alleviated impaired motor function, balance, exhausted exercise, muscle strength and spontaneous exploration in aged mice. Most importantly, performance on RotaRod and beam balance testing in the PCC1-treated group was improved compared with that in the initial pretreatment condition. Collectively, the phytochemical compound PCC1 selectively targets senescent cells in the tissue microenvironment and generates remarkable biological effects in naturally aged mice.

Similar to chemically synthesized counterparts, naturally derived procyanidins manifest anti-inflammatory, anti-arthritic, anti-allergic and anticancer activities, scavenge oxygen free radicals and suppress radiation-induced peroxidation activity36,71. As an epicatechin trimer isolated from plant material, most prominently from grape seeds, PCC1 was shown to provide health benefits in chronic pathological conditions72. However, thorough evaluation of the toxicological effects of PCC1 in vivo is crucial for a potential clinical application. Our data showed that high-concentration (20 mg per kg) and high-frequency PCC1 (biweekly) treatment had no apparent systemic toxicities. In summary, our study demonstrates the superiority and relative safety of a geroprotective strategy that selectively targets senescent cells in aged or treatment-damaged tissues across a broad spectrum of cell types. However, it is possible that PCC1 concentrations in vivo vary between organs and depend on the administered dose, pharmacodynamics and pharmacokinetics and that local concentrations are not high enough to achieve a senolytic effect in some tissue types. In this case, it seems likely that a combination of both senolytic and senomorphic effects underlies the outcomes that we observed in vivo.

Altogether, our study opens a new avenue for extending healthspan and prolonging lifespan and treating age-related pathologies with a senotherapeutic agent (with both senomorphic and senolytic potential), which is derived from natural sources and possesses pronounced efficacy. The potential anti-ageing effects of PCC1 demonstrated in our preclinical assays provide good support for further translational and clinical development of PCC1, with the overall aim of achieving a longer and healthier life.",https://www.nature.com/articles/s42255-021-00491-8#Sec10,
"Rejuvant®
, a potential life‐extending compound formulation with
alpha‐ketoglutarate and vitamins, conferred an average 8 year
reduction in biological aging, after an average of 7 months of use,  
in the TruAge DNA methylation test","Combination of alpha-ketoglutarate, a natural product used to make energy and control the biological aging ""epigenetic"" clock, plus vitamins, reversed age by an ave of EIGHT years in 7 months.","The search continues for possible interventions that delay and/or reverse biological aging, resulting in extended
healthspan and lifespan. Interventions delaying aging in animal models are well established; however, most
lack validation in humans. The length of human lifespan makes it impractical to perform survival analysis.
Instead, aging biomarkers, such as DNA methylation (DNAm) clocks, have been developed to monitor biological
age. Herein we report a retrospective analysis of DNA methylation age in 42 individuals taking Rejuvant®, an
alpha‐ketoglutarate based formulation, for an average period of 7 months. DNAm testing was performed at
baseline and by the end of treatment with Rejuvant® supplementation. Remarkably, individuals showed an
average decrease in biological aging of 8 years (p‐value=6.538x10‐12). Furthermore, the supplementation with
Rejuvant® is robust to individual differences, as indicated by the fact that a large majority of participants
decreased their biological age. Moreover, we found that Rejuvant® is of additional benefit to chronologically
and biologically older individuals. While continued testing, particularly in a placebo‐controlled design, is
required, the nearly 8‐year reversal in the biological age of individuals taking Rejuvant® for 4 to 10 months is
noteworthy, making the natural product cocktail an intriguing candidate to affect human aging.","Aging is a near universal biological process that manifests
as a general decline in health and vitality, eventually
leading to death. Aging is associated with the
development of a wide range of chronic diseases,
including cancer, Alzheimer's, diabetes, cardiovascular
disease and many other conditions. If aging can be
delayed, chronic disease onset will be forestalled,
functional capacity maintained and, in all likelihood,
complications due to infectious diseases, such as Covid19 and influenza, reduced. In short, humans will have
a longer healthspan and lifespan.
Aging is typically measured chronologically in days or
years, with median human survival on the order of eight decades. If we hope to control the aging process,
we need to learn how to measure the rate of aging in
shorter time periods. Moreover, aging progresses at
different rates in different individuals. Our true
biological age is influenced by many additional
factors, such as genetic background, lifestyle, and
disease. To address this challenge, several biological
markers of aging have been developed. These markers
are unique sets of molecules or changes in the
epigenetic state of an individual's DNA that reflect
their current aging status. Among the most
promising biomarkers of the aging process are
DNA methylation patterns. DNA methylation is an
epigenetic mechanism that plays an important role
in the regulation of gene expression, organism
development and disease.
Methylation of lysine residues within core histones, H3
and H4, initiates a conformational modification in the
chromatin structure that is associated with changes in
transcriptional activity. However, the most widely
studied epigenetic mark is the direct methylation of
DNA itself. This modification involves the conversion
of cytosine to 5’-methylcytosine, catalyzed by DNA
methyltransferases, and typically occurs within the CpG
dinucleotide sequences (CpGs). These CpG sequences,
clustered in regions known as CpG islands (CGIs), are
most often found in promoters of housekeeping genes. It has been shown that hypermethylation of
CpG islands is linked with transcriptional silencing,
whereas demethylated CpG islands are more often
found during embryogenesis and serve as a hallmark of
actively transcribed genes. During aging, two types of
changes in DNA methylation have been observed and
carefully characterized: (1) epigenetic drift, or
progressive stochastic changes in DNA methylation
patterns between individuals that occurs with increasing
age, and (2) the epigenetic clock – a DNA
methylation-derived measure that is highly correlated
with chronological age and proposed to measure
biological age.
The epigenetic clock is an attractive biomarker of aging
because it applies to most human tissues, capturing
aspects of biological age such as frailty,
cognitive/physical fitness in the elderly, ageacceleration in obesity, premature aging in Down’s
syndrome and HIV infection, Parkinson’s
and Alzheimer’s disease-related neuropathologies,
as well as cancer and lifetime stress. Markers
of biological aging represent an important tool to
clinically validate the effects of longevity-based
interventions. For the first time, these biomarkers of
aging give scientists the opportunity to study the effects
of anti-aging compounds in real-time and directly in
humans. One of the most promising anti-aging compounds discovered to date is Alpha-Ketoglutarate
(AKG).
AKG is an endogenous intermediary metabolite in the
Krebs cycle whose levels naturally decline during
aging. AKG is involved in multiple metabolic and
cellular pathways. These include functioning as a (an)
signaling molecule, energy donor, precursor in the
amino acid biosynthesis, and a regulator of epigenetic
processes and cellular signaling via protein binding. AKG deficiency in stem and progenitor
cells increases with age. As animals age,
mitochondrial function is progressively impaired
and cellular metabolic flux in the mitochondria
declines, which exacerbates AKG deficiency. Chin et
al. reported that AKG increased the lifespan of
C. elegans. Building on these results, AKG
(and calcium salt) combined with other Generally
Recognized as Safe (GRAS) compounds were studied
in mice. The non-genetically altered mouse is the
preferred mammalian model to study aging, since the
biochemical processes involved in mice aging may
apply to other mammals, including humans. In a
recent study, sponsored by Ponce de Leon Health and
performed at the Buck Institute for Research on
Aging, the effect of alpha-ketoglutarate (delivered in
the form of a calcium salt - CaAKG) on healthspan
and lifespan in C57BL/6 mice was reported. The
authors showed that in the mice, AKG reduced frailty
and enhanced longevity, indicating a compression of
morbidity. These and other discoveries suggest
that AKG may be an ideal candidate for pro-longevity
human studies.
In this study, we examined the cross-sectional and
longitudinal association between the epigenetic clock,
health status, physical fitness and the effects of taking
Rejuvant® (sustained release CaAKG + a specific
vitamin depending on sex) on human biological aging.
We followed 42 self-reported healthy individuals who
had taken AKG supplementation for a period of 4 to 10
months. The effects of AKG on biological aging, and
the possible correlation of other physiological effects,
are discussed. ","AKG has been shown to extend lifespan in various
model systems. In this study, we used a previously
developed algorithm that predicts human biological age
to determine if Rejuvant®, sustained release CaAKG +
vitamins has a beneficial effect on human longevity. A
total of 42 individuals, known to be taking Rejuvant®
and who had submitted saliva samples for DNA
methylation testing, were selected to participate in a
customer biological aging result survey and analysis.
Their DNAm TruAge Index had been measured at
baseline (before starting Rejuvant®) and retested after
an average of 7 months of use. Overall, these 42
individuals showed statistically significant average
reduction in their biological age of approximately 8
years.
General thinking in the aging research field is that
interventions are likely to affect subsets of the
population, and no one intervention (lifestyle or small
molecule) will delay or reduce biological age in the
entire population. Surprisingly, in this group the vast
majority of participants responded with a reduced
biological age after Rejuvant® treatment. While the
study does have limitations (described below), these
findings are encouraging. Interestingly, there were two
parameters that influenced the magnitude of the
response: those participants with higher biologic age
relative to chronological age and those with higher
chronological age at baseline. This suggests, perhaps
contrary to expectations given the known role of AKG
in augmenting exercise performance, that Rejuvant®
has a larger response in participants biologically older
than their chronological age. One might also predict this
outcome for a longevity intervention on the basis of the
hypothesis that individuals with low relative biological
age are already undergoing near optimized aging and
have less to gain. Currently, there is insufficient data on
human aging to predict which populations will respond
to a particular intervention.
The TruAge methylation test, which remains
proprietary, was developed by examining a limited
number of methylation sites in CpG islands of
promoters, based on optimization to chronological age
using a machine learning approach. While it surveys a
smaller portion of the genome than other methylation
clocks, it has the advantage of being more affordable. In
addition, TruAge test is easily used by consumers, who
place saliva on a paper card and mail in the sample for
analysis. The TruAge test was shown to report similar
results when compared to other epigenetic clocks
(unpublished), yet further testing using other
methylation clocks and different biomarkers of aging
would be beneficial to measure the effects of Rejuvant®
on human longevity. A fundamental question regarding
different biological aging measures relates to their level
of concordance: do they measure the same, overlapping
or completely different aspects of the aging process?
The data in this study, while limited, suggests that
CaAKG may indeed impact aging, at least as measured
by methylation. It is also worth noting that AKG is a
known substrate for DNA demethylases [31], which
potentially demethylate DNA sites interrogated by
TruAge. However, the AKG supplementation leads to
both demethylation and hyper methylation of some CpG
sites in saliva cells, suggesting that Rejuvant® may
have a larger effect on methylation-based aging clocks
than other indicators of biologic age.
There are several limitations to this study. Primarily, it
is not placebo controlled. Therefore, one potential
concern is that the placebo effect may have contributed
to some extent to the changes observed. However, the
self-reported trust in the efficacy of dietary supplements
was not deemed a statistically significant predictor in
any of our regression models, which mitigates risk to an
extent. Moreover, the study describes a limited sample
size and we were unable to collect other kinds of data
relevant to aging, for instance clinical markers of aging
and disease, and apply other biological aging clocks.
Future randomized clinical trials will be required to
confirm the findings presented here. Nevertheless, the
results in this manuscript suggest that Rejuvant® may
have significant effects on biological age as measured
by DNA methylation of saliva samples.",https://www.aging-us.com/article/203736,
Nicotinamide mononucleotide ameliorates senescence in alveolar epithelial cells,"The NAD booster nicotinamide mononucleotide (NMN), prevents cellular senescence & improves respiratory function in old mice & young mice treated with bleomycin, a DNA damaging agent.","Alveolar epithelial cells (ACEs) gradually senescent as aging, which is one of the main causes of respiratory defense and function decline. Investigating the mechanisms of ACE senescence is important for understanding how the human respiratory system works. NAD+ is reported to reduce during the aging process. Supplementing NAD+ intermediates can activate sirtuin deacylases (SIRT1–SIRT7), which regulates the benefits of exercise and dietary restriction, reduce the level of intracellular oxidative stress, and improve mitochondrial function, thereby reversing cell senescence. We showed that nicotinamide mononucleotide (NMN) could effectively mitigate age‐associated physiological decline in the lung of 8–10 months old C57BL/6 mice and bleomycin‐induced pulmonary fibrosis in young mice of 6–8 weeks. Besides, the treatment of primary ACEs with NMN can markedly ameliorate cell senescence phenotype in vitro. These findings to improve the respiratory system function and reduce the incidence and mortality from respiratory diseases in the elderly are of great significance.","One of the most significant changes in lung with aging is the decrease in the number and function of alveolar epithelial cells (ACEs). 1 , 2 , 3 ACEs mainly locate at the terminal end of the respiratory tract and are the main components of the blood gas barrier. They are classified into type I alveolar epithelial cells (AECIs) and type II alveolar epithelial cells (AECIIs). AECIs account for 96% of the total AECs, while AECIIs only 4%. Interestingly, AECIIs in the lung are active progenitor and secretory cells, secrete alveolar surfactant, and maintain alveolar surface tension. 4 , 5 AECs constitute 99% of the surface area of the lung, playing a major role in lung functions: barrier, gas exchange, and immunomodulation. The senescence of these cells is the main reason for the decline of lung function in aging. 6 Although AECs senescence is of great significance for the study of senescence‐related chronic lung diseases, its specific mechanism and role are still not fully understood. NAD+ supplementation is reported to improve many aging‐related diseases, such as neurodegenerative diseases. However, the effects of NAD+ precursors on pulmonary aging and AECs senescence have not been reported.

Recent studies have found that supplementation of NAD+ precursor nicotinamide mononucleotide (NMN) and nicotinamide riboside (NR) can increase the levels of NAD+ within cells. Supplementation of NMN to AECs cultured in vitro can significantly alleviate the replication senescence of AECs, preventing age‐related physiological decline, increasing DNA repair function, and improving mitochondrial dysfunction and glucose intolerance. Whether adding NAD+ precursor in vivo could alleviate the lung aging and improve ACEs senescence in vitro is still unclear.","We tried to understand whether NMN could alleviate the replication and stress‐induced senescence in human ACEs via mice models. We found that both in vivo and in vitro, ACEs of the same passage number were significantly reduced in NMN supplemented and nonsupplemented groups. The decreased numbers of cells were not changed by the BLM treatment nor the BLM treatment with NMN addition, senescent ACEs were also significantly reduced. We also found that NMN reduced BLM‐induced inflammation in mouse lungs. Our data suggested that NMN could effectively alleviate the replicative and stress‐induced senescence of ACEs in vivo and in vitro. It provides a preventive and therapeutic approach for aging‐related chronic lung diseases and lung injury caused by external stimuli in the future.

NAD+ decreases in vivo with aging and is considered to be an important regulator of age‐dependent pathological processes. NAD+ participates in various physiological activities in the body and is a key cofactor for glycolysis, tricarboxylic acid cycle, and oxidative phosphorylation, also in various redox reactions in cells. The antiaging effect of NAD+ supplementation has been confirmed in many tissues and organs, such as satellite cells, angiogenesis, and myocardium of skeletal muscle. NMN and NR can be used as a nutritional additive to increase the level of NAD+ in vivo, thereby preventing the occurrence of aging. There are many factors leading to cell senescence, including oxidative stress, mitochondrial dysfunction, and so on. Studies have shown that the toxicity of BLM in cells is mainly due to genetic instability and increased oxidative stress. NMN supplementation not only decreases the level of oxidative stress in cells, but also reduces the damage of cellular function induced by oxidative stress, including genetic instability, improves mitochondrial functions, and increases the activity of NAD+‐dependent enzymes (e.g., SIRT family). Aging is a complex process; we have not studied in depth how NMN prevented the aging of ACEs. Cell senescence is a complex process accompanied by changes in gene expression and secretion of cytokines and proteases (SASP); however, we did not perform SASP test in this experiment. Current studies have shown that senescent ACEs had a high level of proinflammatory cytokine phenotype. However, this age‐related phenotype was not detected in this study. Although our data did not exclude other lung parenchyma cells or extra‐pulmonary factors, nor separating alveolar type I and type II cells. However, our study was based on cells in the lungs, so there is no doubt that addition of NMN to alveolar cells decreases senescence cells absolutely.

In conclusion, we found that the long‐term addition of NMN could effectively improve ACE replication and stress‐induced senescence in vivo and in vitro. Dietary supplementation of NMN might be a new and effective way to prevent and reduce aging‐related lung diseases and stress‐induced lung injury in the future. ",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8491199/,
"Dietary Cocoa Flavanols Enhance Mitochondrial Function in Skeletal Muscle and Modify Whole-Body Metabolism in Healthy Mice
","A molecule in chocolate that's structurally related to resveratrol, epicatechin, increases NAD levels, glucose tolerance & metabolism in healthy mice in 15 days.","Mitochondrial dysfunction is widely reported in various diseases and contributes to their pathogenesis. We assessed the effect of cocoa flavanols supplementation on mitochondrial function and whole metabolism, and we explored whether the mitochondrial deacetylase sirtuin-3 (Sirt3) is involved or not. We explored the effects of 15 days of CF supplementation in wild type and Sirt3-/- mice. Whole-body metabolism was assessed by indirect calorimetry, and an oral glucose tolerance test was performed to assess glucose metabolism. Mitochondrial respiratory function was assessed in permeabilised fibres and the pyridine nucleotides content (NAD+ and NADH) were quantified. In the wild type, CF supplementation significantly modified whole-body metabolism by promoting carbohydrate use and improved glucose tolerance. CF supplementation induced a significant increase of mitochondrial mass, while significant qualitative adaptation occurred to maintain H2O2 production and cellular oxidative stress. CF supplementation induced a significant increase in NAD+ and NADH content. All the effects mentioned above were blunted in Sirt3-/- mice. Collectively, CF supplementation boosted the NAD metabolism that stimulates sirtuins metabolism and improved mitochondrial function, which likely contributed to the observed whole-body metabolism adaptation, with a greater ability to use carbohydrates, at least partially through Sirt3.","Mitochondria are subcellular organelles that are involved in multiple cellular functions, such as energy transduction through mitochondrial oxidative phosphorylation and in mitochondrial hydrogen peroxide production or mitochondrial-mediated cell death activation. Emerging evidence suggests that mitochondrial dysfunction is involved in many pathologies and plays a central role in their pathogenesis. Our group reported mitochondrial impairment in patients with type 1 diabetes long before clinical complications. Moreover, a decrease of mitochondrial oxidative capacity occurs with aging even in healthy people. Therefore, identifications of strategies to mitigate these dysfunctions or enhance mitochondrial mass are opportunities to prevent, or at least to reduce, the incidence of many disorders or increase aerobic capacity of healthy subjects.
In this context, the identification of safe and natural compounds that enhance mitochondrial function and whole-body metabolism with or without limited side effects is of interest. Among natural compounds, cocoa flavanols (CF) are considered promising molecules as their consumption was shown to positively affect cardiovascular health, insulin resistance, or immune function. Indeed, CF was shown to improve glucose metabolism, and a short term administration of CF is followed by an increase in insulin sensitivity, even in glucose-intolerant hypertensive patients.
Among the flavanols, (-)-epicatechin (EPI) is the most commonly found monomer in CF, and EPI is considered the major bioavailable and bioactive molecule of CF. A consistent body of evidence indicates that EPI supplementation improves mitochondrial function and/or content in skeletal muscle. EPI supplementation stimulates multiple pathways converging on peroxisome proliferator-activated receptor gamma coactivator 1-alpha (PGC1α) and major nuclear transcriptional complexes. EPI directly enhances nitric oxide generation, which stimulates mitochondrial biogenesis. Concomitantly, the sirtuins 1 and 3 are also stimulated by EPI supplementation. The use of proanthocyanidins, oligomeric flavonoids which contain EPI, was shown to increase the intracellular nicotinamide adenine dinucleotide (NAD+) levels through the increase of several precursors for NAD biosynthesis and upregulate sirtuin-1 mRNA levels in rat liver. The sirtuin family (Sirt), an NAD+-dependent deacetylase, comprises seven members that differ by their subcellular distribution, substrate specificity, and cellular functions. Sirt1, an extensively studied member of this family, stimulate mitochondrial biogenesis by promoting deacetylation of PGC1α, thereby enhancing its transcriptional activity. Sirt1, a key metabolic sensor, was stimulated following EPI administration. However, the influence of EPI on other sirtuins have not been explored. The assessment of EPI effect will be particularly interesting on sirtuins located within the mitochondria (Sirt3, Sirt 4, and Sirt5), which are known to modulate the activity of the Krebs cycle and respiratory chain enzymes. Sirt3, which is highly expressed in tissues with high metabolic turnover and mitochondrial content, is of particular interest concerning its critical role in maintaining normal mitochondrial function through reversible protein lysine deacetylation. Moreover, Sirt3 plays an important role in the regulation of whole-body metabolism. Sirt3 was shown to be altered in skeletal muscle of models of type 1 and type 2 diabetes and cardiovascular diseases. Furthermore, it was suggested that activation of Sirt3 might represent a promising therapeutic strategy for the improving mitochondrial function and metabolism. Therefore, determining whether Sirt3 underlies part of the beneficial effects of CF supplementation on whole-body metabolism and mitochondrial function is of interest.
In this study, we used an integrative approach to investigate, in mice, the effect of a 15-day CF supplementation. We hypothesised that the CF supplementation would: (i) modify whole-body metabolism and glucose metabolism, (ii) increase mitochondrial function in oxidative and glycolytic muscle in permeabilized fibers, and (iii) boost NAD metabolism. We also explored the putative involvement of Sirt3 in CF-induced mitochondrial biogenesis and whole-body metabolism. Our hypotheses were that CF’s effects on whole-body metabolism and mitochondrial mass would be blunted in Sirt3-/- mice.
","Daily CF supplementation for 15 days leads to mitochondrial function improvements in oxidative and glycolytic muscles. The present results suggest that the increased mitochondrial respiration results in an increase in mitochondrial mass while the cellular level of ROS production was maintained, suggesting qualitative adaptations within the mitochondria. Concomitant to the cellular effect on NO metabolism previously described, CF modulates NAD metabolism, which stimulates sirtuins activity. Sirt3 plays a central role in mitochondrial adaptations and metabolic flexibility following CF supplementation. As sirtuins are recent targets in various disease pathogenesis, such as diabetes or asthma, further studies will explore the potential interest of using CF as a natural activator of sirtuins.",https://www.mdpi.com/2072-6643/13/10/3466,
"Interactions between season of birth, chronological age and genetic polymorphisms in determining later-life chronotype","People born in winter tend to be evening people, especially if they carry a certain version of the SIRT1 longevity-clock gene. The strongest is the A version.","Human chronotype, the temporal pattern of daily behaviors, is influenced by postnatal seasonal programming and ageing. The aim of this study was to investigate genetic variants that are associated with season of birth programming and longitudinal chronotype change. Longitudinal sleep timing and genotype data from 1449 participants were collected for up to 27 years. Gene-environment interaction analysis was performed for 445 candidate single nucleotide polymorphisms that have previously been associated with chronotype. Associations were tested using linear mixed model. We identified 67 suggestively significant genomic loci that have genotype-ageing interaction and 25 genomic loci that may have genotype-season of birth interaction in determining chronotype. We attempted to replicate the results using longitudinal data of the UK Biobank from approximately 30,000 participants. Biological functions of these genes suggest that epigenetic regulation of gene expression and neural development may have roles in these processes. The strongest associated gene for sleep trajectories was ALKBH5, which has functions of DNA repair and epigenetic regulation. A potential candidate gene for postnatal seasonal programming was SIRT1, which has previously been implicated in postnatal programming, ageing and longevity. Genetic diversity may explain the heterogeneity in ageing-related change of sleep timing and postnatal environmental programming of later-life chronotype.","The circadian biological clock, located in the suprachiasmatic nucleus of the hypothalamus, is responsible for generating approximately 24-h rhythms in physiology and behavior and synchronizing them to the environment an organism lives in (Hastings et al., 2018). In humans, the synchronization between the solar cycle and a person’s temporal pattern of daily behaviors defines what is known as chronotype (Roenneberg et al., 2003). Sleep timing is one the manifestations of chronotype. The midpoint of sleep, the scale measure of the chronotype, is normally distributed in the population (Roenneberg et al., 2003). Morning-type (often referred to as ‘larks’) have early sleep onset and offset times compared to evening-type (‘owls’) who have late sleeping and waking habits (Roenneberg et al., 2003). Evening chronotype and disruption of daily rhythm have been associated with metabolic syndrome (Vera et al., 2018; Yu et al., 2015), cardiovascular problems (Patterson et al., 2018), diabetes (Merikanto et al., 2013), cancer (Papantoniou et al., 2015), and psychiatric disorders (Wulff et al., 2010; Au and Reece, 2017). Low sleep quality, early awakening, short duration and increased sleep fragmentation are also common in older adults, which may indicate that the biological rhythm deteriorates with age (Mander et al., 2017). Disrupted circadian rhythms in the elderly have been shown to contribute to health problems and mortality (De Nobrega and Lyons, 2018; Didikoglu et al., 2019). Understanding the biological systems involved in determining chronotype may therefore increase our knowledge of these conditions. Genetic variation explains up to 50 % of the chronotype variation in the population (Lane et al., 2016; Koskenvuo et al., 2007; Klei et al., 2005). Chronological age, sex and environmental factors also contribute to chronotype (Didikoglu et al., 2019; Roenneberg et al., 2007; Fischer et al., 2017).

Development of the clock within the suprachiasmatic nucleus starts in utero and synchronizes to natural cues immediately after birth (Landgraf et al., 2014). The early postnatal period is critical for maturation of the circadian rhythm. Environmental factors such as light and temperature may influence this development (Brooks and Canal, 2013). Studies in rodent models have shown that postnatal seasonal day length programs later-life biological rhythms by changing the transcription of clock genes which in turn influences the circadian pacemaker and the period of the rhythm (Brooks and Canal, 2013; Ciarleglio et al., 2011). In humans, epidemiological studies have reported that increasing day length is associated with evening preferences (Natale and Adan, 1999; Tonetti et al., 2011; Vollmer et al., 2012). However, the literature is not consistent. Some studies report no association (Huang et al., 2015; Touchette et al., 2008) and others report an effect in the opposite direction (Didikoglu et al., 2019; Tegowska et al., 2006). Chronotype is not set for life but adjusts with age, with late sleep timing typically prevalent in adolescence before shifting towards earlier times as we age (Roenneberg et al., 2007). However, there is significant heterogeneity in terms of inter-individual sleep change (Didikoglu et al., 2019). Genetic variants may play a role in determining chronotype and longitudinal sleep trajectories.

Both postnatal environmental programming and ageing show heterogeneity in the population. We hypothesized that genetic variations may result in divergence between individuals by modulating their response to seasonal programming and ageing. Our aim was to identify genetic variants that are associated with season of birth programming and time-invariant chronotype. Gene-environment interaction analysis was performed for candidate single-nucleotide polymorphisms (SNP), previously associated with chronotype.","Understanding the determinants of the human biological clock is important due to its involvement in health and well-being. In this study, we investigated the impact of genetics in postnatal development and life-long change of chronotype. Using longitudinal sleep data of older adults and mixed-effect linear regression, we were able to compare individual trajectories against genotype. We attempted to replicate previous associations between SNPs and chronotype. In addition, we showed that genetic variation may contribute towards inter-individual heterogeneity of season of birth programming and ageing effect in determining later-life chronotype. Associated variants regulated epigenetic and neuronal development processes thus implicating these mechanisms in chronotype variation.

Previous evidence has demonstrated that timing of daily behaviors is critical for physical and mental health. Eveningness has been associated with depression (Au and Reece, 2017), cardiovascular disease (Patterson et al., 2018), metabolic syndrome (Vera et al., 2018; Yu et al., 2015), diabetes (Merikanto et al., 2013) and mortality (Didikoglu et al., 2019). Evening preference has been correlated with harmful behaviors including smoking (Randler, 2008), reduction in exercise (Laborde et al., 2015; Klimentidis et al., 2018) and irregular eating habits (Maukonen et al., 2017). Internal clock misalignment, such as shift-work, has also been linked to psychiatric disorders (Jones et al., 2019a; Murray et al., 2017), cardio-metabolic problems (Barclay et al., 2012; Scheer et al., 2009), obesity (Roenneberg et al., 2012), inflammation (Minami et al., 2018) and cancer (Papantoniou et al., 2015; Kettner et al., 2016). Understanding the interaction of genetic variation and chronotype may provide further insights into susceptibility of these and other health conditions (Jones et al., 2019a).

Heritability of human chronotype has been estimated to be between 0.12 and 0.50 by twin and family studies (Koskenvuo et al., 2007; Klei et al., 2005; Byrne et al., 2013; Gottlieb et al., 2007). SNP-based chronotype heritability studies support these findings with estimates that range between 0.12 and 0.21, with approximately 4% of this contribution determined by clock genes (Lane et al., 2016; Jones et al., 2019a). In addition, the short variant of a variable number tandem repeat (VNTR) in the Period 3 gene, which is one of the main clock genes, is associated with evening preferences (Archer et al., 2003). Variation in the chronotype traits may be explained not only by genetic variants such as SNPs and VNTRs, but also by differences in the transcription level of clock genes. This is supported by studies that have shown that individuals with an evening chronotype have longer periods and delayed phases of circadian rhythm gene expression (Brown et al., 2008; Ferrante et al., 2015; Takahashi et al., 2018). GWAS have also reported associations between the biological clock and sleep timing (Byrne et al., 2013; Gottlieb et al., 2007; Spada et al., 2016). In 2016, three GWAS consistently reported that variants in RGS16, PER2, AK5 and FBXL13 genes are linked to chronotype and the associated SNPs were related to the circadian rhythm pathways (Lane et al., 2016; Jones et al., 2016; Hu et al., 2016). Recently, a GWAS meta-analysis of 697,828 participants reported 351 chronotype-associated loci (Jones et al., 2019a).

One of the strengths of our UMLCHA study is the use of longitudinal measures of self-reported chronotype. This has enabled us to show that 65 of the previously reported SNPs have significant association with the longitudinal mid-sleep time trajectories. A genetic study of chronotype showed that genetic variants that had previously been associated with self-reported chronotype (morning or evening person) are correlated with accelerometer activity measures (Jones et al., 2019a). Another genetic study using UK Biobank accelerometer data from 85,670 participants showed that even if direction and effect sizes are correlated, only 7 genetic loci significantly associated with sleep timing phenotypes (Jones et al., 2019b). This may be the reason why we could not replicate most of the previously reported genetic variants. The UK Biobank results suggest that chronotype is a complex trait and includes mid-sleep time, least-active 5 h during the day (L5), most-active 10 h during the day (M10), subjective categorical chronotype and longitudinal change of chronotype. These distinct dimensions of the diurnal behavioral pattern may be regulated by different genetic variants.

Natale and Adan first suggested that chronotype may be a trait which is programmed by season of birth, with an increased day length being associated with an evening chronotype (Natale and Adan, 1999). Subsequent studies have attempted to test this hypothesis, although the results have been mixed, with some replicating the original finding (Natale and Adan, 1999; Tonetti et al., 2011; Vollmer et al., 2012; Natale et al., 2009; Vitale et al., 2015), some reporting that long day length was associated with morning preferences (Didikoglu et al., 2019; Tegowska et al., 2006), and others finding no association (Huang et al., 2015; Touchette et al., 2008). Explanation for the inconsistency include the different methodologies used. Firstly, the classification of seasons, the measurement of chronotype and statistical methods to compare between groups are different (Tonetti et al., 2011; Vollmer et al., 2012; Vitale et al., 2015). Secondly, the mean age of the cohorts vary between studies (Didikoglu et al., 2019; Huang et al., 2015; Touchette et al., 2008; Vitale et al., 2015). Finally, studies have been conducted in different countries, which may influence sleep timing by varying latitudes and socio-cultures (Tonetti et al., 2011; Natale et al., 2009). In our study, we found that genetic diversity may also be contributing to the conflicting results. We showed that this imprinting-like phenomenon may vary according to genetic background.

Studies in animal models have revealed that the early postnatal period is critical for development of the hypothalamic suprachiasmatic nucleus (SCN), which is where the principal circadian pacemaker in mammals is located (Brooks and Canal, 2013). For example, if mice were raised under a long day length (16 h light, 8 h darkness), they had shorter periods of clock gene transcription in the SCN, altered SCN neuronal function and behavioral rhythm compared to mice raised under a short day length (8 h light, 16 h darkness) (Ciarleglio et al., 2011). Azzi et al. (2014) demonstrated in mice that different early light experiences change the transcription levels of chromatin modifiers, DNA methyltransferases and program behavioral circadian rhythms via differential DNA methylation in the SCN (Azzi et al., 2014). In humans, season of birth has been associated with differential blood DNA methylation (Lockett et al., 2016; McDade et al., 2017). These findings show that epigenetic modifications may regulate seasonal early-life programming. Epigenetic regulations are more dynamic in the early developmental period and have been suggested as a major mechanism for environmental programming (Feil and Fraga, 2012). Our results support the involvement of epigenetics in seasonal programming. However, epigenetic regulations such as DNA methylation are influenced by genetic variants (Grimm et al., 2019). In support of this, we have shown in the UMLCHA and UK Biobank cohorts that genetic differences may explain various responses to seasonal programming and that genes, including protein deacetylase SIRT1, which are responsible for epigenetic regulation, are enriched in this interaction. SIRT1 has also been indicated to have a role in fetal metabolic programming by maternal diet (Nguyen et al., 2019).

A variant in ARNTL gene had the lowest p-value in season of birth interaction. ARNTL has previously been reported to be associated with seasonality of mood and metabolism (Garbazza and Benedetti, 2018). In our study, some of the variants that have an interaction with season of birth are linked to the reproductive system. Evolutionarily, perinatal seasonal programming plays a role in enhancing survival, by modulating breeding and growth periods. Animals born in non-breeding seasons undergo a delayed reproductive development and puberty (Beery et al., 2008). Epidemiological studies suggest that season of birth is associated with survival at birth, lifetime disease risk and life expectancy (Doblhammer and Vaupel, 2002; Boland et al., 2015). Moreover, season of birth may impact birth weight and puberty in humans (Day et al., 2015). It has been reported that changes in light exposure influence DNA methylation, mainly in neuronal developmental genes (Azzi et al., 2014). This is supported by our finding that proteins responsible for nervous system development, including processes of migration and differentiation of cells and axonal elongation, may be regulating the photoperiodic programming of circadian rhythms. For example, we found that SEMA3G, an axon guidance gene, is significantly influenced by season of birth. Interestingly, this gene has been shown to undergo transcriptional change in response to light stimulus (Hrvatin et al., 2018). Furthermore, several studies have shown that perinatal photoperiodic input may affect neural function and cellular composition (Canal et al., 2009; Green et al., 2015).

Disruption of sleep and circadian rhythms are common in the elderly and have been reported to contribute towards oxidative stress, inflammation, atherosclerosis, neuropathology, cognitive decline, health problems and mortality (Mander et al., 2017; De Nobrega and Lyons, 2018; McAlpine et al., 2019; Hill et al., 2018). Since sleep problems and ageing cooccur, understanding how sleep change with ageing is important to prevent health problems. During ageing, circadian clock output decreases in amplitude, with contributors including transcriptional and neural changes in the SCN (Duffy et al., 2015). Dynamic diurnal epigenetic regulation is required to maintain normal function of the biological clock with ageing reducing the efficiency of this process (Deibel et al., 2015). However, genetic variation may influence these age-dependent changes. A previous study showed that longitudinal accumulation of DNA methylation with increasing age between individuals may vary according to genotype (Zhang et al., 2018). This effect has previously been shown with a comparison of circadian phenotypes between mice that come from distinct strains. Here the authors reported a differential ageing effect (Banks et al., 2015). This is further supported in human longitudinal studies where genetic polymorphisms influenced chronotype trajectories in adolescents (Merikanto et al., 2018). Our results show the same genetic-ageing interaction in the elderly. It has been previously reported that SNPs in the TNIP2 and CNTN4 genes moderate ageing-dependent methylome change, which may lead to accelerated epigenetic age and contribute to health problems and mortality (Zhang et al., 2018; Lu et al., 2018). We have found that variants in these genes and SIRT1, SLBP and ERI1 have a significant ageing interaction in our study. The latter two genes have been implicated in the regulation of chromatin structure via modifications of histone proteins (Marzluff and Koreski, 2017). Histone mRNA has a specific region that forms a stem-loop. SLBP is a protein which binds to that region and regulates the abundance of histones by controlling translocation to the cytoplasm, translation and finally degradation of the mRNA with the help of exonuclease ERI1 (Marzluff and Koreski, 2017). Another interesting protein, SIRT1, is involved in epigenetic regulation of gene expression and circadian rhythm with its abundance in the SCN decreasing with age (Chang and Guarente, 2013). Mutant rodents that are not able to express SIRT1 had dysregulation in PER2 gene expression and suffered accelerated ageing and disrupted circadian rhythm (Chang and Guarente, 2013; Wang et al., 2016). In humans, SIRT1 has also been associated with longevity (Tacutu et al., 2018). Our results build on these findings by showing that SIRT1 may also be involved in human age-related circadian rhythm change.

The strongest association between the variants and age-sleep interaction was rs12496304 which is an intron variant at ROBO2 developmental axon guidance gene. This gene has a role in differentiation of monoaminergic neurons (Couch et al., 2004) and a polymorphism in this gene was suggested to influence monoamine metabolites in cerebrospinal fluid (Luykx et al., 2014). We have found that neurotransmitter-related proteins may also be involved in the chronotype ageing effect. SLC18A2 is a monoamine transporter that regulates synaptic secretion of serotonin, histamine and noradrenalin, and PTK2B controls function of a glutamatergic receptor (Rilstone et al., 2013; Heidinger et al., 2002). These neurotransmitters have roles in the sleep cycle and any changes in their level of transcription may explain the age effect (Brown et al., 2012). Another of our associated neurotransmitter-related genes is PLCL1. In our study, the non-synonymous rs1064213 exonic variant in this gene may alter PLCL1 protein structure. The A allele was associated with time-invariant chronotype suggesting that this variant is functional. PLCL1 codes for a protein that has a role in the internalization of the GABAA receptor from the cell membrane and therefore, regulates the abundance of the receptor and modulates the GABAergic signaling in the synapse (Kanematsu et al., 2007). GABAergic inhibition is one of the key mechanisms in controlling the sleep-wake cycle and has been shown to decline with age (Brown et al., 2012; Palomba et al., 2008).

DNA repair is another common mechanism for sleep and ageing. Ageing is characterized by the changes to the DNA including the accumulation of mutations and methylation (Lindahl, 1993; Mourrain and Wang, 2019). DNA repair is primarily conducted during periods of sleep and the accumulation of DNA damage may generate sleep pressure (Mourrain and Wang, 2019). In line with these findings, our results show that ALKBH3-5 and FTO DNA alkylation repair mechanisms may be involved in longitudinal sleep timing change. The variant rs2232831 that influences ALKBH5 was significant after Bonferroni correction in the UMLCHA cohort and rs7111582 which influences ALKBH3 was significant in both cohorts. These AlkB Homologs work in oxidative demethylation of nucleic acids especially N1-methyladenine and N3-methylcytosine (Ougland et al., 2004). In addition, these genes post transcriptionally modify N1-methyladenine and N6-methyladenine, which regulate processing, localization and stability of mRNAs (Dominissini et al., 2016). This novel epigenetic mechanism has also been linked to circadian clock period length (Hastings, 2013).

Our study has some limitations. Firstly, we have only used previously reported chronotype-related autosomal SNPs in this analysis. Genome-wide gene-environment interaction analysis that includes structural variants, sex chromosome and mitochondrial variants may uncover further associations. The sleep phenotypes in the UK Biobank and the UMLCHA cohort were not exactly similar, which may limit the power of the replication analysis. We adjusted age and sex in our analyses, but other covariates may have confounding effects. There is a genetic correlation between blood pressure, psychiatric disorders, body mass index and personality (Table S5). Because of coregulation of these traits and sleep, it is difficult to find direct relation between sleep and ageing. However, physical and mental health problems, working and usage of sleep medication are less frequent in our cohort. Moreover, performing a sensitivity analysis, we showed that our results are robust (Table S6). In addition, using longitudinal data has a limitation of missing phenotype. Participants that have incomplete data in some waves are likely to have bad health and sleep characteristics. However, a previous survival analysis indicated drop-out effect is minimal in this cohort (Didikoglu et al., 2019).

In conclusion, early-life seasonal day length programs the timing of diurnal behaviors and chronotype is influenced by age. Here, we investigated the role of genetic polymorphisms on these processes. We identified 67 loci that may be associated with longitudinal sleep timing trajectories and 25 loci associated with season of birth effect in the UMLCHA cohort. These variants are likely involved in time-dependent changes in chronotype and postnatal season programming. Our results indicate that epigenetic mechanisms and neural development may be the main mechanisms underlying these changes. Gene-environment interactions are key to understanding later-life circadian rhythms. Finally, genetic variants involved in chronotype may prove useful as biomarkers for healthy ageing and provide new targets for prediction, diagnosis and treatment sleep problems in older adults.",https://www.sciencedirect.com/science/article/pii/S004763742030049X?casa_token=DDsx2gHfFkMAAAAA:HJK5ZuO8TvOuBhvOnzVwgWN1udFZSSUFgZO0WHIwitzKFKrfacFfJ01fD2ZaOzi9fepQZtZeqg,
The pharmacological assessment of resveratrol on preclinical models of rheumatoid arthritis through a systematic review and meta-analysis,"A new paper assessing 18 studies of arthritis in mice concludes the molecule resveratrol, in pure form, reduces swelling, tissue damage & cartilage loss ","Resveratrol/RES (3,5,4′-trihydroxy-trans-stilbene) is a natural compound found in many food items and red wine, which exhibits pleiotropic biological effects. Several preclinical studies evaluating the efficacy of RES in animal models of rheumatoid arthritis (RA) have been conducted, but the diversity of the experimental conditions and of their outcomes preclude definitive conclusions about RES's efficacy. We, therefore, performed a meta-analysis to assess its efficacy in mitigating experimental RA. We searched three databases until January 2021 and used the random-effects model for drawing inferences. Eighteen studies involving 544 animals were used in this study. Pooled analysis showed that experimental RA causes paw swelling (Hedge's g = 9.823, p = 0.000), increases polyarthritis score and arthritis index, and RES administration reduces paw volume (Hedge's g = −2.550, p = 0.000), polyarthritis score, and arthritis index besides amelioration in the histopathological score and cartilage loss. RA is accompanied by increased oxidative stress due to high malondialdehyde (MDA) level (p < 0.001) and low superoxide dismutase (SOD) activity (p = 0.002), and RES reduced MDA level (p < 0.001) and increased SOD activity (p < 0.001). Experimental RA exhibited an increase in pro-inflammatory cytokines viz. tumor necrosis factor (TNF)-α (p < 0.001), interleukin (IL)-6 (p = 0.002), and IL-1 (p < 0.001); however, insufficient quantitative data precluded us from assessing changes in the anti-inflammatory cytokine, IL-10. In experimental RA, RES decreased TNF-α (p < 0.001), IL-6 (p < 0.001) and IL-1 (p = 0.001) and increased IL-10. This meta-analysis suggests that RES can be a clinically effective therapy for RA, pending clinical trials.","Rheumatoid arthritis (RA) is a painful inflammatory disease of joints
and has an autoimmune onset. RA is associated with extra-articular
manifestations like cardiovascular diseases, impaired renal function,
neurological disorders, reduced bone mineral density, etc. that increase
morbidity by 44.70% along with increased mortality (Chen et al., 2020).
Autoantibodies directed against the major cartilage proteins including
type-II collagen and proteoglycans result in the immune-destruction of
cartilage in joints (Xiao et al., 2021). Mainstream therapies for RA are
primarily directed against the inflammatory/immunological processes
that determine RA disease activity and proved effective in alleviating
pain and mitigating the severity of clinical and functional symptoms.
Non-steroidal anti-inflammatory drugs (NSAIDs) and corticosteroids
relieve the symptoms for a limited time by suppressing inflammatory
response and oxidative stress (Crofford, 2013). The first line
disease-modifying anti-rheumatic drugs (DMARDs) include the
anti-cancer drugs, methotrexate, and leflunomide. Failure of the
first-line DMARDs in preventing the clinical or radiological disease
progression is addressed by biologic DMARDs (bDMARDs) that are
mostly neutralizing antibodies against inflammatory cytokines such as
TNFα, IL-1, IL-6, and IL-6 receptor (Nzioka Mutua et al., 2017). Patients
receiving bDMARDs are at an increased risk of serious infection as the
pro-inflammatory cytokines that are involved in the pathogenesis of RA
are also involved in affording protection against microbial pathogens
(Pradeepkiran, 2019). There is a richness of diverse microbial pathogens
in the tropical countries which require immune surveillance executed by
the pro-inflammatory cytokines such as TNF-α and various ILs. Thus, the
bDMARDs pose a disadvantage for RA patients of tropical countries by
increasing the risk of infection by opportunistic microorganisms.
Therefore, there is a need for effective and safe alternatives to the present drugs for the treatment of RA.
Resveratrol/RES (3,5,4′
-trihydroxy-trans-stilbene) is a phytoalexin that is present in many dietary sources. RES has been evaluated clinically in several cancers (prostate, breast, colorectal and multiple
myeloma) (Patel et al., 2011), neurological disorders (Kennedy et al.,
2010), cardiovascular diseases (Wong et al., 2011), diabetes (de Brito
Oliveira et al., 2017), and non-alcoholic fatty liver disease (NAFLD)
(Jakubczyk et al., 2020). Promising effects of RES were observed in
neurological disorders, cardiovascular diseases, but the effects on
NAFLD and cancers remain inconclusive. RES has diverse cellular actions including cell growth, survival, and differentiation in various cell
types. These actions of RES are mediated chiefly by activating the
deacetylase sirtuin 1 (SIRT1) (Hao et al., 2017), resulting in the upregulation of Nrf1 and downregulation of TNF-α and IL-6 mediated induction of NF-κB, thereby mitigating the inflammatory burden and
oxidative stress. Such action mechanism of RES is relevant for the
treatment of RA as NF-κB activation is reported in the rheumatoid
synovium (Benito et al., 2004; Handel et al., 1995).
Literature now possesses a robust understanding of mechanism-ofaction of RES and its clinical assessment in several cancers and cardiometabolic diseases observed a wide safety and tolerability profile (up
to 5g/day for several months) (Nelson et al., 2020). These two conditions allow testing clinical efficacy of RES in other diseases provided
preclinical efficacy has been established. Therefore, we undertook a
meta-analysis of the preclinical data to assess the effect of RES in
experimental RA that could guide and facilitate future clinical studies in
RA patients. We first performed a systematic review of the literature
describing the induction of RA and the effect of RES in ameliorating the
disease in the preclinical setting followed by meta-analyses of the
results. ","Besides the lack of studies on the effects of RES in humanized mouse
models of RA that mimic human RA more closely than adjuvant-induced
arthritis, RES has not been compared head-to-head with any DMARDs.
In addition, the effect of RES in combination with non-bDMARDs could
be studied to curtail the drug dose and consequent increase in therapeutic (safety) index of the drug.
The heightened inflammatory milieu of RA not only degrades cartilage but also causes focal destruction of marginal and subchondral
bones, juxta-articular osteoporosis, and generalized osteopenia (Seriolo
et al., 2006; Wang et al., 2014). Members of TNF family cytokines are
generally pro-osteoclastogenic, out of which RANKL is the most potent.
TNF-α, IL-1β, IL-6, and other inflammatory cytokines produced at higher
levels in RA potently induce RANKL production from the osteoblastic
cells (Komatsu et al., 2021). This meta-analysis showed that RES
decreased TNF-α, IL-1, and IL-6 which could likely diminish RANKL. The
ratio of RANKL and osteoprotegerin (OPG, the decoy receptor for
RANKL) is elevated in serum or synovial fluid of RA patients with
osteoporosis. RES decreases RANKL and increases OPG production in
osteoblasts (Feng et al., 2018) and also inhibits RANKL-induced osteoclastogenesis (He et al., 2010). Therefore, RES appears to have the dual
action of protecting joints and bones against RA. Pre-clinical studies
assessing the effect of RES in juxta-articular osteoporosis of long bones
and generalized osteopenia should be undertaken in the RA model.
Given the well understood toxicity profile of RES, combined with its
nutraceutical use to reduce cardio-vascular diseases- and cancer risks,
from the meta-analysis of the plethora of preclinical in vivo evidence we
have summarized here, it is timely to consider RES as a pharmacotherapy for RA. ",https://www.sciencedirect.com/science/article/pii/S0014299921006580?casa_token=urY1on7UyAYAAAAA:6fd0j5mBuXa08qc84uJVqbzvVWiNcBIowCyhVvMKKZI1scACtVfqos-3HQMA1viOyZS6tlILGg,
"Effect of dietary L-theanine supplementation on skeletal muscle fiber type
transformation in vivo",A new study in mice finds that the sleep inducing suplement L-Theanine also induces the Sirt1 gene and makes muscle fatigue-resistant and more antioxidant capacity & mitochondria,"The aim of this study was to investigate the effect of dietary L-theanine supplementation on skeletal muscle fiber type transition in mice. Our data indicated that dietary 0.15% L-theanine supplementation significantly increased the mRNA expression levels of muscle fiber type related genes (MyHC I, MyHC
IIa, PGC-1α, Sirt1, Tnnt1, Tnnc1, Tnni1, MEF2C) and the protein expression levels of MyHC IIa, myoglobin, PGC-1α, Sirt1 and Troponin I-SS, but significantly
decreased the mRNA and protein expression levels of MyHC IIb. Dietary 0.15% L-theanine supplementation significantly increased the activities of SDH
and MDH and decreased the activity of LDH. Furthermore, immunofluorescence demonstrated that dietary 0.15% L-theanine supplementation significantly
increased the percentage of type I fibers, and significantly decreased the percentage of type II fibers. In addition, we found that dietary 0.15% L-theanine
supplementation increased the fatigue-resistant, antioxidant capacity, mitochondrial biogenesis, and function in skeletal muscle of mice. Furthermore, dietary 0.15% L-theanine supplementation significantly increased the mRNA levels of prox1, CaN and NFATc1, the protein levels of prox1, CNA and NFATc1
and the activity of CaN in GAS muscle when compared with the control group. These results indicated that dietary L-theanine supplementation promoted
skeletal muscle fiber transition from type II–type I, which might be via activation of CaN and/or NFATc1 signaling pathway.","L-Theanine (γ -glutamylethylamide), a characteristic non–
protein-derived amino acid widely distributed in tea plant
(Camellia sinensis), accounts for 1–2% of dry weight of tea leaves.
It is the predominant amino acid in green tea, accounting for 50%
of the total free amino acids. L-theanine has recently attracted
much attention due to its various biological functions. Accumulating evidence has demonstrated that L-theanine can improve
growth performance, antioxidative capacity, immune function and
intestinal health of duck, and broilers. Additionally, it
has been reported that dietary supplementation of L-theanine can
improve meat quality of broilers and attenuate transport-stressinduced impairment of meat quality of broilers by improving
muscle antioxidant status. Muscle fiber is the basic unit of skeletal muscle and its characteristics is one of the most key
factors affecting meat quality. There is growing evidence that
some nutrients improve meat quality through changing muscle
fiber characteristics. However, until now, there is no study
on the effect of L-theanine on skeletal muscle fiber type transformation. Therefore, the present study investigated the effect of
dietary L-theanine supplementation on skeletal muscle fiber type
transformation in mice.","Skeletal muscle consists of different types of muscle fibers. According to contractile and metabolic properties, muscle fibers can
be divided into four types: type I (slow twitch, oxidative), type
IIa (fast-twitch oxidative), type IIb (fast-twitch glycolytic), and type
IIx (fast-twitch, oxidative-glycolytic). There are four myosin heavy
chain (MyHC) isoforms (MyHC I, MyHC IIa, MyHC IIb, MyHC IIx)
expressed in skeletal muscle. Until now, some nutrients, such
as resveratrol, arginine, leucine, ferulic acid and apple polyphenol,
have been reported to promote the formation of slow oxidative fiber and induce the fiber type transformation from type II–type
I. In this study, our data showed that dietary supplementation of L-theanine significantly increased the expression of slow
MyHC and decreased the expressions of fast MyHC and MyHC IIb.
The result was further confirmed by immunofluorescence. In addition, there is evidence that the increase of oxidized enzymes (such
as SDH and MDH) and the decrease of glycolytic enzyme (such as
LDH) can indirectly reflect the muscle fiber type transformation
from type II–type I. In this study, dietary supplementation of
L-theanine increased the activities of SDH and MDH and decreased
the activity of LDH. Together, these results suggested that dietary
L-theanine supplementation could promote skeletal muscle fiber
type transformation from type II–type I.
Type I fibers have higher levels of mitochondria and oxidative
enzyme activities than type II fibers. Furthermore, type I fibers are
considered to fatigue-resistant, while type II fibers (especially the
high proportion of type IIb fiber) are susceptible to fatigue. It
has been reported that fatigue is closely related to free radicals,
which can accelerate oxidative skeletal muscle fatigue. The increase of fatigue-resistant may be related to the decrease of oxidative stress and the improvement of mitochondrial function in skeletal muscle. The accumulation of LD is the major cause of
muscle fatigue. LDH, an important indicator of muscle damage, catalyzes the interconversion of pyruvate and lactate. In
addition, SUN (a metabolic product of protein and amino acid),
glycogen (an important resource of energy) and NEFA (mobilization and utilization of fat) are also key indicators to judge the degree of fatigue. There is evidence that the time to exhaustion
is used as an indicator of the degree of fatigue. In the present
study, our data showed that dietary supplementation of L-theanine
could increase the time to exhaustion of mice, decrease the serum
LD, LDH and BUN levels, and increase the levels of NEFA in serum and the content of liver glycogen and muscle glycogen. In addition,
L-theanine significantly increased the antioxidant capacity and mitochondrial biogenesis and function. Taken together, these results
suggested that L-theanine could delay fatigue by increasing antioxidant capacity and mitochondrial biogenesis and function, resulting
in an increase in the expression of type I fibers.
Calcineurin (CaN) is a heterodimer composing of a catalytic
subunit (CNA) and a regulatory subunit (calcineurin B, CNB), and
CNA is tightly bound to CNB. A large number of studies have
demonstrated that CaN and/or nuclear factor of activated T cells
(NFAT) signaling pathway plays an important role in regulating the
formation of slow oxidative fibers. Calcineurin is activated
by calcium and/or calmodulin, and then promotes the nuclear import of NFAT. The NFAT gene family (NFATc1, NFATc2, NFATc3,
and NFATc4) are all expressed in skeletal muscle, but calcium only
induces NFATc1 translocation from cytoplasm to nuclei in multinucleated myotubes. Previous studies have demonstrated that
NFATc1 is an essential NFAT isoform downstream of CaN responsible for regulating fiber type switching from fast-twitch to slowtwitch [28,29]. Moreover, recent studies have found that prox1 (a promising candidate gene of meat quality) is specifically expressed
in slow muscle fibers and it can induce nuclear translocation of
NFATc1 and activate the NFAT signaling pathway, and loss
of prox1 can cause a switch from slow- to fast-twitch fiber type. In the present study, dietary supplementation of L-theanine
significantly increased the protein expression levels of prox1, CNA
and NFATc1, and also increased the mRNA expression of prox1, CaN,
NFATc1, and its downstream target gene modulatory calcineurin interacting protein exon 4 isoform (MCIP1.4).
In conclusion, our results showed for the first time that dietary L-theanine supplementation possess fatigue-resistant effects by improving antioxidant capacity and mitochondrial biogenesis and function. Moreover, we also provided the first evidence that
dietary L-theanine supplementation promotes skeletal muscle fiber
type transformation from type II–type I, which might be achieved
by activation of CaN, and/or NFATc1 signaling.",https://www.sciencedirect.com/science/article/pii/S0955286321002795?casa_token=YmYPhfbKh6QAAAAA:rtUey2eWUYICFBO_f-q_kcLK1aDF-8fEw7XjK-AsKtjbRXUFJ6nUug9zLjsgQ-YIkRH-YrU4YQ,
Single-oocyte transcriptome analysis reveals aging-associated effects influenced by life stage and calorie restriction,"30-70% of eggs in fertility clinics do not produce babies because of broken DNA. In mice, fasting produces healthier eggs & the fasting-mimic, NMN, reverses infertility","Chromosome segregation errors in oocytes lead to the production of aneuploid eggs, which are the leading cause of pregnancy loss and of several congenital diseases such as Down syndrome. The frequency of chromosome segregation errors in oocytes increases with maternal age, especially at a late stage of reproductive life. How aging at various life stages affects oocytes differently remains poorly understood. In this study, we describe aging-associated changes in the transcriptome profile of mouse oocytes throughout reproductive life. Our single-oocyte comprehensive RNA sequencing using RamDA-seq revealed that oocytes undergo transcriptome changes at a late reproductive stage, whereas their surrounding cumulus cells exhibit transcriptome changes at an earlier stage. Calorie restriction, a paradigm that reportedly prevents aging-associated egg aneuploidy, promotes a transcriptome shift in oocytes with the up-regulation of genes involved in chromosome segregation. This shift is accompanied by the improved maintenance of chromosomal cohesin, the loss of which is a hallmark of oocyte aging and causes chromosome segregation errors. These findings have implications for understanding how oocytes undergo aging-associated functional decline throughout their reproductive life in a context-dependent manner.","Fertility in human females declines with age. Until menopause, which marks the end of the reproductive life by ceasing the ovulation cycle, the rates of infertility and pregnancy loss increase with age. One of the major contributors to these increases is egg aneuploidy. Aneuploidy is found in ~35% of spontaneous abortions, and most of the aneuploidies are derived from eggs (Nagaoka et al., 2012). In fertility clinics, 30%–70% of human eggs are found to be aneuploid, and the rate of aneuploidy increases with age (Fragouli et al., 2011; Gabriel et al., 2011; Geraedts et al., 2011). Aneuploid eggs are produced by chromosome segregation errors during meiosis in oocytes. How maternal aging leads to elevated rates of chromosome segregation errors in oocytes remains incompletely understood (Chiang et al., 2012; Herbert et al., 2015; Jones & Lane, 2013; Mihajlović & FitzHarris, 2018; Nagaoka et al., 2012; Webster & Schuh, 2017).

Aging impacts the transcriptome landscape of cells, and the effects depend on the type of cells. In ovarian follicles, oocytes communicate with their surrounding cumulus cells through gap junctions, which have critical roles in oocyte development and function (Kidder & Mhawi, 2002; Su et al., 2009). Although previous studies have described aging-associated changes in the transcriptome of oocytes, whether those changes are coordinated with their surrounding cumulus cells remains unknown. Previous studies used microarray approaches, which identified differential patterns of gene expression between the oocytes of young and old mice (Hamatani et al., 2004; Pan et al., 2008). Recently, single-oocyte RNA sequencing described transcriptome differences between mouse oocytes at 8 months old and at 1 month old (Zhang et al., 2019). However, chronological changes in the transcriptome landscape of single oocytes up to a late reproductive stage, at which aging-associated chromosome segregation errors are pronouncedly increased (e.g., 7–25% at ~15 months old in BDF1 mice (Pan et al., 2008; Chiang et al., 2010; Sakakibara et al., 2015)), remain unknown.

Reproduction can be affected by nutrition and by metabolic state. Calorie restriction (CR), which retards aging-associated functional declines in many experimental models (Benayoun et al., 2015; Guarente, 2008; Zhang et al., 2020), prevents aging-associated increases in egg aneuploidy in mouse oocytes (Selesniemi et al., 2011). CR prevents aging-associated increases in spindle abnormalities in oocytes (Selesniemi et al., 2011), which may partly explain the CR-dependent prevention of aneuploidy. However, the vast majority of chromosome segregation errors in oocytes of naturally aged mice are preceded by precocious chromosome separation (Sakakibara et al., 2015). This defect is at least partly due to the aging-associated, irreversible reduction of chromosomal cohesin, a protein complex that mediates chromosome cohesion (Burkhardt et al., 2016; Chiang et al., 2010; Lister et al., 2010). Thus, the CR-dependent prevention of egg aneuploidy may be mediated by the suppression of aging-associated reduction of chromosomal cohesin. Whether dietary conditions influence the aging-associated reduction of chromosomal cohesin in oocytes, and if so, whether the influences are associated with transcriptome changes in oocytes and their surrounding cumulus cells, remains unknown.

In this study, we characterized the transcriptome datasets of single oocytes and their surrounding cumulus cells at different life stages up to a late reproductive stage in mice. The results show that oocytes exhibit dramatic changes in their transcriptome profiles at a late reproductive stage, whereas their surrounding cumulus cells undergo changes at an earlier stage. CR promoted a transcriptomic shift in oocytes, with the up-regulation of genes involved in chromosome segregation. At the protein level, CR attenuated the aging-associated reduction of chromosomal cohesin. Thus, aging-associated effects on oocytes depend on life stages and can be modified by dietary conditions.","The results of this study indicate that aging-associated effects on ovarian follicles depend on cell type, life stage, and dietary conditions. Whereas cumulus cells undergo aging-associated transcriptome changes during early stages of reproductive life, oocytes undergo transcriptome changes at later stages of reproductive life. CR impacts the transcriptome of oocytes to a greater extent than that of cumulus cells. CR up-regulates genes involved in chromosome segregation, which is associated with the improved maintenance of chromosomal cohesin.

Our results indicate that oocytes and their surrounding cumulus cells undergo different aging-associated changes in transcriptome profiles. The transcriptome profile of oocytes is relatively stable against aging during the early stages of reproductive life, compared to the transcriptome profile of cumulus cells. In the late stages of reproductive life, however, oocytes undergo global changes in the transcriptome. These late-onset changes appear to be associated with a sharp increase in the rate of aneuploidy in eggs during the late stages of reproductive life, although whether this is a causal relationship is unknown. Our single-oocyte approach allowed us to perform a pairwise analysis of oocytes and their surrounding cumulus cells, which suggested that the degree of aging of individual oocytes at the transcriptome level is unpredictable from the transcriptome of their surrounding cumulus cells.

Our results demonstrate that CR substantially impacts oocytes. CR elevated the expression of genes involved in chromosome segregation and attenuated aging-associated reduction of chromosomal cohesin. Whether these effects contribute to the CR-dependent prevention of egg aneuploidy, reported previously (Selesniemi et al., 2011), remains unknown. Our study did not directly address CR-dependent prevention of egg aneuploidy, for which readers are referred to (Selesniemi et al., 2011). The connection between nutrition and oocyte quality is observed in several animal models. In Drosophila, a proper nutrient state during oogenesis is critical for the establishment of the unique quiescent state of oocytes (Sieber et al., 2016). In mammals, primordial oocytes are retained in a quiescent state that is prolonged with aging, and proteostatic regulation during the primordial stage is critical for the long-term maintenance of chromosomal cohesin (Burkhardt et al., 2016; Reichmann et al., 2020). Mouse oocytes have a functional insulin signaling cascade (Acevedo et al., 2007), which may be involved in the transcriptional response to CR. Insulin signaling is a key regulator of the function of mitochondria, which are the primary source of reactive oxygen species (ROS) (Guarente, 2008). In Drosophila, levels of the ROS scavenger superoxide dismutase (SOD) are critical for the long-term maintenance of chromosome cohesion in oocytes (Marquardt et al., 2016; Perkins et al., 2016). Thus, the modulation of oxidative stress is a potential mechanism that might be responsible for the CR-dependent attenuation of aging-associated reduction of chromosomal cohesin in oocytes. Alternatively, the up-regulation of genes involved in the maintenance of chromosome cohesion may also be involved in the CR-dependent effects on chromosomal cohesin.

Overall, this study reveals life stage-dependent changes in the transcriptome of oocytes and the influence of CR. Further investigation of molecular links from nutrition signals to aging-associated transcriptome dysregulation and protein deterioration in oocytes will be important for understanding the maintenance of female reproductive lifespan.",https://onlinelibrary.wiley.com/doi/10.1111/acel.13428,
The economic value of targeting aging,"Our new paper calculates the value to the US economy of widely prescribing Metformin, a cheap medicine that could extend lifespan by >2.6 years, is at least $38 trillion dollars.","Developments in life expectancy and the growing emphasis on biological and ‘healthy’ aging raise a number of important questions for health scientists and economists alike. Is it preferable to make lives healthier by compressing morbidity, or longer by extending life? What are the gains from targeting aging itself compared to efforts to eradicate specific diseases? Here we analyze existing data to evaluate the economic value of increases in life expectancy, improvements in health and treatments that target aging. We show that a compression of morbidity that improves health is more valuable than further increases in life expectancy, and that targeting aging offers potentially larger economic gains than eradicating individual diseases. We show that a slowdown in aging that increases life expectancy by 1 year is worth US$38 trillion, and by 10 years, US$367 trillion. Ultimately, the more progress that is made in improving how we age, the greater the value of further improvements.","Life expectancy (LE) has increased dramatically over the past 150 years1, although not all of the years gained are healthy. Analysis of the Global Burden of Disease dataset2 suggests that the proportion of life in good health has remained broadly constant, implying increasing years in poor health. Furthermore, the disease burden is shifting towards chronic non-communicable diseases, estimated to have caused 72.3% of deaths in the United States in 2016. The result is “a substantial part of life, and certainly most deaths, now occur in a period in the lifespan when the risk for frailty and disability increases exponentially.”3 As a consequence, there is a growing emphasis on ‘healthy aging’ and an emerging body of research focusing on the biology of aging (see refs. 4,5). According to another paper, “this era marks an inflection point, not only in aging research but also for all biological research that affects the human healthspan.”6

These developments pose a number of important questions. Is it preferable to make lives healthier by compressing morbidity, or longer by extending life? What are the gains from targeting aging itself, with its potential to make lives both healthier and longer? How does the value of treating aging compare to eradicating specific diseases? How will the economic value of these gains evolve over time? To answer these questions, we take an economic rather than biological perspective. Specifically, we use the value of statistical life (VSL) methodology to place a monetary value on the gains from longer life, better health, and changes in the rate at which we age7,8,9.

VSL models have two distinct advantages for our purposes. First, they are already used by government agencies to evaluate different policy measures and treatments, for example10,11. Second, as our model is based around optimizing economic agents, we can calculate not only the current gains from targeting aging but also how these gains will evolve in response to potential future changes in health and LE. The results reveal a distinctive feature of age-targeting treatments. Interactions between health, longevity, economic decisions and demographics create a virtuous circle, such that the more successful society is in improving how we age, the greater the economic value of further improvements.","The economic value of gains from targeting aging are large because delaying aging produces complementarities between health and longevity, affect a large number of diseases due to the rising prevalence of age-related comorbidities, and create synergies arising from competing risks. Crucially, delaying aging leads to a virtuous circle in which slowing aging begets demand for further slowing in aging. This virtuous circle arises because society’s gains from delaying aging rise with the average age of society, increase with the quality of life in old age, and depend on the number of older people. This provides a distinctive dynamic to targeting aging compared to treatments aimed at specific diseases, in which gains diminish once successful treatments are discovered.

Our estimates are larger than those in ref. 29, which calculates a slowdown in aging producing a 2.2-year increase in LE as worth US$7.1 trillion to those aged over 51. This case is closest to our 2-year increase in Table 4. Adjusting for differences in chosen discount rates and VSLs and restricting our gains to the over 50 s only leads to an estimate of the aggregate gains as worth US$21 trillion. The remaining differences are attributable to ref. 29 assuming a phased rather than immediate improvement in aging. Although differences remain, the most important insight is that their different approach (using an empirical microsimulation model based on US individual data) arrives at similar very large estimates for the value of delaying aging.

Our estimates abstract from both inequalities in health and income. Allowing for health inequalities is likely to increase the value of the aggregate gains, but introducing income inequality raises important distributional issues. Our estimates suggest that treatments that target aging are extremely valuable. If the cost of such treatments is low then access to them will be widespread. If, however, the costs are high then issues of access and redistribution will become important. What is clear from the magnitude of the potential values outlined in our simulations is the need to ensure widespread access if the full value of these social gains is to be realized.",https://www.nature.com/articles/s43587-021-00080-0#Sec9,
New intranasal and injectable gene therapy for healthy life extension,"Intranasal telomerase in mice, to extend old chromosomes, improves metabolism, physical performance, prevents loss of body mass, hair loss & extends lifespan by 40% (n=8)","As the global elderly population grows, it is socioeconomically and medically critical to have diverse and effective means of mitigating the impact of aging on human health. Previous studies showed that adenovirus-associated virus (AAV) vector induced overexpression of certain proteins can suppress or reverse the effects of aging in animal models. Here, we sought to determine whether the high-capacity cytomegalovirus vector can be an effective and safe gene delivery method for two such-protective factors: telomerase reverse transcriptase (TERT) and follistatin (FST). We found that the mouse cytomegalovirus (MCMV) carrying exogenous TERT or FST (MCMVTERT or MCMVFST) extended median lifespan by 41.4% and 32.5%, respectively. This is the first report of CMV being used successfully as both an intranasal and injectable gene therapy system to extend longevity. Treatment significantly improved glucose tolerance, physical performance, and prevented loss of body mass and alopecia. Telomere shortening seen with aging was ameliorated by TERT, and mitochondrial structure deterioration was halted in both treatments. Intranasal and injectable preparations performed equally well in safely and efficiently delivering gene therapy to multiple organs, with long-lasting benefits and without carcinogenicity or unwanted side effects. Translating this research to humans could have significant benefits associated with increased health span. ","How to achieve healthy longevity has remained a challenging subject in biomedical science. It has been well established that aging is associated with a reduction in telomere repeat elements at the ends of chromosomes, which in part results from insufficient telomerase activity. Importantly, the biological functions of the telomerase complex rely on telomerase reverse transcriptase (TERT). TERT plays a major role in telomerase activation, and telomerase lengthens the telomere DNA. Because telomerase supports cell proliferation and division by reducing the erosion of chromosomal ends in mitotic cells, animals deficient in TERT have shorter telomeres and shorter life spans. Recent studies on animal models have shown the therapeutic efficacy of TERT in increasing healthy longevity and reversing the aging process. Telomere shortening also increases the risk of heart disease by a mechanism that remains unclear. The follistatin (FST) gene encodes a monomeric secretory protein that is expressed in nearly all mammalian tissues. In muscle cells, FST functions as a negative regulator of myostatin, a myogenesis inhibitory signal protein. FST overexpression is known to increase skeletal muscle mass in transgenic mice by 194-327% by neutralizing the effects of various TGF-β ligands involved in muscle fiber break-down, including myostatin and activin inhibition complex. FST- knockout mice have smaller and fewer muscle fibers, and show retarded growth, skeletal defects, reduced body mass, and die in a few hours after birth, suggesting an important role of FST in skeletal muscle development. These findings strongly implicate the therapeutic potential of FST in the treatment of muscular dystrophy and muscle loss caused by aging or microgravity. Thus, TERT and FST are among prime candidates for gene therapy aimed to improve healthy life spans.

As more longevity-supporting factors are discovered, it is of interest to determine potential large capacity vectors for delivering multiple genes simultaneously. Unlike AAV, lentiviruses or other viral vectors used for gene delivery, cytomegaloviruses have a large genome size and unique ability to incorporate multiple genes. Cytomegaloviruses also do not integrate their DNA into the host genome during the infection cycle, thus mitigating the risk of insertional mutagenesis. They also do not elicit symptomatic immune reactions in most healthy hosts. Notably, the CMV vector does not invoke genome instability and has not been identified to cause malignancies. Human CMV (HCMV) has been proven a safe delivery vector for expressing therapeutic proteins in human clinical trials. MCMV and HCMV are similar in many aspects, including viral pathogenesis, homology, viral protein function, viral gene expression and viral replication. Using mouse cytomegalovirus (MCMV) as a viral vector, we examined the therapeutic potential of TERT and FST gene therapy to offset biological aging in a mouse model, and demonstrated significant lifespan increase, as well as positive metabolic and physical performance effects. Further studies may elucidate the full CMV cargo capacity and effectiveness. Translational studies are required to determine whether our findings can be replicated in human subjects.","FST treated mice showed an increase in body mass, as expected based on previous publications. We confirmed visually that skeletal muscle mass was indeed larger than that of controls at necropsy (data not shown). The increased robustness may explain the improved motor control in executing the beam test. However, it was unexpected that CMV-based FST gene therapy alone would increase longevity to the extent observed. Although it is known that FST has a concentration-dependent inhibiting effect on the myostatin-driven rate of muscle breakdown, which contributes to increasing frailty in aging individuals, the overall effect of increasing longevity warrants further inquiry. We anticipate that sarcopenia, muscular dystrophy, or even special circumstances causing muscle atrophy, such as low gravity exposure during space travel, could be mitigated with a CMV-based FST gene delivery method.

Another surprising finding was the equivalent effectiveness of both treatment regimens in blood glucose control, because the cellular mechanisms activated by TERT and FST which ultimately result in glucose control are different. FST has a systemic role in upregulating factors controlling mitochondrial biogenesis, energy metabolism, cellular respiration and thermogenesis, inducing browning of white adipose tissue. The FST interference with the TGF-beta signaling pathway resulted in the efficient regulation of glucose homeostasis we observed. On the other hand, TERT seems to act at the level of pancreatic beta cells by upregulating insulin secretion rather than effecting glucose uptake. Nonetheless, telomerase is known to interact with various cellular inflammatory pathways to reduce oxidative stress, and has been detected in mitochondria, where it protects mitochondria from oxidative damage, which explains the systemic benefits and increased longevity. Furthermore, FST and TERT have shown positive effects in neurological diseases, and the fact that our treatment showed that brain FST and TERT levels increase significantly over baseline supports its use for treatment of these conditions. It would be of great interest to understand the compounded effect the two therapies might have when delivered simultaneously.

Finally, our therapeutic regimen appeared to require monthly administration in order to have continuous effects, which may be advantageous when treatment indications do not require permanent expression of therapeutic load, but rather episodic or during specific circumstances, to achieve a reduced risk of long-term consequences in case of adverse reactions, should any occur.

In summary, our study justifies further efforts to investigate the use of CMV TERT and FST vectors against aging-related chronic inflammatory conditions, type 2 diabetes, sarcopenia, dementia, lung, kidney, and heart diseases responsible for decreased quality of life and premature death.",https://www.biorxiv.org/content/10.1101/2021.06.26.449305v1.full,
"Association of Meal and Snack Patterns With Mortality of All‐Cause, Cardiovascular Disease, and Cancer: The US National Health and Nutrition Examination Survey, 2003 to 2014","Study of 21.5K people for 150K human years finds eating veggies at DINNER is associated with lower risk of cancer, heart disease & death. Eating refined grains, cheese, cured meat, and red meat at LUNCH had the opposite effect","Although accumulating evidence has demonstrated that consumption time of energy and macronutrients plays an important role in maintaining health, the association between consumption time of different foods and cardiovascular disease, cancer, and all‐cause mortalities is still largely unknown.
A noninstitutionalized household population of the US 21 503 participants from National Health and Nutrition Examination Survey was included. Meal patterns and snack patterns throughout a whole day were measured using 24-hour dietary recall. Principal component analysis was performed to establish dietary patterns. Cox proportional hazards models were used to evaluate the association between dietary patterns across meals and cardiovascular disease (CVD), cancer, and all-cause mortalities. During the 149 875 person-years of follow-up, 2192 deaths including 676 deaths because of CVD and 476 because of cancer were documented. After adjusting for potential confounders, participants consuming fruit-lunch had lower mortality risks of all-cause (hazard ratio [HR], 0.82; 95% CI, 0.72–0.92) and CVD (HR, 0.66; 95% CI, 0.49–0.87); whereas participants who consumed Western-lunch were more likely to die because of CVD (HR, 1.44; 95% CI, 1.10–1.89). Participants who consumed vegetable-dinner had lower mortality risks of all-cause, CVD, and cancer (HRall-cause, 0.69; 95% CI, 0.60–0.78; HRCVD, 0.77; 95% CI, 0.61–0.95; HRcancer, 0.63; 95% CI, 0.48–0.83). For the snack patterns, participants who consumed fruit-snack after breakfast had lower mortality risks of all-cause and cancer (HRall-cause, 0.78; 95% CI, 0.66–0.93; HRcancer, 0.55; 95% CI, 0.39–0.78), and participants who consumed dairy-snack after dinner had lower risks of all-cause and CVD mortalities (HRall-cause, 0.82; 95% CI, 0.72–0.94; HRCVD, 0.67; 95% CI, 0.52–0.87). Participants who consumed a starchy-snack after main meals had greater mortality risks of all-cause (HRafter-breakfast, 1.50; 95% CI, 1.24–1.82; HRafter-lunch, 1.52; 95% CI, 1.27–1.81; HRafter-dinner, 1.50; 95% CI, 1.25–1.80) and CVD (HRafter-breakfast, 1.55; 95% CI, 1.08–2.24; HRafter-lunch, 1.44; 95% CI, 1.03–2.02; HRafter-dinner, 1.57; 95% CI, 1.10–2.23).
Fruit-snack after breakfast, fruit-lunch, vegetable-dinner, and dairy-snack after dinner was associated with lower mortality risks of CVD, cancer, and all‐cause; whereas Western-lunch and starchy-snack after main meals had greater CVD and all-cause mortalities.","Chrono-nutrition, as an emerging field of nutritional research, aims to understand how meal times impact health.The concept of chrono-nutrition emphasizes that, in addition to the quantity and quality of food, consumption time of diet is also critical for the well-being of an organism.4 Accumulating animal and human studies have shown that high energy intake at dinner is associated with dyslipidemia and hyperglycemia, whereas high energy intake at breakfast or prolonged time-restricted energy intake throughout the night have beneficial effects on body weight, glucose, lipid control, and long-term survival. Although these studies suggest that intakes of energy and macronutrients at different time periods impact health differently, it is still largely unknown whether and how the consumption time of different foods may impact health.
Furthermore, compared with examining a single food, an examination of dietary pattern parallel more closely resembles the real world, in which nutrients and foods are consumed in combination, and their joint effects may best be investigated by considering the entire eating pattern. Analyzing consumption time of dietary patterns across meals may therefore provide a comprehensive understanding of the health impact of chrono-nutrition. In this study, we hypothesized that a specific dietary pattern might differently affect health according to the time periods of consumption. In order to test this hypothesis, this study prospectively assessed the association of meal and snack patterns throughout a whole day with cardiovascular disease (CVD), cancer, and all-cause mortalities in the noninstitutionalized household population of the United States using data from the National Health and Nutrition Examination Survey (NHANES).","Nutritional recommendation is a critical element for maintaining public health. Nutritional guidelines and intervention strategies should integrate and emphasize the importance of optimal consumption times for foods in a day. Based on the findings in this study, the optimal consumption time for fruit was likely in the daytime, and the optimal consumption time for vegetables was at dinner. The dairy products could be consumed as a snack after dinner. This information is of importance in providing nutritional recommendations for the public.
In conclusion, higher intake of fruit at lunch, and higher intake of vegetables and dairy products in the evening were associated with lower mortality risks of CVD, cancer, and all‐cause; whereas higher intake of refined grain, cheese, added sugars, and cured meat at lunch, and higher intake of potato and starchy foods after main meals were associated with greater CVD and all‐cause mortalities.",https://www.ahajournals.org/doi/epub/10.1161/JAHA.120.020254,
"Time-restricted feeding improves blood
glucose and insulin sensitivity in overweight
patients with type 2 diabetes: a randomised
controlled trial","In a Randomized Control Trial of 10 hours TRE on people with type 2 diabetes found significant improvement in glucose control, reduced HbA1C and modest weight loss. ","Time-restricted feeding is an emerging dietary intervention that is becoming increasingly popular.
There are, however, no randomised clinical trials of time-restricted feeding in overweight patients with type 2 diabetes. Here, we explored the efects of time-restricted feeding on glycaemic regulation and weight changes in overweight patients with type 2 diabetes over 12 weeks.
Overweight adults with type 2 diabetes (n=120) were randomised 1:1 to two diet groups: time-restricted
feeding (n=60) or control (n=60). Sixty patients participated in a 10-h restricted feeding treatment program (ad
libitum feeding from 8:00 to 18:00 h; fasting between 18:00 and 8:00 h) for 12 weeks.
Haemoglobin A1c and body weight decreased in the time-restricted feeding group (−1.54%±0.19 and
−2.98±0.43 kg, respectively) relative to the control group over 12 weeks (p<0.001). Homeostatic model assessment of β-cell function and insulin resistance changed in the time-restricted feeding group (0.73±0.21, p=0.005;
−0.51±0.08, p=0.02, respectively) compared with the control group. The medication efect score, SF-12 score,
and the levels of triglycerides, total cholesterol and low-density lipoprotein cholesterol were improved in the
time-restricted feeding group (−0.66±0.17, p=0.006; 5.92±1.38, p<0.001; −0.23±0.08 mmol/L, p=0.03;
−0.32±0.07 mmol/L, p=0.01; −0.42±0.13 mmol/L, p=0.02, respectively) relative to the control group. High-density lipoprotein cholesterol was not signifcantly diferent between the two groups.
These results suggest that 10-h restricted feeding improves blood glucose and insulin sensitivity, results
in weight loss, reduces the necessary dosage of hypoglycaemic drugs and enhances quality of life. It can also ofer
cardiovascular benefts by reducing atherosclerotic lipid levels.","Diabetes mellitus has become an important global public
health issue; it imposes a huge economic burden on the
global healthcare system of $827 billion a year. Approximately 90% of people with diabetes have type 2 diabetes,
which is often associated with being overweight or obese.
Tis crisis will continue until a solution is found. In fact, the early observation linking calorie restriction (CR)
to improved health is now a century old. McCay found
that pups fed a limited diet lived longer than pups fed
randomly. Te positive efects of CR on longevity and
health have been demonstrated in many model organisms, such as fruit fies, mice and primates. In addition, CR interventions can prevent and treat a variety of
metabolic disorders, including diabetes.
While CR has many benefts, this type of diet may be
difcult because it requires a vigilant daily calorie count. Intermittent fasting, as an alternative to calorie
restriction, has become increasingly popular over the
past few decades. Intermittent fasting is divided
into three subtypes: alternate-day fasting, the 5:2
diet, and time-restricted feeding (TRF). Alternate-day
fasting is defned as alternating between ""fasting days""
and “free feast days"". Te 5:2 diet involves fasting for just
two days a week, followed by fve free eating days. Tese
two diets require a strict calorie count on fasting days
and a calorie limit of 800  kcal. Unlike these two diets,
time-limited feeding (TRF) does not require individuals
to deliberately count calories and monitor food intake.
TRF only restricts eating time to 4–12 h per day, where
one does not consume any calories during the remaining
hours of the day. Terefore, as an emerging dietary
strategy, TRF has attracted great attention.
To date, more than a dozen animal studies have examined the efects of TRF on metabolic disease.
Gill subjected Drosophila melanogaster adults to
12-h TRF of a standard cornmeal diet for 5 weeks. Teir
endurance, motor control and cardiac function improved
signifcantly. For cafeteria diet-induced obesity in rats,
the 8-h TRF regimen for 16 weeks is an efective strategy
to enhance body weight gain, lipid profles, and atherogenic indices. In mutant mice, 10-h TRF for 12 weeks
prevented obesity and metabolic syndrome. Mice
fed 8-h TRF for 16 weeks were protected against obesity,
hyperinsulinemia, hepatic steatosis, and infammation,
and their motor coordination was improved. Tus,
TRF has multiple metabolic benefts, preventing chronic
disease in mice and fies and, more importantly, reversing
the consequences of obesity and aging. Te benefcial efects were also evident in high-fat fed mice when
TRF was administered 5 days a week and free access to
food was allowed on weekends.
Te efects of TRF in humans have been poorly studied. To date, TRF has been studied in only seven human
trials. Human data on the benefts of TRF have
focused on healthy humans who are overweight or obese. Studies in overweight people showed
that 10–12-h TRF for 10  weeks decreased fat mass and
fasting plasma glucose concentration, and 10-h TRF
for 12  weeks reduced visceral fat. However, few studies have been performed on people with metabolic
disorders. A single-arm trial in people with metabolic
syndrome supported that 10-h TRF for 12 weeks induced
reductions in body weight, adiposity, lipaemia and blood
pressure. In men with prediabetes in a crossover
study, 6-h TRF for 5 weeks reduced signs of IR.
However, TRF as a behavioural intervention has never
been studied in patients with type 2 diabetes. It is not
known whether patients who have already received pharmacotherapy can beneft from adopting TRF. Previous
studies have shown that further narrowing of the feeding
window brings no additional benefts. Terefore, we
designed a 12-week 10-TRF intervention in overweight
patients with type 2 diabetes. We hypothesised that 10-h
TRF would improve blood glucose levels. It is also not
known whether the types and amounts of antidiabetic
drugs could be reduced or even stopped after adopting
TRF as a therapy.
Our research is the frst randomised controlled clinical
trial to explore the efect of TRF on type 2 diabetes. We
hypothesised that compared with the control condition,
a 10-h TRF intervention for 12  weeks would result in a
greater improvement in glucose regulation and insulin
sensitivity. We also anticipated weight loss and improvements in cardiovascular disease (CVD) risk markers,
such as atherogenic lipid levels. Finally, we expected that
the 10-h TRF group would have better quality of life and
require fewer medications.","Our research is the frst randomised controlled trial to
explore the efects of TRF in humans with type 2 diabetes. Our study showed that 10-h TRF reduced body
weight and blood glucose and improved insulin sensitivity in overweight patients with type 2 diabetes. Tese
results occurred without deliberate attempts to increase
physical activity and change the quality or quantity of
diet. Importantly, we also found signifcant improvement in CVD risk markers (triglycerides, total cholesterol and LDL cholesterol) without the use of stains or
fbrates. Additionally, when all of the above indicators
were signifcantly controlled, after the TRF intervention,
the dosage of hypoglycaemic drugs in the experimental
group of participants was signifcantly reduced, and their
perception of physical functions and daily activities were
improved. Furthermore, the good compliance, high level
of adherence to TRF, and low dropout rate in our study
indicate that the 10-h window for TRF may be feasible for
patients with type 2 diabetes to follow.",https://nutritionandmetabolism.biomedcentral.com/track/pdf/10.1186/s12986-021-00613-9.pdf,
,"In a controlled clinical study, when healthy adults slept 4h/day for 21 days, they ate more, put on one pound extra weight and gained belly fat.","Although the consequences of sleep deficiency for obesity risk are increasingly apparent, experimental
evidence is limited and there are no studies on body fat distribution.
The purpose of this study was to investigate the effects of experimentally-induced sleep curtailment in
the setting of free access to food on energy intake, energy expenditure, and regional body composition.
Twelve healthy, nonobese individuals (9 males, age range 19 to 39 years) completed a randomized,
controlled, crossover, 21-day inpatient study comprising 4 days of acclimation, 14 days of experimental sleep restriction
(4 hour sleep opportunity) or control sleep (9 hour sleep opportunity), and a 3-day recovery segment. Repeated
measures of energy intake, energy expenditure, body weight, body composition, fat distribution and circulating
biomarkers were acquired.
With sleep restriction vs control, participants consumed more calories (P ¼ 0.015), increasing protein
(P ¼ 0.050) and fat intake (P ¼ 0.046). Energy expenditure was unchanged (all P > 0.16). Participants gained signifi-
cantly more weight when exposed to experimental sleep restriction than during control sleep (P ¼ 0.008). While changes
in total body fat did not differ between conditions (P ¼ 0.710), total abdominal fat increased only during sleep restriction
(P ¼ 0.011), with significant increases evident in both subcutaneous and visceral abdominal fat depots (P ¼ 0.047 and
P ¼ 0.042, respectively).
Sleep restriction combined with ad libitum food promotes excess energy intake without varying energy
expenditure. Weight gain and particularly central accumulation of fat indicate that sleep loss predisposes to
abdominal visceral obesity","Habitual sleep deficiency affects more than
one-third of the U.S. adult population
and has been linked to obesity, morbidity,
and premature mortality. Although null findings
have been reported, observational population-
based data implicating short sleep duration as a factor
promoting obesity are strongly suggestive though
inferential. Conversely, experimental studies on
sleep curtailment and weight regulation are limited
and conflicting, and few laboratory-based investiga-
tions have monitored concurrently both energy
intake and energy expenditure. In addition,
whether sleep loss actually induces fat gain is un-
clear, with major limitations of previous studies including short duration of sleep manipulation and
use of surrogate measures of adiposity. Furthermore,
an unanswered and more relevant question is where
the excess fat is stored, because accumulation of fat
in the abdominal cavity (visceral obesity) is more haz-
ardous than other obesity phenotypes.
In this randomized, controlled, crossover, 21-day
inpatient study, we sought to investigate the effects
of prolonged sleep restriction vs normal (control)
sleep on energy intake, energy expenditure, and
regional fat storage in healthy nonobese individuals.","This study shows that prolonged experimental sleep
restriction in an obesogenic setting promotes excess
energy intake without affecting energy expenditure,
leading to preferential accumulation of fat in the
abdominal compartment, and especially in the
visceral depot. Our data provide insights into under-
standing the linkage between insufficient sleep and
heightened cardiometabolic risk, and have important
implications for public health policy and initiatives",https://www.sciencedirect.com/science/article/pii/S0735109722003102?casa_token=oJm8ySXX0TsAAAAA:Wg1s8Ld3LmaXFTSsuO4mO18WSUem7PlWABApvQJdyD8wwDEIqtUz6yx_HBQ7eYMZ9eOutRI,
Cigarette smoke-inactivated SIRT1 promotes autophagy-dependent senescence of alveolar epithelial type 2 cells to induce pulmonary fibrosis,"Cigarette smoke induces cell senescence & lung fibrosis by damaging DNA and lowering NAD. Is countered by NMN, rapamycin & the Sirt1 activator SRT1720 ","The senescence of alveolar epithelial type 2 (AT2) cells is implicated in the pathogenesis of idiopathic pulmonary fibrosis (IPF). Cigarette smoke (CS) is a strong risk factor for IPF and it is also a pro-senescent factor. Here we aimed to investigate whether and how CS induces AT2 cells senescence via a SIRT1/autophagy dependent pathway. Our results showed that CS extract (CSE) reduced autophagy and mitophagy and increased mitochondrial reactive oxygen species (mitoROS) in MLE-12 cells, an AT2 cell line. The autophagy inducer rapamycin (RAPA) and the mitochondria-targeted antioxidant mitoquinone (mitoQ) inhibited CSE-related senescence and decreased mitoROS. Next, we found that CSE promoted DNA damage, downregulated the nicotinamide adenine dinucleotide (NAD+)/nicotinamide adenine dinucleotide (NADH) ratio and suppressed SIRT1 activity. Activating SIRT1 with its activator SRT1720 attenuated senescence through an autophagy-dependent pathway. The NAD+ precursor nicotinamide mononucleotide and the poly ADP-ribose polymerase (PARP1) inhibitor olaparib also exerted anti-senescent effects by activating SIRT1. Moreover, the results showed that mitoQ and RAPA, in turn, elevated SIRT1 activity by inhibiting DNA damage. Consistent with these results, SRT1720 and mitoQ mitigated CS-induced AT2 cells senescence and lung fibrosis in vivo. Moreover, autophagy in AT2 cells was rescued by SRT1720. Taken together, our results suggested that CS-induced senescence of AT2 cells was due to decreased autophagy mediated by SIRT1 inactivation, which was attributed to competitive consumption of NAD+ caused by DNA damage-induced PARP1 activation. The reduction in autophagy, in turn, decreased SIRT1 activity by promoting mitochondrial oxidative stress-related DNA damage, thereby establishing a positive feedback loop between SIRT1 and autophagy in CS-induced AT2 cells senescence. Consequently, CS-inactivated SIRT1 promoted autophagy-dependent senescence of AT2 cells to induce pulmonary fibrosis.","Idiopathic pulmonary fibrosis (IPF) is a fatal age-related disease with unknown etiology. Cellular senescence, a state of irreversible cell cycle arrest, is one of the hallmarks of aging. In contrast to other forms of growth arrest, senescent cells are hyporeplicative, but metabolically active. Alveolar epithelial type 2 (AT2) cells are one of the main senescent cells in IPF lungs, and its senescence can initiate abnormal communication with other cells and with itself, and eventually result in fibrosis. Studies have demonstrated that lung fibrosis can be mitigated by senescent AT2 cells depletion, but is exacerbated by senescence induction. This emerging evidence indicates that inhibiting AT2 cells senescence may be an effective therapeutic strategy for IPF.

Cigarette smoke (CS) is a strong risk factor for the development of IPF and is implicated in the senescence of various cells, including AT2 cells. However, its profibrotic and prosenescent mechanism remain largely unexplored. Autophagy is an evolutionarily conserved catabolic process during which cargo can be degraded or recycled by lysosomes. Our previous study confirmed that impaired autophagy contributes to CS-induced lung fibrosis. Moreover, its relationship with senescence is a basic concern in the field of aging. Abnormal autophagy over time has been reported in different species and in age-related diseases. It is also involved in CS induced senescence of bronchial epithelial cells. However, the role of autophagy in senescence is still inconclusive and contradictory. Whether and how autophagy is involved in CS-induced AT2 cells senescence remains unclear.

SIRT1 is a nicotinamide adenine dinucleotide (NAD+)-dependent deacetylase. Studies have found that aging can decrease SIRT1 and thereby exacerbate liver and renal fibrosis. SIRT1 is also an autophagy modulator. It can interact with and deacetylate Atg5, Atg7 and Atg8/LC3, and subsequently regulate the autophagy process. In addition, histone H4 lysine 16 (H4K16) is the primary histone target of SIRT1 and its acetylation level is negatively associated with the expression of autophagy-related genes, indicating that SIRT1 can modulate autophagy at both the transcriptional and posttranslational levels. Moreover, the anti-senescent effect of SIRT1 is associated with autophagy. Although a reduction in SIRT1 is involved in CS-induced senescence, it is still not clear whether CS-dysregulated SIRT1 promotes AT2 cells senescence via an autophagy-dependent pathway. The effects of CS on SIRT1 also need further investigation.

In the present study, we explored the mechanism underlying CS-induced AT2 cells senescence and lung fibrosis. We demonstrated that CS-induced AT2 cells senescence was linked to reduced autophagy mediated by SIRT1 inactivation, which resulted from competitive consumption of NAD+ in response to DNA damage-induced poly ADP-ribose polymerase (PARP1) activation. Decreased autophagy in turn inhibited SIRT1 activity by inducing mitochondrial oxidative stress and subsequent DNA damage, and therefore a positive loop was established between SIRT1 activity and autophagy in CS-induced AT2 cells senescence and lung fibrosis. Taken together, these data indicated that CS-inactivated SIRT1 promotes autophagy-dependent senescence of AT2 cells to induce pulmonary fibrosis.","There are some limitations in the present study that need further investigation. For example, these findings were not verified in patients. The reasons for the contradictions between the present study and previous studies are not clear. In vivo, we only elucidated the role of SIRT1 activator and mitoROS scavenger, while the effects of other factors, such as NAD+ salvage and autophagy inducer, have not been confirmed.

Taken together, these data demonstrated that CS-induced PARP1 activation in response to DNA damage competitively consumed NAD+, resulting in SIRT1 inactivation that contributed to AT2 cells senescence by inhibiting autophagy. Decreased autophagy, in turn, inhibited SIRT1 activity by leading to mitochondrial oxidative stress-related DNA damage, and thus, a positive feedback loop between SIRT1 and autophagy was formed. Consequently, CS-inactivated SIRT1 promoted autophagy-dependent AT2 cells senescence to induce pulmonary fibrosis. Rational targeting these events may have therapeutic potential in pulmonary fibrosis.",https://www.sciencedirect.com/science/article/pii/S0891584921000939?via%3Dihub,
Protective effects of Quercetin and Resveratrol on aging markers in kidney under high glucose condition: in vivo and in vitro analysis,Resveratrol and quercetin reduce the levels of the toxic molecule methylglyoxal and protect rat kidneys from the effects of diabetes,"Resveratrol and Quercetin as powerful herbal antioxidant reduce kidney complications of diabetes. The purpose of this study was to evaluate the protective effects of Quercetin and Resveratrol on aging markers in high glucose condition.

In this study, HEK-293 cells and kidney tissue of male Wistar rats were used in hyperglycemia condition. Quercetin and Resveratrol for cell culture and in vivo evaluation were used. Amount of methylglyoxal and the gene and protein expression of the Senescence Marker Protein30 (SMP30) were measured using qRT-PCR and ELISA, respectively. High glucose condition in kidney cell and tissue caused increase in glyoxal amount compared to control sample. Treatment with Resveratrol and/or Quercetin significantly decreased the amount of methylglyoxal in a dose-dependent manner. SMP30 expression significantly decreased compare to control group while treatment with Resveratrol and/or Quercetin considerably increased expression level of the protein. Six weeks treatment with Quercetin and Resveratrol had more beneficial effects on aging markers compared with three weeks.

According to the results, it can be concluded that resveratrol and quercetin, as potent herbal antioxidants, may have significant effects in reducing the complications of diabetes in kidneys, as well as preventing the aging of the kidney cells.","Diabetes mellitus (DM) is one of the most common endocrine disorders affecting more than 100 million people annually, the seventh known cause of death. Most people are unaware of the onset of their illness and find that when the body is unable to control blood sugar and the disease has progressed, deficiency or a relative decrease in insulin levels is associated with acute and chronic metabolic complications. Aging is programmed biological process activated in response to various types of stress that includes telomere uncapping, oxidative stress, oncogene activity, and so on. Once cells enter senescence period, they undergo a series of morphologic and metabolic changes. There are enzymatic and non-enzymatic defense systems in the body to eliminate or neutralize free radicals, but oxidative stress occurs if the production of free radicals exceeds antioxidant capacity and the balance between production and consumption of ROS is lost, which can be considered as a factor in the development of diabetes. Oxygen-free radicals are produced as a result of natural metabolism in the body but increase in stress and physical conditions, which can cause permanent damage to the body’s macromolecules, such as proteins, lipids and DNA, through oxidative stress. Non-enzymatic glycation of proteins is one of the factors that can cause kidney damage due to high concentration of glucose. Non-enzymatic glycation results in increased levels of advanced glycation endproducts (AGEs). In hyperglycemic conditions, the intracellular forms of alphacetoaldehyde include methylglyoxalate (MG) as the main intracellular source of AGE and the abnormal accumulation of MG is associated with the progression of diabetes complications in tissues and organs. According to studies, glycation can be effective in the process of kidney cell aging. SMP30 is anti-aging marker protein is a 34 kDa cytosolic protein that plays an important role in calcium homeostasis, oxidative stress, and ascorbic acid biosynthesis. The marker was extracted from rat liver in 1992 and identified as an anti-aging factor that decreases expression of this protein with age. Resveratrol (3,5,4′-trihydroxystilbene) is phenol hydroxyl which is naturally found in many parts of plants and compounds including grapes and seeds. They have proven the role of resveratrol (R) in the prevention of various diseases including diabetes, neurodegenerative disorders, cancer and kidney disease.

Quercetin (3,3′,4′,5,7-pentahydroxyflavone) is a five-hydroxyl group of flavonoids. This compound has hydroxy radical scavenging activity [7]. The aim of this study was to evaluate the progression of aging of human embryonic kidney (HEK-293) cells in high glucose medium using cellular markers and to investigate the protective and antioxidant effect of R and Q in this cell line. Also, immunohistochemical analysis was used to evaluate kidney tissue aging in diabetic rats.","According to the results of this study, it can be concluded that high glucose concentration can induce aging in human kidney cell line and also in diabetic mice induce aging in kidney mice. Evaluation of aging markers in cell lines as well as tissue samples showed that potent antioxidants of R and Q in a dose- and time-dependent behavior could be effective in preventing and controlling the aging process. It can also be used to promote the use of herbal ingredients in the pharmacology industry as a complementary drug to improve and treat stress caused by high concentrations of glucose in diabetic patients to prevent the progression of kidney cell aging.",https://pubmed.ncbi.nlm.nih.gov/34273031/,
Effect of resveratrol on inflammatory cytokines: A meta-analysis of randomized controlled trials,"In human trials, resveratrol decreases inflammatory molecules TNF-α and IL-6 when dosed greater than 500 mg/d ","The aim of the current study was to perform a meta-analysis of randomized clinical trials regarding the effect of resveratrol in decreasing the levels of inflammatory cytokines, including interleukin (IL)-1, IL-6, IL-8, and tumor necrosis factor (TNF)-α in a combination of inflammatory diseases. Literature search was carried out in Scopus, ISI web of science, Medline, and Cochrane Library databases by up to September 2020. The pooled effect size was determined through measuring the weighted mean differences (WMD) and their corresponding 95% confidence intervals (CI) for the difference between the resveratrol-receiving and control groups. Finally, 33 publications, including 3 studies on IL-1, 26 studies on IL-6, 4 studies on IL-8, and 21 studies on TNF-α met our final inclusion criteria and included in the quantitative analysis. Analysis in the overall population showed a significant effect of resveratrol consumption in reducing serum TNF-α levels (WMD = −0.66 pg/ml, 95% CI = −1.05 to −0.27, P = 0.001). A significant reduction of IL-6 concentration was observed only in the patients receiving ≥500 mg/day dose of resveratrol (WMD = −1.89 pg/ml, 95% CI = −3.73 to −0.05, P = 0.04) with inter-study heterogeneity (I2 = 94.4%, P < 0.001). Nonetheless, no significant alteration was observed in IL-1 (WMD = −0.14 pg/ml, 95% CI = −0.31 to 0.03, P = 0.10) and IL-8 (WMD = 0.18 pg/ml, 95% CI = −1.04 to 1.40, P = 0.73) levels following resveratrol consumption. Based on the present findings, resveratrol is able to decrease TNF-α and IL-6 (in ≥500 mg/day dose) levels but not IL-1 and IL-8 levels.

","Resveratrol (3,5,4′-trihydroxy-trans-stilbene) is a phytoalexin pro-
duced by various plants and is found abundantly in red grapes skin and
peanuts. It is commonly utilized as a dietary supplement to improve the
metabolic disorders (Hunter and Hegele, 2017). Beneficial effects of
resveratrol are antioxidation, stimulating the generation of endothelial
nitride oxide, declining lipid levels, preventing the platelet aggregation,
and repressing the vascular inflammation (Borghi and Cicero, 2017;
Cicero et al., 2017; Li et al., 2012). Such advantageous characteristics of
resveratrol have been associated with medical applications in hyper-
tension (Csiszar et al., 2009), type 2 diabetes (T2D) (Zhang et al., 2010),
cardiovascular diseases (Fan et al., 2008), ischemic stroke (Weiner and
Ducruet, 2014), atrial fibrillation (Baczko et al., 2014), heart failure
(Baczko et al., 2014), hepatic steatosis (Aguirre et al., 2014; Andrade
et al., 2014), cancer (Carter et al., 2014), and metabolic syndrome (Mts). Studies have revealed that resveratrol can downmodulate the in-
flammatory profile through suppression of production of inflammatory
mediators, and inhibition of activator protein-1 (AP-1) that leads to
interruption of eicosanoid synthesis and immune cells modulation (Das
and Das, 2007). Additionally, resveratrol was shown to downregulate
the transcription of inducible nitric oxide synthase (iNOS) and cyclo-
oxygenase (COX)-2 in tumor cells and macrophages (Martinez and
Moreno, 2000). Resveratrol is able to reduce the Peroxisome
proliferator-activated receptor (PPAR)-γ coactivator 1-alpha (PGC-1α)
acetylation thorough suppression of nicotinamide adenine
dinucleotide-dependent deacetylase sirtuin 1 (SIRT1), leading to
downmodulation of oxidized low-density lipoprotein (ox-LDL) levels
(Chow et al., 2007; Zarzuelo et al., 2013). According to trials, resveratrol
is involved in the suppression of nuclear factor (NF)-κB and
Mitogen-activated protein kinase (MAPK), resulting in decreased levels
of inflammatory cytokines (Kang et al., 2009). Furthermore, resveratrol
was shown to modulate the mRNA binding activity of KH-type splicing
regulatory protein (KSRP), leading to promotion of mRNA degradation
of inflammatory mediators (Bollmann et al., 2014).
A trial on resveratrol reported that nonalcoholic fatty liver disease
(NAFLD) patients receiving a dose of 500 mg/day resveratrol for 12
weeks manifested a remarkable reduction in the inflammatory bio-
markers by downregulation of NF-κB expression (Faghihzadeh et al.,
2014). That notwithstanding, there are also evidence that resveratrol
consumption did not have beneficial effects in the individuals with
obesity (Poulsen et al., 2013a; Yoshino et al., 2012). Incongruous out-
comes of resveratrol supplementation may underlie diversity in the
dosage and formulation of resveratrol, supplementation duration, study
design and sample sizes, and geoepidemiology of the study populations.
Hence, here we tried to perform a systematic review and meta-analysis
on the RCTs evaluation the effects of resveratrol supplementation on the
inflammatory mediators, including interleukin (IL)-1, IL-6, IL-8, and
tumor necrosis factor (TNF)-α in a combination of pooled inflammatory
diseases.","Here, we performed an updated meta-analysis to clarify the bona fide
role of resveratrol supplementation in decreasing the inflammatory
markers IL-6 and TNF-α and a first analysis on the IL-1 and IL-8 levels.
We observed the beneficial effect of resveratrol consumption on the
reduction of TNF-α levels. In addition, a significant decrease in IL-6 level
was observed in the patients receiving ≥500 mg/day dose of resveratrol.
However, resveratrol supplementation did not alter the levels of IL-1 and
IL-8. Further studies are needed to obtain an authentic conclusion on the
effect of resveratrol supplementation on inflammatory markers, partic-
ularly IL-1 and IL-8.",https://www.sciencedirect.com/science/article/pii/S0014299921005331?via%3Dihub,
"The role of metformin, statins and diet in men on active surveillance for prostate cancer","Metformin for diabetes/aging, statins for cholesterol, and finasteride for hair loss, reduce the risk of dying from prostate cancer","A sound scientific basis has been emerging on the anti-neoplastic role of metformin, statins and dietary interventions. However, evidence in prostate cancer patients remains mixed owing to an absence of completed randomized trials. This overview examines the rationale for metformin, statins and dietary intervention for secondary prevention in men on active surveillance by summarizing current evidence base and biological mechanisms in influencing cancer progression and mortality.

A comprehensive literature search was performed to identify studies that evaluated the role of metformin, statins and diet in the secondary prevention of prostate cancer as well as those that described the anti-cancer mechanisms of these agents. The search included Pubmed, MEDLINE, EMBASE and Cochrane library from inception till August 2021.

A total of 14 trials on metformin, 21 trials on statins and 13 trials on dietary measures were evaluated. Majority were observational population-based cohort studies or meta-analysis of them. Three ongoing prospective randomized controlled trials were also reported. Overall, mixed results were obtained.

The role of metformin and statins remains promising with several trials showing reduced rates of progression and cancer specific mortality. Combination therapy strategies have also been evaluated in more advanced patients showing synergism. Dietary interventions especially fruits, vegetables and fish intake has shown some benefit albeit with mixed results for others like legumes, red meat, coffee and multivitamins. Several ongoing randomized trials will provide stronger evidence in the future for secondary prevention.","The last 2 decades has seen a significant shift in prostate cancer diagnosis and staging from disease that presented late to one that is diagnosed earlier, in a pre-clinical stage. Active surveillance regimes have grown in prominence for this group having shown comparable survival results when compared to radical treatment modalities which do carry a risk of adverse events that can negatively affect the patient’s quality of life. Active surveillance cohorts with at least 5 years of available data have shown low rates of metastatic disease at 0.1–2.8% and prostate cancer specific death at 0–1.5%. This long latency of disease, older age of onset, and increasing prevalence makes patients with prostate cancer on active surveillance an ideal target for primary and secondary prevention strategies. Secondly, radical treatment has been pursued in 24–40% of these patients who progressed, indicating a significant proportion who would directly benefit from secondary prevention strategies. Thirdly, secondary prevention methods could also be an attractive strategy which could facilitate relaxing of active surveillance regime intensity in patients deemed at lower risk of progression. Despite this, we currently still lack any evidence-based recommendations for secondary prevention in active surveillance. We have summarized currently ongoing and previously published trials in the secondary prevention of prostate cancer. In tandem with increasing incidence of prostate cancer we are seeing a worldwide epidemic of diabetes mellitus and hyperlipidemia complicated by poor dietary habits and obesity. We sought to review how medications like metformin, statins as well as diet may secondarily prevent prostate cancer progression in low-risk prostate cancer patients on active surveillance.","Increasingly, metformin, statins and dietary strategies have shown sound potential anti-cancer properties based on basic science research. However, the evidence for them in secondary prevention of men on active surveillance remains mixed owing to absence of randomized control trials in this space. It would be premature to relax currently active surveillance regimes purely on the basis of these secondary prevention agents. Ongoing trials should shed light on this important space as more prostate cancer patients are choosing active surveillance.",https://link.springer.com/article/10.1007/s00345-021-03858-4#Sec19,
Time-Restricted Eating Effects on Body Composition and Metabolic Measures in Humans who are Overweight:  A Feasibility Study,This TRE IntermittentFasting study on mostly women participants showed 8-h TRE can lead to weight loss and other health improvements,"Objective: In contrast to intentionally restricting energy intake, restricting the eating window may be an option for treating obesity. By comparing time-restricted eating (TRE) with an unrestricted (non-TRE) control, it was hypothesized  that  TRE  facilitates  weight  loss,  alters  body  composition,  and improves metabolic measures.Methods: Participants  (17  women  and  3  men;  mean  [SD]:  45.5  [12.1]  years; BMI 34.1 [7.5] kg/m2) with a prolonged eating window (15.4 [0.9] hours) were randomized to TRE (n=   11: 8-hour window, unrestricted eating within window) versus non-TRE (n=  9: unrestricted eating) for 12 weeks. Weight, body composition (dual x-ray absorptiometry), lipids, blood pres-sure, 2-hour oral glucose tolerance, 2-week continuous glucose monitor-ing,  and  2-week  physical  activity  (actigraphy  assessed)  were  measured  during the pre- and end-intervention periods.Results: The TRE group significantly reduced the eating window (end-intervention window: 9.9 [2.0] hours) compared with the non-TRE group (end-intervention window: 15.1 [1.1] hours) (P < 0.01).  Compared  with non-TRE,  TRE  decreased  the  number  of  eating  occasions,  weight,  lean mass, and visceral fat (all P ≤  0.05). Compared with preinterven-tion  measures,  the  TRE  group  reduced  the  number  of  eating  occa-sions (−21.9% [30.1%]) and reduced weight (−3.7% [1.8%]), fat mass (−4%  [2.9%]),  lean  mass  (−3.0%  [2.7%]),  and  visceral  fat  (−11.1%  [13.4%])  (all  P  ≤   0.05).  Physical  activity  and  metabolic  measures  re-mained unchanged.Conclusions: In the setting of a randomized trial, TRE presents a simpli-fied view of food intake that reduces weight","Obesity affects 38% of the US adult population and presents a significant health care burden.  Intentional  energy  restriction  plays  an  important  role  in  obesity  treat-ment.  However,  intentional  energy  restriction  is  hampered  by  lack  of  patient  adher-ence. Therefore, obesity treatment options beyond intentional energy restriction are needed. 2020861In contrast to intentionally restricting energy intake, time-restricted eat-ing (TRE) presents a simplified approach toward eating by restricting the eating window. Because adult humans (n = 156)  have  a documented median  daily  eating  window  of  14.75  hours, there  is  interest  in  the  implications  of  restricting  the  eating  window  to  10  hours  or  less.  Human TRE studies providing all food to maintain isoenergetic intake have  reported  weight  stability  with  variable  effects  on  glycemic  mea-sures. Human studies on TRE with ad libitum intake have reported modest weight loss (approximately 3%, 3 kg), although glyce-mic measures remained unchanged.The  findings  from  the  TRE  literature  have  been  promising.  However,  several features may limit generalizability. One limitation is the provi-sion  of  all  food,  reducing  applicability  to  the  real-world  setting.  Through administration of TRE with ad libitum intake as the primary intervention, studies have primarily compared outcomes with preinter-vention  values  or  with  outcomes  of  a  historical  control  or  assigned  group.  Although  some  studies  have  compared  TRE  with  ad libitum intake with non-TRE in a randomized setting, these studies were within the setting of resistance training, potentially mask-ing the effects of TRE. For  an  intervention,  the  gold  standard  reference  is  a  randomized  con-trol  group.  Therefore,  we  conducted  a  12-week  feasibility  study,  randomizing TRE (ad libitum intake during an 8-hour window) or non-TRE  (ad  libitum)  in  humans  with  overweight  or  obesity  with  a  doc-umented  eating  window  ≥  14  hours.  We  hypothesized  that  TRE  with  ad  libitum  intake  would  facilitate  weight  loss,  reduce  body  fat,  and  improve metabolic measures compared with non-TRE.","We compared TRE with ad libitum intake with non-TRE (randomized control)  in  humans  with  overweight  or  obesity  who  had  an  eating  window  ≥  14  hours.  We  found  that  TRE,  compared  with  non-TRE,  resulted  in  fewer  EO  and  greater  body  weight  loss,  lean  mass  loss,  and  visceral  fat  loss  relative  to  preintervention  values.  TRE  also  reduced  fasting  glucose  and  triglyceride  concentrations  relative  to  preintervention values, although this was not significant when com-pared with the non-TRE group. Within the TRE group, greater eat-ing-window restriction was associated with greater loss of fat mass and visceral fat. We reported that both TRE and non-TRE groups had reduced EO, with greater  reductions  observed  in  the  TRE  group.  These  findings  suggest  that TRE may cause involuntary reductions of energy intake via reduced EO. Previously, dietary records have been used by studies on TRE with ad  libitum  intake  to  document  reduced  energy  intake  (approximately  350-650  kcal/d)  (8,10).  Alternatively,  reduced  energy  intake  has  been  estimated from an image or text entry using reference nutritional values without considering the food volume (3). As we could not ascertain food volume  or  energy  density  from  the  mCC-based  images,  we  could  not  assess TRE effects on energy intake. We acknowledge that our observed reduction  in  EO  may  be  influenced  by  reduced  logging.  Nevertheless,  our significant findings for the TRE group, compared with those for the randomized non-TRE group, offset this concern, assuming that both the TRE and non-TRE groups had similar reductions in logging. Moving for-ward, in future studies on TRE with ad libitum intake, researchers should consider energy restriction as a comparison group.We  used  DXA  to  document  TRE  alterations  in  body  composition.  We  observed  a  significant  loss  of  fat  mass  and  lean  mass,  which  is  greater  than  the  technical  error  of  a  DXA  measurement  (approxi-mately  1%-3%)  (23).  Both  dietary  restriction  (24-26)  and  bariatric  surgery (27,28) are associated with lean mass loss. This loss has been attributed to alterations in protein intake or protein turnover (24,29), which can be mitigated either by high-protein diets (25,30,31) or an exercise intervention (32,33). Indeed, studies incorporating TRE with resistance training and either unrestricted intake (10), TRE with slight caloric  restriction  (200-250  kcal/d)  and  randomized  whey  protein  supplementation (11), or TRE with instructions to maintain isoener-getic intake (34) have demonstrated preserved or increased lean mass.Although TRE was associated with lower fasting glucose levels, greater time within the CGMS target range, and lower fasting triglyceride con-centrations  relative  to  preintervention  values,  these  findings  were  not  significant  when  compared  with  non-TRE.  The  literature  has  reported  TRE having varying effects on metabolic measures. In a healthy, normal weight  population,  isoenergetic  TRE  was  associated  with  higher  fast-ing  glucose  levels  and  greater  impairment  of  glucose  tolerance  (4).  In  participants  with  obesity  (8)  or  the  metabolic  syndrome  (9),  TRE  with  ad libitum intake did not alter fasting glucose levels, fasting triglyceride levels,  or  HOMA-IR.  In  men  with  prediabetes,  isoenergetic  TRE  with  completion of all food intake by 3:00 pm improved insulin sensitivity (5).The  study  has  several  strengths.  TRE  with  ad  libitum  intake  altered  weight  and  body  composition,  compared  with  non-TRE  in  a  random-ized  group,  despite  the  small  sample  size  (n=   20),  short  duration  (12  weeks),  and  achieved  eating-window  restriction  (approximately  10  hours  despite  the  instructed  8-hour  window).  Because  all  participants  documented intake using the mCC application, we were able to monitor intervention compliance and use the non-TRE referent group to account for  mCC  application  use.  The  logging  also  documented  the  extent  of  eating-window  restriction,  which  we  correlated  with  observed  out-comes. Lastly, we performed detailed clinical phenotyping (body com-position [DXA], physical activity [actigraphy], and glycemic measures [CGMS, OGTT, and HbA1c]) to assess effects of TRE.Several  study  limitations  exist.  Our  reliance  on  the  mCC  application  assumed consistent use. Therefore, we enrolled only participants who displayed  high  logging  adherence  during  the  preintervention  period.  Although  logging  adherence  was  lower  in  the  TRE  group  than  in  the  non-TRE group, this difference was not statistically significant. Another study limitation is the use of EO as a surrogate for energy intake because food volume could not be ascertained. Another limitation is our inclu-sion of participants with an eating window ≥ 14 hours. Therefore, these findings may be attenuated and less applicable to humans with a shorter eating window. Because all participants self-selected an 8-hour eating window  to  include  the  evening  meal,  we  were  unable  to  address  the  contribution of TRE timing (early vs. late eating) (5,35) or the effect on circadian clock genes (35). Lastly, we acknowledge that the short time frame  (12  weeks),  the  preponderance  of  women,  and  the  small  sam-ple size may potentially limit translation. Nevertheless, our significant results in relevant measures (body weight, body composition, and EO) within the context of a randomized intervention provide important data to inform the design of future TRE studies.",https://onlinelibrary.wiley.com/doi/epdf/10.1002/oby.22756,
Sex- and age-dependent outcomes of 9-hour time-restricted feeding of a Western high-fat high-sucrose diet in C57BL/6J mice,"Female mice on 9h-TRF still gained as much weight as the female mice that ate anytime. But

Just like male TRF mice, female TRF mice also showed improved glucose regulation. 

Similar to humans, TRF may not cause weightloss, but improves blood glucose.","Time-restricted feeding (TRF) is a nutritional intervention wherein food intake is limited to a consistent 8- to
10-h daily window without changes in nutritional quality or quantity. TRF can prevent and treat diet-induced
obesity (DIO) and associated metabolic disease in young male mice fed an obesogenic diet, the gold standard preclinical model for metabolic disease research. Because age and sex are key biological variables
affecting metabolic disease pathophysiology and response to therapies, we assessed their impact on TRF
benefits by subjecting young 3-month-old or middle-aged 12-month-old male and female mice to ad libitum
or TRF of a Western diet. We show that most of the benefits of TRF are age-independent but are sex-dependent. TRF protects both sexes against fatty liver and glucose intolerance while body weight benefits are
observed only in males. We also find that TRF imparts performance benefits and increases survival to sepsis
in both sexes.","Sex and age modulate both the impact of an obesogenic diet on
metabolic health and the outcomes of interventions for metabolic diseases. Females are more likely to gain fat mass and
resist net fat loss. In fact, the global prevalence of obesity is
higher in women than in men (Kelly et al., 2008). Females have
more fat and less lean mass relative to males, yet for equivalent
adiposity, females are more insulin sensitive than males. Epidemiological and clinical studies have demonstrated major sex differences in the prevalence of metabolic disorders, with women
being protected from cardiovascular disease (CVD) relative to
men before menopause. Despite this lower risk, in the last 20
years, CVD risk has increased in pre-menopausal women, which
has been attributed to an increase in the prevalence of diabetes
that can offset this sexual dimorphism (Regensteiner et al.,
2015). Once females develop metabolic diseases, such as fatty
liver disease, they are prone to progress faster to more severe
forms. Age is also a major biological variable in metabolic health.
There is an increased tendency to accumulate adiposity in middle age, which alone or in combination with older age elevates
the risk for metabolic diseases.
Although age and sex significantly modulate metabolic health,
most preclinical metabolism research is done on young male
mice. This is partly due to earlier observations that young female
mice of the widely used C57BL/6 strain fed a high-fat diet (HFD)
are less prone to diet-induced obesity (DIO) and associated
metabolic diseases than are male mice. The female mice are
leaner, do not develop hyperinsulinemia, and display less adipose tissue inflammation (Grove et al., 2010; Pettersson et al.,
2012) than do the male mice. Estrogen depletion experiments
(ovariectomized females or post-menopausal models) (Hong
et al., 2009; Riant et al., 2009; Zhu et al., 2013) and estrogen
replacement (E2 treatment) (Riant et al., 2009) strongly support
the idea that estrogen signaling underlies sexual dimorphism in
metabolic and inflammatory responses between males and females. Recent studies have shown that C57BL/6J (B6) female
mice fed a HFD become glucose intolerant, and long-term
feeding of a HFD leads to similar cardiometabolic dysfunction
in both male and female C57BL/6J mice (Bruder-Nascimento
et al., 2017).
Time-restricted feeding (TRF) is a nutritional intervention strategy in which mice are fed within a consistent 8- to 12-h interval
daily. A TRF cohort typically consumes isocaloric food as their
ad libitum-fed (ALF) counterparts and yet exhibit significant
reduction in weight gain and metabolic diseases that are found
in the ALF cohort (Chaix et al., 2014; Delahaye et al., 2018; Duncan et al., 2016; Hatori et al., 2012; Sherman et al., 2012; Sundaram and Yan, 2016; Woodie et al., 2018). TRF impacts multiple
metabolic organs including but not limited to liver, muscle, and
adipose tissues. In addition, lifelong TRF on a normal chow
diet can increase longevity in male mice (Mitchell et al., 2019).
This raises the possibility that TRF can be beneficial in females
as well. Accordingly, recent studies showed that TRF can (1)
delay mammary tumor progression in HFD-fed female mice
(Sundaram and Yan, 2018) and (2) provide protection from DIO, fatty liver, and insulin resistance in postmenopausal or
ovariectomized C57BL/6J mice (Chung et al., 2016; Omotola
et al., 2019). An experimental paradigm in which TRF and ALF
cohorts consume isocaloric diet offers a clinically relevant model
to assess both the impact of an obesogenic diet on disease and
an isocaloric intervention on both sexes and at different ages.
Recently, a series of pilot human studies have shown that
time-restricted eating (TRE) can be an effective behavioral intervention to reduce the burden of metabolic diseases, yet these
smaller studies are not powered enough to identify the effects
of age or sex on clinical outcomes (Gabel et al., 2018; Gill and
Panda, 2015; Sutton et al., 2018; Wilkinson et al., 2019).
In this study, we tested the effects of 3 months of ALF or TRF of
a Western diet (WD) on body composition, metabolic health, and
overall fitness in young and middle-aged male and female mice.
Extensive metabolic phenotyping as well as performance and
cognitive assays were conducted in parallel in young (3 months
old) and middle-aged (12 months old) male and female C57BL/
6J mice. We found in male mice that TRF prevented weight
gain and improved health relative to ALF at both ages. Although
female mice under TRF were not protected from weight gain,
they showed several health benefits relative to their ALF counterparts and were even protected from lipopolysaccharide (LPS)-
induced lethality. Thus, TRF can delay metabolic dysfunction
and promote healthy aging in middle-aged pre-clinical male
and female mouse models of metabolic disease.","Our study provides a parallel systematic evaluation of the metabolic and performance effect of 3 months of Western diet
feeding in young and middle-aged male and female C57BL/6J
mice in standard ad libitum feeding conditions and under TRF
dietary intervention. The age of the mice, precise composition
of the diet, and location of the animal study critically contribute
to metabolic parameters (Corrigan et al., 2020). In this study, all
groups were analyzed in the same facility with the same
research staff, with a sole source for the diet (Research Diet
D12451) and the mice (The Jackson Laboratory), and will thus
serve as a fundamental resource for researchers across the nutrition, exercise, metabolic, and circadian research
community.
There are several key conclusions from this study. In agreement with previous studies, we show a sexually dimorphic
response to Western diet feeding between males and females,
with reduced insulin levels and adipose tissue inflammation in females. Nevertheless, females, especially as they aged, were still
glucose intolerant and highly sensitive to fatty liver. Dietary intervention such as TRF was able to protect the female mice from
these metabolic impairments. Alternatively, no benefits of TRF
were seen in performance and behavior assays in females, suggesting that additional interventions might be necessary to
improve these health outcomes in females. In males, TRF
improved all metabolic and performance parameters tested,
regardless of age, suggesting that TRF could improve healthspan and lifespan even with Western diet feeding. Future studies
of lifelong TRF will test this hypothesis. Finally, we show that
being on TRF can increase survival to a septic challenge in middle-aged male mice, with a trend in females. This is especially
relevant in the context of the COVID-19 pandemic during which
this paper was written since poor metabolic health is the major
risk factor for severe COVID-19 (Ayres, 2020; Bornstein et al.,
2020). Future studies will determine whether TRF could also
improve metabolic health complications that have been
observed in survivors of previous SARS-CoV-2 infections.",,
"A decrease in NAD+ contributes to the loss of osteoprogenitors and bone mass with aging
","NAD supplementation through NR or NMN may help sustain bone mass in old age (in mice). The paper complements several other beneficial effects of boosting NAD to reduce the burden of aging. 
","Age-related osteoporosis is caused by a deficit in osteoblasts, the cells that secrete bone matrix. The number of osteoblast progenitors also declines with age associated with increased markers of cell senescence. The forkhead box O (FoxO) transcription factors attenuate Wnt/β-catenin signaling and the proliferation of osteoprogenitors, thereby decreasing bone formation. The NAD+-dependent Sirtuin1 (Sirt1) deacetylates FoxOs and β-catenin in osteoblast progenitors and, thereby, increases bone mass. However, it remains unknown whether the Sirt1/FoxO/β-catenin pathway is dysregulated with age in osteoblast progenitors. We found decreased levels of NAD+ in osteoblast progenitor cultures from old mice, associated with increased acetylation of FoxO1 and markers of cell senescence. The NAD+ precursor nicotinamide riboside (NR) abrogated FoxO1 and β-catenin acetylation and several marker of cellular senescence, and increased the osteoblastogenic capacity of cells from old mice. Consistent with these effects, NR administration to C57BL/6 mice counteracted the loss of bone mass with aging. Attenuation of NAD+ levels in osteoprogenitor cultures from young mice inhibited osteoblastogenesis in a FoxO-dependent manner. In addition, mice with decreased NAD+ in cells of the osteoblast lineage lost bone mass at a young age. Together, these findings suggest that the decrease in bone formation with old age is due, at least in part, to a decrease in NAD+ and dysregulated Sirt1/FoxO/β-catenin pathway in osteoblast progenitors. NAD+ repletion, therefore, represents a rational therapeutic approach to skeletal involution.","Loss of bone mass is a major cause for the occurrence of osteoporotic fractures in the aged population. It is well documented that trabecular bone mass and cortical thickness decrease and intracortical porosity increases with age, with particular incidence in women. The integrity of bone is maintained by a process known as bone remodeling in which teams of osteoclasts—giant multinucleated cells of the hematopoietic lineage—resorb the bone matrix which is then replaced by teams of osteoblasts—cells of mesenchymal lineage that secrete bone matrix. With aging the number of osteoblasts slowly declines leading to unbalanced remodeling and loss of bone mass8. Much similar to humans, mice lose trabecular and cortical bone mass with age due to unbalanced remodeling and develop cortical porosity. The decrease in bone formation with age is associated with lower osteoprogenitor numbers; nonetheless, the cellular and molecular mechanisms responsible for skeletal aging remain unclear.

In several tissues the oxidized form of nicotinamide adenine dinucleotide (or NAD+), a co-enzyme that functions as an electron acceptor, decreases with aging. Importantly, NAD+ precursors such as nicotinamide mononucleotide (NMN) or nicotinamide riboside (NR) exert anti-aging effect in tissues like the eye, muscle, vasculature, and pancreas. NAD+ also increases longevity in different species such as worms, rats, and mice. Thus, a decline in NAD+ is a bona fide hallmark of aging. Most of the cellular NAD+ is recycled from nicotinamide NAM via the NAD salvage pathway. First, nicotinamide phosphoribosyl-transferase (NAMPT) converts NAM into NMN, which in turn leads to the production of NAD+ in a reaction catalyzed by nicotinamide mononucleotide adenylyl-transferase (NMNAT). Raising NAD+ levels prevents several age-associated events such as DNA damage, mitochondria dysfunction, cell senescence, and stem cell loss in different tissues in mice. Some of these effects are mediated via the NAD-dependent Sirtuin1 (Sirt1).

Genetic evidence from our group and others has shown that Sirt1 promotes bone formation and the accrual of normal bone mass via action in osteoprogenitors. Furthermore, long-term administration of Sirt1 activators to mice attenuates skeletal aging. In osteoblast progenitors, Sirt1 deacetylates FoxOs and β-catenin and, thereby, stimulates Wnt signaling and osteoblastogenesis. We have shown that an increase in mitochondrial reactive oxygen species (ROS) in cells of the mesenchymal lineage, including stem cells, osteoblast progenitors osteoblasts, and osteocytes, contribute to the decrease in bone formation with aging. ROS stimulate FoxOs and promote the binding of FoxOs to β-catenin and, thereby, decrease osteoblastogenesis. Importantly, FoxOs in osteoprogenitors are major inhibitors of bone formation26. These indirect lines of evidence support the idea that the Sirt1/FoxO/β-catenin pathway in osteoblast progenitors contributes to skeletal aging. However, whether this pathway is altered with aging remains unclear.

Here, we found that FoxO and β-catenin acetylation increase with age in osteoblast progenitors due to a decline in NAD+ levels. Using pharmacological and genetic tools to manipulate NAD+ levels, we provide data to suggest that a decline in NAD+ in osteoprogenitor cells contributes to skeletal aging.","Here we show that NAD+ supplementation by the NAD+ precursor NR can restore a youthful number of osteoprogenitor cells and attenuate skeletal aging in female mice. These, along with the findings that the levels of NAD+ decline with age in osteoblast progenitors, strongly suggest that NAD+ is a major target of aging in osteoblastic cells. A decrease in NAD+ was also seen in bone marrow stromal cells from 15-month-old when compared to 1-month-old mice. In agreement with our findings, long-term administration of NMN increased bone mineral density in male C57BL/6 mice. In contrast, administration of NMN to 12-month-old mice for only 3 months was not sufficient to alter bone mass32.

The decrease in NAD+ with age in osteoblast progenitors was associated with an increase in Cd38—the main nicotinamide nucleotidase in mammalian tissues. Cd38 is a multifunctional protein involved in the generation of the second messengers ADPR and cyclic-ADPR (cADPR) which promote intracellular calcium signaling. Due to its NADase activity Cd38 is also a major contributor to cellular and tissue NAD+ homeostasis35. Similar to our findings in osteoblast progenitors, the levels and activity of Cd38 increase with aging in liver, adipose tissue, spleen, and skeletal muscle. Importantly, genetic or pharmacology inhibition of Cd38 in mice increases NAD+ levels in multiple organs and prevents the age-related NAD+ decline, attenuates mitochondrial dysfunction, and improves glucose tolerance, cardiac function, and exercise capacity. In multiple cell types, inflammatory cytokines such as TNF promote Cd38 expression via activation of NF-kB. These, along with the findings that the expression of inflammatory cytokines increases with age in multiple bone cell populations and that NF-kB is stimulated in osteoprogenitors from aged mice, represent a potential explanation for the age-associated increase of Cd38 in osteoblast progenitors.

We also found that the protein levels of Nampt in osteoblastic cells from old mice were lower than in cells from young mice. These along with the findings that deletion of Nampt in mesenchymal lineage cells is sufficient to decrease bone mass support the premise that the age-associated decrease in NAD+ in osteoblast progenitors attenuates bone formation. Further support is provided by evidence that NR administration increases osteoprogenitor number and mineralizing surface in aging mice. In tissues such as muscle and intestine, progenitor cells are critical targets of the anti-aging effects of NR. Nonetheless, the systemic nature of NR treatment precludes definitive conclusion about the target cells responsible for the beneficial effects on the skeleton.

We and others have shown that osteoprogenitors from old humans or mice exhibit markers of cellular senescence. Elimination of senescent cells via genetic or pharmacologic manipulations increases bone mass in aged mice, suggesting that cellular senescence contributes to skeletal aging. Our present findings that NR administration decreases markers of senescence in osteoblast progenitors from old mice provide strong support for the contention that a decline in NAD+ is a major contributor to the age-associated bone cell senescence. This contention is further supported by evidence that a decrease in NAD+ exacerbates replicative senescence in bone marrow-derived stromal cell cultures. NR administration also attenuates cellular senescence in brain and skin of aged mice. Interestingly, in macrophages and endothelial cells Cd38 expression can be induced by factors associated with the SASP48, suggesting that cellular senescence re-enforces the decline in NAD+.

In most tissues, the downstream mechanisms mediating the beneficial effects of NMN and NR remain unclear. Here, we found that NR decreased the age-related acetylation of FoxOs and β-catenin. Furthermore, lowering NAD+ levels via pharmacological or genetic means strongly enhanced acetylation of these proteins. Acetylation of FoxOs increases their association with β-catenin and inhibits Wnt signaling and osteoblastogenesis. Sirt1 deacetylates FoxOs and β-catenin and promotes osteoblastogenesis. Our findings that osteoblastic cells from mice lacking FoxO1, 3, and 4 are partially protected from the effects of FK866 support the premise that Sirt1/FoxOs mediate the effects of NAD+ on osteoblastogenesis. Further support is provided by evidence that administration of Sirt1 stimulators to mice attenuates skeletal aging20,21, and that Sirt1 mediates some of the beneficial effects of NR in the liver, muscle, and gut. Interestingly, both Sirt1 and FoxOs have been linked to cellular senescence43,50. Therefore, it is possible that a decline in NAD+ with age contributes to osteoprogenitor senescence via Sirt1- and FoxO-dependent mechanisms. However, given the wide range of NAD+ targets and the complex interactions between NAD+-dependent processes, further work will be required to define the cellular and molecular targets of NAD+ in bone.

Our findings that heterozygous deletion of Nampt had no impact on bone development and growth but caused loss of bone mass in young adult mice suggest that a decrease in NAD+ in mesenchymal lineage cells caused accelerated skeletal aging. In C57BL/6 mice, femoral growth ceases at 6–7 months, and at about 12 months, which is roughly equivalent to 40 years in humans, the marrow space begins to expand and additional bone is slowly added to the periosteum (outer bone surface); however, the former exceeds the latter, leading to a thinner and more fragile cortex. Interestingly, Nampt deletion replicated the effects of aging on cortical bone characterized by cortical thinning associated with the expansion of both medullary and total area. The apposition of bone in the periosteum that occurs with advancing age might be compensatory response to the enlargement of medullary cavity in an effort to maintain bone strength. Whether this is the case in Namptfl/+;ΔPrx1 mice requires future studies.

Based on the results of the present work, we propose that intrinsic defects in osteoblast progenitors that cause a decrease in NAD+ contribute to the age-related decline in bone formation and bone mass. Repletion of NAD+ with precursors such as NR, therefore, may represent a therapeutic approach to age-associated osteoporosis as it does for other age-related pathologies.
",https://www.nature.com/articles/s41514-021-00058-7#Sec9,
Pediatric BMI changes during COVID-19 pandemic: An electronic health record-based retrospective cohort study,"This study mined ~100,000 medical records of children and found COVID pandemic related disruptions had disproportionately increased weight-gain among younger children. ","Beginning March 2020, the COVID-19 pandemic has disrupted different aspects of life. The impact on children's rate of weight gain has not been analysed.
In this retrospective cohort study, we used United States (US) Electronic Health Record (EHR) data from Optum® to calculate the age- and sex- adjusted change in BMI (∆BMIadj) in individual 6-to-17-year-old children between two well child checks (WCCs). The mean of individual ∆BMIadj during 2017–2020 was calculated by month. For September-December WCCs, the mean of individual ∆BMIadj (overall and by subgroup) was reported for 2020 and 2017–2019, and the impact of 2020 vs 2017–2019 was tested by multivariable linear regression.
The mean [95% Confidence Interval - CI] ∆BMIadj in September-December of 2020 was 0·62 [0·59,0·64] kg/m2, compared to 0·31 [0·29, 0·32] kg/m2 in previous years. The increase was most prominent in children with pre-existing obesity (1·16 [1·07,1·24] kg/m2 in 2020 versus 0·56 [0·52,0·61] kg/m2 in previous years), Hispanic children (0·93 [0·84,1·02] kg/m2 in 2020 versus 0·41 [0·36,0·46] kg/m2 in previous years), and children who lack commercial insurance (0·88 [0·81,0·95] kg/m2 in 2020 compared to 0·43 [0·39,0·47] kg/m2 in previous years). ∆BMIadj accelerated most in ages 8–12 and least in ages 15–17.
Children's rate of unhealthy weight gain increased notably during the COVID-19 pandemic across demographic groups, and most prominently in children already vulnerable to unhealthy weight gain. This data can inform policy decisions critical to child development and health as the pandemic continues to unfold.","Since March 2020, the Coronavirus Disease 2019 (COVID-19) pan-
demic has disrupted all aspects of life, with children greatly impacted
by the sudden shift to lockdown restrictions which focused on school
closures or remote learning and the removal of peer-to-peer interac-
tive play. Numerous papers based on parent-reported lifestyle sur-
veys in many countries have described increased use of electronic
devices (i.e., screen time) and decreased physical activity since the
start of the COVID-19 pandemic [15]. These papers predict health
effects including an increase in obesity. Unhealthy weight gain in
childhood, in turn, carries long-term health implications, including
increased risk of obesity, hypertension, and diabetes in adulthood As the aforementioned literature raises a concern for increased child-
hood weight gain with consequences both immediate and reaching
into adulthood, it is important to determine the extent to which the
pandemic has increased the rate of weight gain in children. No papers
have yet compared rate of weight gain before and during the pan-
demic.
To fill that gap, we use routinely collected BMI data from preven-
tive care visits. From birth, children’s height, weight, and body mass
index (BMI) are consistently documented as part of routine pediatric
care, known among pediatricians in the United States (US) as Well
Child Checks (WCC). WCCs are attended by a high percentage of US
children [7] and are used to promote health and monitor the rate of
growth over time. Children, unlike adults, are expected to increase
BMI as part of healthy growth. However, children’s growth trajecto-
ries should not, on average, cross established BMI percentiles [8],
available in CDC age- and sex-specific growth charts [9]. BMI in children is typically categorized into underweight BMIs (< 5th per-
centile), healthy BMIs (between 5th and 85th percentile), overweight
BMIs (between 85th and 95th percentile), or with obesity BMIs (
95th percentile). WCC attendance in 2019 was above 90% across races
and poverty levels [9]; BMI data from WCCs therefore will not be sub-
ject to the selection bias that might result from using BMI data from
sick visits, which may be associated with conditions either decreasing
or increasing a child’s weight. It will also avoid the self-selection and
subjectivity of survey-based data.
Because healthy BMI changes through childhood, the methods of
longitudinal BMI assessment in children are different from those in
adults [1014]. To measure longitudinal changes in BMI among chil-
dren is challenging as the range of ‘healthy’ BMIs increases with age;
consequently, the absolute change in BMI between health care
encounters cannot be used to monitor appropriate weight gain. Fortu-
nately, this difficulty can be surmounted by using the age- and sex-
adjusted BMI change (ΔBMIadj), which is the change in distance from
the median BMI for age and sex [11] between two successive WCCs, to
measure and quantify harmful weight gain. This metric has advantages
compared to the use of BMI percentile or Z-score, both of which tend
to be compressed in the higher BMI ranges and will therefore mask
increases in children with pre-existing obesity while relatively inflat-
ing increases in children with pre-existing healthy weight [1115].
Prior research has indicated lower physical activity and higher
screen time during the COVID-19 pandemic, raising concern for
increased unhealthy weight gain in children, but has not described the actual change in rate of weight gain, if any, associated with the
pandemic. This study aims to describe the trend in ΔBMIadj measured
at WCCs 20172020 among US children, and test for differences
between 2020 and 20172019, to identify temporal changes in the
rate of BMI change associated with the pandemic era.","The results of this study showed an increase in the rate of
unhealthy weight gain during the pandemic compared to the rate
before the pandemic.
This is likely, in part or in whole, a result of the pandemic-related
lifestyle changes reported in previous literature (including
decreased physical activity, changes in diet composition,
and increased indoor sedentary behaviors). The findings are con-
sistent in direction to predictions made by a simulation study.
The gap widened between subgroups who were, and subgroups
who were not, already vulnerable to excess weight gain. Children
who lack commercial insurance, who had pre-existing obesity, and
who were Black or Hispanic had the highest rate of DBMIadj before
the pandemic and, on top of that, suffered the biggest increase over pre-pandemic rates. This finding highlights important public health
implications for disparities as part of the pandemic.
It was to some degree unexpected that 8-to-12-year-olds had
more marked increases in weight gain than adolescents. One possible
explanation is that screen time habits may be well-established by the
time a child reaches adolescence, whereas younger children may
have been pressed to accommodate enhanced screen time in
response to the COVID-19 pandemic.
Interestingly, boys’ DBMIadj increased more than girls’ relative to
pre-pandemic levels, although pre-pandemic levels were roughly
equal. This may also be reflective of greater prevalence of electronic
gaming device use among boys, another means of screen time during
the pandemic.
Strengths of this research include the size of the pediatric popula-
tion accessible and the longitudinal quality of the study. These data,
drawing from all payor types, represent a diverse range of socioeco-
nomic status. Moreover, the recency of data availability was a unique
strength of this study. The DBMIadj metric allows longitudinal evalua-
tion of BMI more consistent than change in BMI percentile or z-score
and more sensitive than change in percent-obesity. Evaluating
DBMIadj at WCCs in the setting of high WCC attendance prevents the
selection bias encountered by relying on BMIs measured at sick visits
or on self-reporting in surveys.
The study has some limitations. The method of age assignment
will not bias the results noticeably, as already discussed, but will
widen CIs slightly, particularly in younger age groups. Additionally,
this database does not reflect the geographic and racial diversity of
the US, being drawn mainly from the northeast and midwest. While
sub-group analyses provided data on minority subgroups, the Asian
subgroup in particular was small enough that its confidence intervals
were quite large. Finally, the cohort passed through three selection
steps discussed above to be included in the study. While it is possible
that these children had a different outcome from those not selected,
the sensitivity analysis identifying similar and comparable results
between 2019 and 2020 strongly reassures against selection playing
a determinant role in the results.
Opportunities for further research include evaluating the weight
gain trends in pre-school children. Only school-aged children were
included in this analysis, as these children were likely most impacted
by school closures and lockdown restrictions in the US since the start
of the pandemic. Additionally, opportunities exist to explore the
influence of geographic factors and local school closure patterns. As
part of the de-identification process, patient location in this data was
only reported as the census division, which is a geographical area
much bigger than standard policy-forming regions. Further, there is
limited knowledge regarding the equivalence and generalizability of
the health care systems sending data from different census divisions
to OptumÒ. Therefore, we did not attempt geographic analysis with
this data. Finally, examining the association between DBMIadj and
individual food choices, activity patterns, and mental health would
be informative but it was not possible to explore with this data.
Although this study focused on the early effect of pandemic
restrictions on children and resulting unhealthy weight gain
observed during the first year of COVID-19, it will be important to
continue monitoring health indicators in this population. During the
next phase of the pandemic, as restrictions are eased, state-based re-
openings are planned and implemented, and vaccines are distributed,
information on the health outcomes of children will be critical to pol-
icy-makers, educators, and health care professionals as health pro-
motion and mitigation efforts are enacted which provide more
robusust immediate and long-term support to one of the US’s most vul-
nerable populations.
",https://www.sciencedirect.com/science/article/pii/S2589537021003060,
"Skeletal muscle energy metabolism during
exercise",Muscles dont store all the energy we need during exercise. This review nicely describes how liver and fat stores are mobilized to sustain exercise. ,"The continual supply of ATP to the fundamental cellular processes that underpin skeletal muscle contraction during exercise is
essential for sports performance in events lasting seconds to several hours. Because the muscle stores of ATP are small, metabolic pathways must be activated to maintain the required rates of ATP resynthesis. These pathways include phosphocreatine
and muscle glycogen breakdown, thus enabling substrate-level phosphorylation (‘anaerobic’) and oxidative phosphorylation
by using reducing equivalents from carbohydrate and fat metabolism (‘aerobic’). The relative contribution of these metabolic
pathways is primarily determined by the intensity and duration of exercise. For most events at the Olympics, carbohydrate
is the primary fuel for anaerobic and aerobic metabolism. Here, we provide an overview of exercise metabolism and the key
regulatory mechanisms ensuring that ATP resynthesis is closely matched to the ATP demand of exercise. We also summarize
various interventions that target muscle metabolism for ergogenic benefit in athletic events.","In 2020, athletes from around the world were to gather in Tokyo
for the quadrennial Olympic festival of sport, but the event has
been delayed until 2021 because of the COVID-19 pandemic.
When the Olympics takes place, we will witness extraordinary physical and mental efforts in track and field, water and air. Perhaps we
may wonder how these feats are achieved. Such efforts are a culmination of years of dedicated training, and athletic performance
is determined by a complex interaction of biological, mental and
environmental factors. The availability of ATP is critical for skeletal muscle contractile activity, both in explosive-power or sprint
events lasting for seconds or minutes and in endurance events
lasting for hours.
ATP is required for the activity of key enzymes involved in membrane excitability (Na+/K+ ATPase), sarcoplasmic reticulum calcium handling (Ca2+ ATPase) and myofilament cross-bridge cycling
(myosin ATPase). Because the intramuscular stores of ATP are relatively small (~5 mmol per kg wet muscle), they are unable to sustain contractile activity for extended periods. For example, during
all-out, maximal exercise (such as sprinting) at a power output of
900 W (~300% maximal oxygen uptake (VO2 max)), the estimated
rate of ATP utilization is 3.7 mmol ATP kg−1
 s−1
, and exercise could
last <2 s if stored ATP were the sole energy source. During submaximal exercise at ~200 W (~75% VO2 max), the corresponding
values are 0.4 mmol ATP kg−1
 s−1
 and ~15 s, respectively.
Therefore, other metabolic pathways must be activated (Box 1),
including substrate-level phosphorylation (or anaerobic) and oxidative phosphorylation (or aerobic). The latter is critically dependent
on the respiratory and cardiovascular systems, to ensure adequate
oxygen delivery to contracting skeletal muscle, and on reducing
equivalents from the metabolism of primarily carbohydrate and fat1
.
The anaerobic energy pathways have a much higher power (rate of
ATP production) but a smaller capacity (total ATP produced) than
the aerobic pathways2
. In terms of oxidative metabolism, carbohydrate oxidation has a higher power output but a lower capacity
than fat oxidation; this is one factor contributing to the decrease in
power output with carbohydrate depletion during prolonged strenuous exercise2
. This Review provides a brief overview of exercise
metabolism and a summary of the key regulatory mechanisms, and
identifies potential strategies that target metabolism for ergogenic
benefit during athletic events.[","To meet the increased energy needs of exercise, skeletal muscle has
a variety of metabolic pathways that produce ATP both anaerobically (requiring no oxygen) and aerobically. These pathways are
activated simultaneously from the onset of exercise to precisely
meet the demands of a given exercise situation. Although the aerobic pathways are the default, dominant energy-producing pathways
during endurance exercise, they require time (seconds to minutes)
to fully activate, and the anaerobic systems rapidly (in milliseconds
to seconds) provide energy to cover what the aerobic system cannot
provide. Anaerobic energy provision is also important in situations
of high-intensity exercise, such as sprinting, in which the requirement for energy far exceeds the rate that the aerobic systems can
provide. This situation is common in stop-and-go sports, in which
transitions from lower-energy to higher-energy needs are numerous, and provision of both aerobic and anaerobic energy contributes
energy for athletic success. Together, the aerobic energy production
using fat and carbohydrate as fuels and the anaerobic energy provision from PCr breakdown and carbohydrate use in the glycolytic
pathway permit Olympic athletes to meet the high energy needs of
particular events or sports.
The various metabolic pathways are regulated by a range of intramuscular and hormonal signals that influence enzyme activation
and substrate availability, thus ensuring that the rate of ATP resynthesis is closely matched to the ATP demands of exercise. Regular
training and various nutritional interventions have been used to
enhance fatigue resistance via modulation of substrate availability
and the effects of metabolic end products.
The understanding of exercise energy provision, the regulation of
metabolism and the use of fat and carbohydrate fuels during exercise has increased over more than 100 years, on the basis of studies
using various methods including indirect calorimetry, tissue samples
from contracting skeletal muscle, metabolic-tracer sampling, isolated skeletal muscle preparations, and analysis of whole-body and
regional arteriovenous blood samples. However, in virtually all areas
of the regulation of fat and carbohydrate metabolism, much remains
unknown. The introduction of molecular biology techniques has
provided opportunities for further insights into the acute and chronic
responses to exercise and their regulation, but even those studies are
limited by the ability to repeatedly sample muscle in human participants to fully examine the varied time courses of key events. The ability to fully translate findings from in vitro experiments and animal
studies to exercising humans in competitive settings remains limited.
The field also continues to struggle with measures specific to the
various compartments that exist in the cell, and knowledge remains
lacking regarding the physical structures and scaffolding inside
these compartments, and the communication between proteins and
metabolic pathways within compartments. A clear example of these
issues is in studying the events that occur in the mitochondria during exercise. One area that has not advanced as rapidly as needed
is the ability to non-invasively measure the fuels, metabolites and
proteins in the various important muscle cell compartments that are
involved in regulating metabolism during exercise. Although magnetic resonance spectroscopy has been able to measure certain compounds non-invasively, measuring changes that occur with exercise
at the molecular and cellular levels is generally not possible.
Some researchers are investigating exercise metabolism at the
whole-body level through a physiological approach, and others are
examining the intricacies of cell signalling and molecular changes
through a reductionist approach. New opportunities exist for the
integrated use of genomics, proteomics, metabolomics and systems
biology approaches in data analyses, which should provide new
insights into the molecular regulation of exercise metabolism. Many questions remain in every area of energy metabolism, the regulation
of fat and carbohydrate metabolism during exercise, optimal training interventions and the potential for manipulation of metabolic
responses for ergogenic benefits. Exercise biology will thus continue
to be a fruitful research area for many years as researchers seek a
greater understanding of the metabolic bases for the athletic successes that will be enjoyed and celebrated during the quadrennial
Olympic festival of sport.",https://www.nature.com/articles/s42255-020-0251-4.pdf,
"Causes, Characteristics, and Consequences of Metabolically Unhealthy Normal Weight in Humans","This highly cited perspective in explains one in 5 normal weight people are metabolically unhealthy, have 3 fold more risk for all cause mortality than healthy normal weight. Conversely, 1 in 2 obese may be metabolically healthy.","A BMI in the normal range associates with a decreased risk of cardiometabolic disease and all-cause mortality. However, not all subjects in this BMI range have this low risk. Compared to people who are of normal weight and metabolically healthy, subjects who are of normal weight but metabolically unhealthy (20% of the normal weight adult population) have a greater than 3-fold higher risk of all-cause mortality and/or cardiovascular events. Here we address to what extent major risk phenotypes determine metabolic health in lean compared to overweight and obese people and provide support for the existence of a lipodystrophylike phenotype in the general population. Furthermore, we highlight the molecular mechanisms that induce this phenotype. Finally, we propose strategies as to how this knowledge could be implemented in the prevention and treatment of cardiometabolic diseases in different stages of adiposity in routine clinical practice.","The prevalence of overweight and obese individuals has
increased globally during the last few decades, and an elevated
fat mass in those with a BMI > 18.5 kg/m2 is thought to promote
morbidity and mortality (NCD Risk Factor Collaboration (NCDRisC), 2016). While there has been considerable debate as
to whether the lowest risk of mortality is actually found in
the overweight population (defined by WHO as a BMI of
25.0–<30.0 kg/m2
) (Flegal et al., 2013; Hughes, 2013; Ahima
and Lazar, 2013), the largest study with the most rigorous
criteria to account for confounding factors recently showed
that a BMI in the normal weight range (defined by WHO as a
BMI of 18.5–<25.0 kg/m2
) associates with the lowest all-cause
mortality (Di Angelantonio et al., 2016). Specifically, a BMI of
20.0–25.0 kg/m2 was found to be the most protective. These
data suggest that maintaining the BMI in this range may effectively reduce the risk of early death. However, does this assumption apply to all subjects in this BMI range? The research into the
causes and consequences of metabolically healthy obesity
(MHO) has taught us that for a certain BMI, the risk of cardiometablic disease and death can vary substantially among subjects
(Karelis et al., 2004; McLaughlin et al., 2007; Wildman et al.,
2008; Ahima and Lazar, 2013; Stefan et al., 2013; Bluher, 2014; €
Samocha-Bonet et al., 2014; Lotta et al., 2015; Mathew et al.,
2016). In this respect, the largest meta-analysis showed that,
compared to metabolically healthy people in the normal weight
range, subjects with MHO are not at an increased risk of
all-cause mortality and/or cardiovascular events (RR 1.19,
0.98–1.38) during a mean (SD) follow-up of 11.5 (8.3) years.
Nevertheless, this RR increased to 1.24 (1.02–1.55) when only studies with 10 or more years of follow-up were considered, indicating that MHO may be a transient condition. Interestingly, in that analysis the highest risk was found for metabolically unhealthy individuals of normal weight (RR 3.14, 95% CI 2.36–3.93) (Kramer et al., 2013). This finding raises three important questions: (1) what phenotypes characterize these metabolically unhealthy normal weight people, (2) do these phenotypes differ from those which place obese subjects at increased risk, and (3) what molecular mechanisms determine these phenotypes in lean and in obese subjects?","Considering the very high risk of cardiometabolic disease, colorectal cancer, and mortality in metabolically unhealthy normal
weight subjects, it is important to understand what phenotypes
characterize this population. In this regard, we have summarized
published information and provided data indicating that in lean
subjects elevated glucose, dyslipidemia, and hypertension—
the parameters that are commonly used to determine the metabolic risk—are more strongly associated with a relatively low leg
fat mass than with high subcutaneous abdominal fat mass,
visceral obesity, or fatty liver. This finding provides information
that a lipodystrophy-like phenotype exists in the general population. In addition to the abnormalities in lipid storage, this phenotype is also strongly characterized by impaired insulin secretion
capacity and by insulin resistance, low cardiorespiratory fitness,
and increased cIMT. While most of the latter parameters also
determine the metabolic risk in obese subjects, among the
body fat compartments and fatty liver, disproportionate lipid
storage in the lower body is not independently associated with
their metabolic risk. Furthermore, genetic analyses suggest
that metabolic risk appears to be determined by different pathways in normal weight and obese subjects. These findings may have several implications for clinical interventions and
for drug development. First, in the case that a subject with
normal weight may have two or more parameters of the metabolic syndrome in a clinical examination, it would be important
to determine whether impaired glucose tolerance, fatty liver, or
early atherosclerosis is present, so the early treatment of these
conditions can be implemented. Second, in respect to the
improvement of hyperglycemia and dyslipidemia in normal
weight individuals, drugs that can promote adipocyte differentiation may be most effective, when it comes to pharmacological
intervention. Third, concepts of drug development directed
toward the expansion of adipose tissue may prove to be promising to treat not only metabolically unhealthy lean people, but
also overweight and a subset of obese people. Fourth, applying
well-defined phenotyping strategies in clinical trials to better
separate the metabolic risk in normal weight and obese subjects will help to more precisely understand the pathophysiology of
cardiometabolic disease in that targeted lifestyle and pharmacological intervention can be implemented to accomplish the goal
of providing personalized medicine to our patients.",https://www.cell.com/cell-metabolism/fulltext/S1550-4131(17)30429-1?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1550413117304291%3Fshowall%3Dtrue,
Time-restricted feeding protects the blood pressure circadian rhythm in diabetic mice,Treating non-dipping blood pressure for diabetics is challenging. This paper demonstrates Intermittent Fasting can better manage non-dipping hypertension in a pre-clinical animal model. ,"The quantity and quality of food intake have been considered crucial for peoples’ wellness. Only recently has it become appreciated that
the timing of food intake is also critical. Nondipping blood pressure (BP) is prevalent in diabetic patients and is associated with increased
cardiovascular events. However, the causes and mechanisms of nondipping BP in diabetes are not fully understood. Here, we report that food intake and BP were arrhythmic in diabetic db/db mice fed a normal chow diet ad libitum. Imposing a food intake diurnal rhythm by time-restricted feeding (TRF; food was only available for 8 h during the active phase) prevented db/db mice from developing nondipping BP and effectively restored the already disrupted BP circadian rhythm in db/db mice. Interestingly, increasing the time
of food availability from 8 h to 12 h during the active dark phase in db/db mice prompted isocaloric feeding and still provided robust protection of the BP circadian rhythm in db/db mice. In contrast, neither 8-h nor 12-h TRF affected BP dipping in wild-type mice. Mechanistically, we demonstrate that TRF protects the BP circadian rhythm in db/db mice via suppressing the sympathetic activity during the light phase when they are inactive and fasting. Collectively, these data reveal a potentially pivotal role of the timing of food intake in
the prevention and treatment of nondipping BP in diabetes.","As the incidence of type 2 diabetes mellitus continues to rise,
diabetes has become one of the most prevalent and costly
chronic diseases worldwide. About 75% of type 2 diabetic
patients have hypertension, which worsens diabetic vascular
complications and significantly contributes to their morbidity and
mortality. With the increased use of ambulatory blood pressure
(BP) monitoring, accumulating evidence indicates that not only
the level but also the circadian rhythm of BP is critical for cardiovascular health. BP exhibits a circadian rhythm, with a rise
during the early morning and a higher level maintained throughout the day, followed by a decrease of about 10 to 20% at night
(dip). Nondipping BP, defined as a less than 10% nocturnal decline in BP, is highly prevalent in patients with type 2 diabetes. The prevalence of nondipping BP varies among patient populations but is as high as 73% in type 2 diabetic patients.
Nondipping BP, independent of hypertension, is associated with
left ventricular hypertrophy, increased proteinuria, secondary
forms of hypertension, increased insulin resistance, and increased
fibrinogen level. Moreover, a recent large meta-analysis of
17,312 hypertensive individuals with 4 to 8 y follow-up revealed
that nondippers, relative to dippers, have a significant 27% higher
risk for total cardiovascular events. Despite the risk associated
with nondipping BP in diabetic patients, the mechanisms underlying this problem are largely unknown. As a result, there is currently no effective medication for the prevention and treatment of
nondipping BP in diabetes.
It is well known that excessive food intake is a major risk factor
for the onset of obesity and type 2 diabetes. Research over the past decade has demonstrated that the quantity of food intake (how much we eat) and/or the quality of food intake (what
we eat) has a profound impact on obesity and diabetes. Only
recently has it become appreciated that the timing of food intake
(when we eat) is also critical for diet-induced metabolic diseases. Observational human studies revealed that late-night eating
is higher in obese and diabetic patients compared to healthy
individuals and is associated with the development of metabolic disorders. In line with these human studies, animal
work further demonstrated a causal relationship between the
timing of food intake and the development of obesity and metabolic diseases. Of interest, when mice are fed an obesogenic
diet ad libitum, they develop obesity and metabolic disorders. In contrast, when mice are fed the same diet only during the
active dark phase (time-restricted feeding [TRF]), they do not
develop obesity and metabolic disorders. Together,
these studies suggest that the timing of food intake has a profound impact on metabolic health. However, the relationship
between the timing of food intake and BP circadian rhythm is
unknown, and whether TRF can serve as a therapeutic strategy
against nondipping BP in diabetes is also unknown.
The current study seeks to investigate the cause-and-effect
relationship between the timing of food intake and the BP circadian rhythm in diabetic db/db mice. Our results show that the loss of the food intake diurnal rhythm coincides with nondipping 
BP in db/db mice, and imposing a food intake diurnal rhythm by
TRF during the active dark phase effectively protects the BP
circadian rhythm in db/db mice. Mechanistically, we demonstrated that TRF protects the BP circadian rhythm via suppressing the sympathetic nervous system (SNS) activity during the fasting inactive light phase in db/db mice. Our results reveal a
potentially pivotal role of the timing of food intake in BP circadian rhythm in diabetes.","In summary, the current study demonstrated that TRF not only
prevents db/db mice from nondipping BP but restores BP dipping
in nondipping db/db mice. Mechanistically, we unveiled that imposing fasting during the light phase by TRF inhibits sympathetic
activity, thus protecting BP circadian rhythm in db/db mice. The
current study suggests that people with prediabetes or patients
with diabetes can improve their BP circadian rhythm by lifestyle
changes (i.e., eating only at the right time), which can potentially
improve their health outcomes.",https://www.pnas.org/doi/10.1073/pnas.2015873118,
Distribution of dietary protein intake in daily meals influences skeletal muscle hypertrophy via the muscle clock,Protein-rich breakfast may help sustain muscle mass in humans. Night active rodents build muscle when fed a protein rich diet in the evening. This time-of-day effect of diet on muscle is abolished in mice lacking a circadian clock,"The meal distribution of proteins throughout the day is usually skewed. However, its physiological implications and the effects of better protein distribution on muscle volume are largely unknown. Here, using the
two-meals-per-day feeding model, we find that protein intake at the early active phase promotes overloading-induced muscle hypertrophy, in a manner dependent on the local muscle clock. Mice fed branched-chain
amino acid (BCAA)-supplemented diets at the early active phase demonstrate skeletal muscle hypertrophy.
However, distribution-dependent effects are not observed in ClockD19 or muscle-specific Bmal1 knockout
mice. Additionally, we examined the relationship between the distribution of proteins in meals and muscle
functions, such as skeletal muscle index and grip strength in humans. Higher muscle functions were
observed in subjects who ingested dietary proteins mainly at breakfast than at dinner. These data suggest
that protein intake at breakfast may be better for the maintenance of skeletal muscle mass.","Dietary protein intake is important for skeletal muscle growth and
maintenance (Paddon-Jones and Rasmussen, 2009). They are
not only a source of body protein but also activate skeletal
muscle synthesis. In particular, branched-chain amino acids
(BCAAs), such as leucine, isoleucine, and valine, are known to
activate skeletal muscle synthesis via the mammalian target of
rapamycin (mTOR) pathway and are important for muscle growth
(Reidy and Rasmussen, 2016).
Surveys of diets in Western and Asian countries revealed that
protein intake during breakfast is usually low, and the distribution
of proteins across various meals throughout the day is skewed
(Ishikawa-Takata and Takimoto, 2018; US Department of Agriculture [USDA], 2012; Tieland et al., 2015). The distribution of
protein ingestion has been related to muscle functions, such
as muscle synthesis, grip strength, and muscle volume, in humans and rodents (Mamerow et al., 2014; Mishra et al., 2018;
Norton et al., 2017). It has been reported that supplementation
of protein at breakfast and lunch increases skeletal muscle volume in older adults (Norton et al., 2016). Thus, it is hypothesized
that not only the total amount of protein intake but also its distribution across meals is important for muscle growth. This can be
attributed to the postprandial anabolic threshold of protein being
reached across three meals by equal distribution (Paddon-Jones
and Rasmussen, 2009). However, digestive and absorptive capacities and metabolic processes exhibit day-night variations
(Tahara and Shibata, 2013, 2014). It is unclear whether protein
intake distribution along with the day-night variation of its
bioavailability and/or threshold influences muscle growth.
Several physiological functions, including nutritional metabolic
processes, undergo day-night oscillations. Most of these are
driven by the negative feedback loop of circadian core clock
genes comprising Circadian locomotor output cycles kaput
(Clock), Brain and muscle arnt-like 1 (Bmal1), Period1/2 (Per1/
2), and Cryptcrome1/2 (Cry1/2). A heterodimer of CLOCK and
BMAL1 activates the transcription of Pers and Crys via binding
to the E-box site of Pers and Crys. Increasing levels of PERs
and CRYs inhibit their own transcription. Most clock gene knockouts or mutant mice lack the metabolic day-night variation and
show dysfunctions (Tahara and Shibata, 2016), suggesting that
clock genes drive day-night variation in nutrient utilization. For
example, the absorptive capacity of a specific peptide such as
b-alanyl-L-histidine was found to change throughout the day,
because the expression of the peptide transporter was regulated
by the circadian clock (Okamura et al., 2014; Saito et al., 2008). In recent years, it has been found that the circadian clock controls
the day-night oscillation of amino acid metabolism in murine
skeletal muscle (Dyar et al., 2018). Amino acid metabolism in humans was also found to vary in response to the time of the day,
that is, whether the meal is breakfast or dinner (Sato et al., 2018;
Takahashi et al., 2018). These studies suggest that the bioavailability of dietary amino acids and proteins is dependent on the
day-night oscillation modulated by the circadian clock system.
Thus, we hypothesized that an appropriate distribution of daily
protein intake across meals promotes muscle function. Here,
we report that the skeletal muscle hypertrophy is dependent
on the dietary protein distribution in mice having two-mealsper-day feeding. We show here that BCAAs are involved in
inducing distribution-dependent hypertrophic effects. Additionally, the hypertrophic effects of protein distribution were not
observed in clock-disrupted mice, such as Clock mutant
(ClockD19) mice and muscle-specific Bmal1 knockout (MKO)
mice. Finally, we showed that the distribution of protein intake
at breakfast was positively correlated with the skeletal muscle
volume in healthy older women.","In summary, our study on mice showed that the distribution
of protein intake throughout the day influenced skeletal muscle
hypertrophy, and high protein intake at breakfast accelerated
overloading-induced skeletal muscle hypertrophy. BCAAs
played a central role in the hypertrophic effects of the protein
feeding pattern. In addition, there is a possibility that the regulation of myogenesis by the circadian clock may be involved in
the effects dependent on protein feeding patterns. In our human study, a higher SMI and grip strength were observed in
older adult women who consumed a high-protein diet at
breakfast compared with at dinner, suggesting a relationship
between dietary protein distribution in a day and skeletal muscle volume. In the future, more detailed studies on the controlling mechanisms via the circadian clock will be required to explore the effects of dietary protein distribution on muscle
volume.",https://www.cell.com/cell-reports/fulltext/S2211-1247(21)00712-9?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS2211124721007129%3Fshowall%3Dtrue,
Meta-analysis of published exercise transcriptome data from human muscle and blood,A Meta-Analysis of exercise induced gene expression changes from 43 studies involving 739 individuals found the early and late response to exercise. ,"Exercise training prevents multiple diseases, yet the molecular mechanisms that drive exercise adaptation are incompletely understood. To address this, we create a computational framework comprising data from skeletal muscle or blood from 43 studies, including 739 individuals before and after exercise or training. Using linear mixed effects meta-regression, we detect specific time patterns and regulatory modulators of the exercise response. Acute and long-term responses are transcriptionally distinct and we identify SMAD3 as a central regulator of the exercise response. Exercise induces a more pronounced inflammatory response in skeletal muscle of older individuals and our models reveal multiple sex-associated responses. We validate seven of our top genes in a separate human cohort. In this work, we provide a powerful resource (www.extrameta.org) that expands the transcriptional landscape of exercise adaptation by extending previously known responses and their regulatory networks, and identifying novel modality-, time-, age-, and sex-associated changes.

","Performing regular exercise is one of the most important actions by which individuals of all ages can improve their health. It prevents multi-system chronic diseases, reduces anxiety, improves cognitive function, and overall quality of life. Recent studies applying mendelian randomization have also established the causal benefit of exercise in the prevention of depression, bipolar disorder and breast and colorectal cancer, while no effect was observed for Parkinson’s disease or schizophrenia. Both acute effects and long-term adaptation have been studied in response to endurance (repeated contractions of low force) and resistance (fewer contractions of greater force) exercise in humans, where skeletal muscle and blood are the most accessible and well-studied tissues. Exercise-induced adaptation occurs in working skeletal muscles, however, multiple myokines are released from skeletal muscle into blood, with the potential to affect all organs. Resistance training specifically leads to skeletal muscle hypertrophy, while endurance training leads to increased capillary density, ATP-producing capacity and improves cardiorespiratory fitness. Both exercise modalities improve insulin sensitivity, decrease blood pressure, and improve the blood lipid profile by increasing high-density lipoprotein cholesterol and reducing low-density lipoprotein and triglyceride levels. Several key transcriptional pathways in skeletal muscle are also common between the two exercise modalities.

The global transcriptional response to both acute exercise and long-term training, measured using microarrays or RNA-sequencing, has been investigated in healthy humans by sampling human blood and skeletal muscle. These studies identified multiple differentially expressed genes. However, current studies of the molecular response to exercise are limited in size due to the complexity of conducting controlled interventions in healthy individuals that include invasive biological sampling, something that the NIH common fund initiative the Molecular Transducers of Physical Activity is aiming to address. Consequently, available datasets differ substantially in the clinical attributes of their subject sets, including: sex, age, training modality, and sampled post-exercise time points.

Meta-analysis is a standard tool for systematic quantitative analysis of previous research studies. It can generate precise estimates of effect sizes and is often more powerful than any individual study contributing to the pooled analysis. Moreover, the examination of variability or heterogeneity in study results is also critical, especially when apparently conflicting results appear. Meta-analysis can be based on simple weighted averaging using random-effects (RE) models, or, in more complex cases, be based on meta-regression of effect sizes vs. covariates, which are typically assumed to be effect moderators. Meta-analysis (and meta-regression) has been traditionally used in medicine and epidemiology, but it has been gaining popularity in large-scale genomics studies. Recently, Pillon et al. presented a random-effects meta-analysis of exercise gene expression studies from skeletal muscle. The analysis was split based on the training modality and type (e.g., acute endurance exercise studies were analyzed separately from other studies), and thousands of genes were identified to have differential expression, with NR4A3 (nuclear receptor subfamily 4 group A member 3) as the top candidate gene. However, the analysis accounted for only a single moderator, and the dependence of the effects on additional moderators was not analyzed systematically. Thus, the dynamic response of genes over time was not inspected in a systematic way for all genes.

In this work, we gathered and annotated publicly available transcriptome datasets from both endurance and resistance exercise interventions in humans, covering 1724 samples from human blood or skeletal muscle from 739 subjects in total. We address the statistical issues discussed above by (1) fitting a model for each gene while accounting for the explained variability by moderators including: sex, age, training type, and time post exercise, (2) applying a set of filters on the results to promote replicability, and (3) integrating the meta-analysis results with various biological networks using systems biology methods. Our results expand the current understanding of the transcriptional landscape of exercise adaptation by extending previously known expression responses and their regulatory networks, and identifying modality-, time-, age-, and sex-associated changes. We identify SMAD3 (SMAD family member 3) as a central regulator of the acute exercise response, observe a more pronounced inflammatory response to training in older individuals and find multiple sex-associated differentially regulated genes, where MTMR3 (myotubularin related protein 3) has not been associated with exercise before.","We took a meta-analysis approach to study the transcriptional landscape of acute and long-term exercise adaptation in human skeletal muscle and blood (ExTraMeta). Integration with interaction and regulation networks provided key insights into the molecular map of exercise responses. Gene Set Enrichment Analysis using the fold-changes inferred from a naive meta-analysis identified multiple relevant pathways with a marked overlap between the different analyses. Improving the inference at the gene level was done by fitting a model for each gene, accounting for potential moderators such as time, sex, training modality, and age. However, the gene-level overlap between the response to acute and chronic exercise was low, indicating that more data are required to further delineate the interactions by which the acute response transduces signals that translate into long-term effects.

For the muscular response to an acute exercise bout, we identified co-expressed gene clusters that provide insights into the differential temporal response patterns. The network analyses suggest information propagation through the underlying gene networks, with SMAD3 as the main hub and a modality-independent central regulator. Integrating the time trajectories with known gene regulation networks (using DREM), predicted upstream regulators of the temporal response. The clustered gene sets were significantly enriched with pathways related to structural changes, angiogenesis, immune regulation and energy metabolism.

For long-term interventions in muscle, the response was dominated by structural changes and metabolic pathways. Our holistic approach identified a comprehensive response interactome, but also highlighted that current curated pathways cover less than half of the identified genes, illustrating the existing knowledge gap and providing novel candidates for future research. Delving into moderator-associated responses, we identified sex- and age-associated differential expression, where long-term training induced a more prominent inflammatory response in older individuals. It is not clear how a greater inflammatory response at the transcriptional level affects the adaptation in older individuals, as inflammation is an essential part of exercise adaptation, and may also affect the degree of training-induced muscle damage.

Our meta-analysis substantially expands the catalog of sex-associated exercise responses, and offers multiple candidates for further research. We identified 247 genes that were associated with long-term training response and sex. They were functionally associated with histone modifications, which has been shown to be a key transcriptional regulation mechanism in response to endurance training in human skeletal muscle. It is important to note that a limitation to these analyses was the inability to moderate for the physiological response in each individual, which is known to be heterogeneous In blood, the results were not as coherent as in skeletal muscle. We observed the following limitations of these data: (1) we were unable to perform a moderator-specific analysis due to lack of coverage of moderators, (2) less genes were included into the gene model selection process (e.g., only 8650 genes were considered in the acute response, see Methods for details), and (3) the fold-changes tended to be lower. Nevertheless, the base models identified 37 differential genes in the acute analysis, and 48 genes in the long-term analysis. These patterns were mainly enriched for immune responses, which is expected during recovery. However, the limited sample size and the inability to test for moderator effects due to low coverage (e.g., we could not find any public transcriptome datasets for acute resistance interventions) highlight where future research efforts are needed.

Recently, Pillon et al. took a meta-analysis approach to analyze exercise and inactivity gene expression studies from skeletal muscle, and thousands of genes were identified to have differential expression. However, this study is limited in both power and false-positive control. First, the analysis accounted for only a single moderator and the dynamic response over time was not analyzed in a systematic way for all genes. Second, splitting the data likely resulted in loss of power for genes that respond to both resistance and endurance training. Finally, and most importantly, the analysis was based on a naive meta-analysis and selection of genes based on their p-values alone, and the effect heterogeneities were not inspected. When the number of studies is moderate or small (e.g., 15 or less, as in the exercise case) then p-values are often biased downwards. This was also observed previously in the gene expression case, where selecting genes based on the p-value of the meta-analysis resulted in thousands of genes and in poor replicability even after false-discovery rate adjustment. Meta-analysis methods have some additional limitations that should be considered, especially when studies are highly heterogeneous. First, random-effects meta-analysis does not directly explain variability. Second, meta-regression is often exploratory in nature and heavily relies on the included moderators. In our case we could not analyze potential moderators, including height, weight, BMI, training status, baseline VO2max, and baseline heart rate because the vast majority of the samples did not have these data available. Third, when the number of studies is moderate or small, small studies may be assigned too much weight. Finally, meta-analysis can suffer from low power when the heterogeneity is high or when studies are sampled in an unbalanced way across their moderators.

We mitigated these issues by introducing a model selection pipeline that fits the moderator set for each gene to better explain the variability in the data. Moreover, we do not rely on p-values alone to select genes: we require that their models have both a high AICc difference and a high effect size (fold-change). As most moderator pairs are not correlated (Supplementary Fig. 3), we expect that the model selection process correctly detected the relevant moderators in most cases. While some specific models may not be accurate, our gene selection process was developed to highlight genes that are expected to manifest greater replicability. This is corroborated by the high concordance between the meta-analysis findings and the results from the validation cohort.

Understanding exercise response mechanisms holds immense potential for human health and medicine. Our analysis reveals differential expression trajectories in skeletal muscle together with their associated subnetworks and regulation events, and identifies sex- and age-associated transcriptional regulation. These results deepen our understanding of the transcriptional responses to exercise and provide a powerful, free, and easily accessible public resource (www.extrameta.org) for future research efforts in exercise physiology and medicine.",https://extrameta.org/,
"Epigenetic rewiring of skeletal muscle
enhancers after exercise training supports a role
in whole-body function and human health","By combining genomewide histone modifications, gene expression and disease GWAS loci in muscle samples from sedentary or endurance trained men, this study links exercise to reduction in risk for multiple diseases. ","Regular physical exercise improves health by reducing the risk of a plethora of chronic disorders. We hypothesized that endurance
exercise training remodels the activity of gene enhancers in skeletal muscle and that this remodeling contributes to the beneficial effects of
exercise on human health. By studying changes in histone modifications, we mapped the genome-wide positions and activities of enhancers in
skeletal muscle biopsies collected from young sedentary men before and after 6 weeks of endurance exercise. We identified extensive
remodeling of enhancer activities after exercise training, with a large subset of the remodeled enhancers located in the proximity of genes
transcriptionally regulated after exercise. By overlapping the position of enhancers with genetic variants, we identified an enrichment of diseaseassociated genetic variants within the exercise-remodeled enhancers. Our data provide evidence of a functional link between epigenetic rewiring of enhancers to control their activity after exercise
training and the modulation of disease risk in humans.","Regular physical activity decreases the risk of multiple common disorders such as cardiovascular disease, type 2 diabetes,
cancer, and neurological conditions, along with the overall
risk of mortality. The beneficial effects of exercise training on
human health are partially driven by adaptations of the skeletal muscle
tissue. Exercise-induced adaptations include coordinated changes in
the expression of genes controlling substrate usage and metabolic
efficiency in skeletal muscle. In addition to the adaptations that
occur within skeletal muscle cells, exercise exerts systemic effects on
whole-body homeostasis by triggering the release of soluble factors
from the muscle that signal to distal tissues, such as brain, liver, and
adipose tissue. The mechanisms by which training-induced adaptations of skeletal muscle orchestrate positive effects at the wholebody level are poorly understood.
During the past two decades, genome-wide association studies (GWAS)
have identified thousands of genetic variants associated with human
complex traits and diseases. The vast majority of these variants are
located in noncoding DNA regions which overlap with generegulatory regions, particularly enhancers. Enhancers are
distal regulatory elements that are bound by multiple transcription factors and drive gene expression by forming physical interactions with
promoters. Enhancers can be identified at a genome-wide level based
on transcription factor binding clusters, mapping of accessible
chromatin through DNase-Seq/FAIRE-Seq, sequencing of bidirectional enhancer RNA (eRNA), or by mapping enhancerassociated histone modifications, including monomethylation of lysine 4 on histone 3 (H3K4me1) and acetylation of lysine 27 on histone 3
(H3K27ac). Combined, these techniques have identified more
than 1.5 million enhancers across hundreds of human cell lines and
demonstrated that enhancer activity is highly dynamic in a cell typespecific and physiological context-dependent manner. Therefore, mapping of enhancers in different tissues or physiological conditions can inform the mechanisms by which disease-associated genetic
variants regulate phenotypic changes and predispose to disease.","In conclusion, our findings that exercise-remodeled enhancers in
skeletal muscle are significantly enriched for genetic polymorphisms
associated with human complex traits and diseases, suggest the
importance of this metabolic organ in the regulation of whole-body
phenotypes. By providing insight into the mechanisms that may
mediate the positive effects of exercise on cardiovascular function,
platelet biology, cognitive performance, and renal function, our study
constitutes a powerful resource for the identification of key factors
involved in the beneficial effects of endurance training on human
health.",https://www.sciencedirect.com/science/article/pii/S2212877821001356,
"Effect of a plant-based, low-fat diet versus an animal-based, ketogenic diet on ad libitum energy intake
","When allowed to eat ad libitum, people consumed less calories from a Low fat plant-based diet compared to a Low carb diet.  
When on Low Carb diet, they had  increased BHBA and reduced blood glucose.","The carbohydrate–insulin model of obesity posits that high-carbohydrate diets lead to excess insulin secretion, thereby promoting fat accumulation and increasing energy intake. Thus, low-carbohydrate diets are predicted to reduce ad libitum energy intake as compared to low-fat, high-carbohydrate diets. To test this hypothesis, 20 adults aged 29.9 ± 1.4 (mean ± s.e.m.) years with body mass index of 27.8 ± 1.3 kg m−2 were admitted as inpatients to the National Institutes of Health Clinical Center and randomized to consume ad libitum either a minimally processed, plant-based, low-fat diet (10.3% fat, 75.2% carbohydrate) with high glycemic load (85 g 1,000 kcal−1) or a minimally processed, animal-based, ketogenic, low-carbohydrate diet (75.8% fat, 10.0% carbohydrate) with low glycemic load (6 g 1,000 kcal−1) for 2 weeks followed immediately by the alternate diet for 2 weeks. One participant withdrew due to hypoglycemia during the low-carbohydrate diet. The primary outcomes compared mean daily ad libitum energy intake between each 2-week diet period as well as between the final week of each diet. We found that the low-fat diet led to 689 ± 73 kcal d−1 less energy intake than the low-carbohydrate diet over 2 weeks (P < 0.0001) and 544 ± 68 kcal d−1 less over the final week (P < 0.0001). Therefore, the predictions of the carbohydrate–insulin model were inconsistent with our observations.","Increasing obesity prevalence is thought to have been caused by increased availability, convenience and marketing of food whose quality, quantity and composition have changed over time to promote excess energy intake1. Two competing models of obesity and its treatment contrast the relative roles of dietary fat versus carbohydrate. According to the carbohydrate–insulin model of obesity, intake of high-glycemic carbohydrates results in elevated postprandial insulin, which is believed to promote body fat accumulation and thereby increase hunger and energy intake. Alternatively, high-fat foods may promote passive overconsumption of energy due to their high energy density, their weak effect on satiation and satiety as well as modifying food hedonics in a way that supports increased intake.

Whether consumption of low-carbohydrate (LC) or low-fat (LF) diets offer benefits for appetite control has been the subject of long-standing debate. Outpatient diet studies have repeatedly failed to observe meaningful differences in long-term weight loss when participants are randomized to follow LC versus LF diet prescriptions. However, free-living people do not often adhere to prescribed diets, even when all study food is provided. Inpatient studies ensure diet adherence, but few inpatient studies lasting more than a few days have measured ad libitum intake differences between diets varying in carbohydrate and fat and none has investigated LC versus LF diets that were both sufficiently low in their targeted macronutrients to potentially reveal the benefits of one diet over another. For example, substantial restriction of dietary carbohydrate is required to induce a state of ketosis, which is thought to suppress appetite.

Advocates of LC, ketogenic diets often recommend consumption of nonstarchy vegetables and a variety of animal products, while avoiding foods high in sugar and starch. In contrast, advocates of LF diets often recommend ‘whole food’ plant-based diets that also include nonstarchy vegetables, but with added whole grains, legumes and starchy vegetables, while avoiding oils, cooking fats and spreads.

We conducted an inpatient crossover study in 20 adults without diabetes who were exposed for 2 weeks each in random order to an animal-based, ketogenic, LC diet with ~10% of energy from carbohydrates, ~75% from fat and high energy density (~2 kcal g−1) compared to a plant-based, LF diet with ~10% of energy from fat, ~75% from carbohydrate and low energy density (~1 kcal g−1) (Fig. 1a). Both diets were low in ultra-processed food and were matched for nonstarchy vegetables. The first primary outcome compared mean ad libitum energy intake between each 2-week diet period. The second primary outcome compared mean ad libitum energy intake on the second week of each diet period to allow for physiological adaptations to the diets and dissipation of carryover effects. ","Our study was designed to measure ad libitum energy intake when inpatient participants were exposed to food environments corresponding to either a plant-based, LF diet versus an animal-based, ketogenic, LC diet. The LF diet had higher glycemic load and resulted in greater postprandial glucose and insulin levels compared to the LC diet that was higher in energy density. Energy intake during the LF diet was spontaneously reduced by ~550–700 kcal d−1 compared to the LC diet, with participants losing weight and body fat while reporting no significant differences in hunger, fullness, satisfaction or pleasantness of the meals. These data suggest that while the LC diet had benefits for reducing glucose and insulin levels, the LF diet had benefits for appetite control.

Two previous inpatient studies found that LF diets (15–20% of total energy from fat) resulted in ~630–880 kcal d−1 less energy intake over 14 d compared to diets higher in fat. However, the high-fat diets contained 29–42% of total energy from carbohydrate, which may have been too high to sufficiently decrease insulin or increase ketones, which may mediate the appetite-suppressing benefits of LC diets. An inpatient study of participants with obesity and type 2 diabetes found that a very-low-carbohydrate diet (~4% of energy from carbohydrates) decreased ad libitum energy intake by ~950 kcal d−1 over 14 d following a ‘usual diet’ that was not low in fat (44% of total energy from fat) and included a variety of ultra-processed foods that may promote excess energy intake. An outpatient study of participants with obesity found that a very-low-fat diet (~7% fat, ~78% carbohydrate, ~15% protein) resulted in ~1,000 kcal d−1 decrease in ad libitum energy intake over 21 d as compared to a self-reported baseline diet that was ~32% fat, ~51% carbohydrate and ~17% protein20. Finally, a controlled feeding study of men with obesity found that a high-protein ketogenic diet (5% carbohydrates, 65% fat and 30% protein) resulted in a modest ~170 kcal d−1 lower ad libitum energy intake compared to a moderate carbohydrate diet with matched protein and energy density (36% carbohydrate, 34% fat and 30% protein).

Energy intake on the LF diet was stable over both weeks and was persistently lower than the LC diet. Energy intake during the LC diet was significantly decreased during the second week compared to the first week and coincided with increased capillary β-hydroxybutyrate during the second week of the LC diet. It is intriguing to speculate that the observed ~300 kcal d−1 reduction in energy intake from the first to second week of the LC diet corresponds to the magnitude of the appetite-suppressive effect of ketones. Whether long-term adaptations to the diets would eventually eliminate or reverse the energy intake differences is unknown. A recent study found that after 10–15 weeks of adaptation to a LC diet (~20% carbohydrate, ~60% fat), participants reported significantly reduced satiety as compared to a LF diet (~60% carbohydrate, ~20% fat), which supports our shorter-term observation of greater energy intake during the LC diet.

The physiological process of adapting to a ketogenic diet is multifaceted, involving multiple organ systems and plays out over a variety of time scales. Inpatient feeding of the LC diet for 2 weeks resulted in a substantial degree of physiological adaptation by several metrics. First, we observed impaired glucose tolerance at the end of the second week of the LC diet that likely indicates a substantial degree of physiological adaptation to the diet. Second, daily respiratory quotient was ~0.75 during the LC diet, indicating a substantial increase in fat and ketone oxidation, which has previously been shown to occur within the first week of adaptation to a ketogenic diet with no further changes over the following few weeks24. Third, nutritional ketosis was established within several days of instituting the LC diet and capillary β-hydroxybutyrate was stable during the second week of the diet. Stable fasting blood ketones have been observed at weeks two, three and four of an isocaloric ketogenic diet in a previous inpatient controlled feeding study, suggesting that it is unlikely that further increases in ketones would be expected with prolonged exposure to the LC diet. Finally, plasma uric acid approximately doubles at the onset of a ketogenic diet but returns to ~20–50% greater than baseline after 4–8 weeks of adaptation in an outpatient setting. This was similar to the ~35% greater than baseline uric acid levels that we observed after 2 weeks of inpatient LC feeding (Table 2) and suggests that outpatient studies may require longer adaptation periods to ketogenic diets, perhaps due to reduced diet adherence compared to our inpatient study that had greater control over the food environment.

Despite the substantial differences in energy intake between LF and LC diets, total weight loss after 2 weeks was similar. Greater weight loss during the first week of the LC diet compared to the LF diet was likely primarily due to differences in body water, glycogen, protein and gastrointestinal contents. While fat-free mass was relatively preserved with the LF diet, fat-free mass was decreased with the LC diet, which also resulted in a state of of negative nitrogen balance, indicating net loss of body protein despite consumption of more dietary protein than during the LF diet. Only the LF diet led to significant body fat loss. The DXA method used to measure body composition changes in our study has been demonstrated to accurately detect acute body fluid shifts as changes in fat-free mass without affecting body fat mass measurements.

Unlike the LF diet that resulted in significant loss of body fat, the LC diet had no significant body fat changes, suggesting that energy intake during the LC diet was approximately equivalent to the total amount of energy that was expended. The rate of body fat loss during the LF diet was ~35 g d−1 greater compared to the LC diet, which corresponds to a difference in energy balance of ~330 kcal d−1 between the diets, which was somewhat smaller than the observed differences in energy intake between the LF and LC diets. Indeed, energy expenditure as measured in the respiratory chambers was ~150 kcal d−1 lower during the LF diet compared to the LC diet and therefore partially compensated for the ~690 kcal d−1 reduction in energy intake during the LF diet. That leaves ~210 kcal d−1 unaccounted by our measurements, but the uncertainty of this estimate is ~150 kcal d−1 as calculated by the quadratic sum of the s.e. of the energy intake, energy expenditure and rates of change in body fat differences between the diets. So, it is possible that these energy accounting calculations are simply at the limits of energy balance measurement precision in our short-term study. Alternatively, it is possible that unmeasured diet differences in fecal energy excretion or energy expenditure differences that were undetected during the days spent in the respiratory chambers could have contributed to the unaccounted energy imbalance.

In accordance with previous studies, the approximately eucaloric LC diet with ~15% protein likely led to very little changes in energy expenditure compared with baseline. By contrast, the ~700 kcal d−1 decrease in energy intake during the LF diet resulted in decreased energy expenditure. We previously observed that a controlled ~800 kcal d−1 selective reduction of dietary fat from an energy balanced baseline diet, without reductions in dietary carbohydrate or protein, led to a nonsignificant ~50 kcal d−1 decrease in 24-h energy expenditure34. However, the diet used in the previous study was composed of ~8% fat, 71% carbohydrate and 21% protein and was therefore significantly higher in protein compared to the 14% protein consumed during the LF diet. Because dietary protein is more thermogenic than carbohydrate or fat35, the comparatively higher protein intake in our previous study might have been responsible for the relative maintenance of 24-h energy expenditure compared to the LF diet in the present study, which resulted in a ~150 kcal d−1 decrease in 24-h energy expenditure.

Both fasting and postprandial triglycerides are thought to increase risk for cardiovascular disease. The LC diet resulted in decreased fasting triglycerides compared to baseline, whereas the LF diet increased fasting triglycerides. Notably, despite lower fasting triglycerides during the LC diet, postprandial triglycerides were higher following the LC test meal compared to the isocaloric LF test meal likely due to the very high fat content of the LC meal. In contrast, the LF meal led to higher postprandial glucose and insulin levels. The CGM measurements of interstitial glucose concentrations demonstrated that both mean and postprandial glucose excursions were much larger throughout the LF diet period compared to the LC diet. This is of potential concern because high glucose variability is thought to be a risk factor for coronary artery disease. Interestingly, postprandial lactate concentrations were much higher following the LF meal compared to the LC meal, likely due to increased glucose uptake and glycolysis after the LF meal. High lactate levels might have widespread implications for immune modulation as well as oncogenesis.

What was the mechanism for the reduced ad libitum energy intake in the LF diet compared to the LC diet? The LC and LF diets had similar protein presented to the participants, but protein intake during the LC diet was increased compared to the LF diet. Higher protein consumption during the LC diet would be expected to increase satiety and decrease energy intake compared to the LF diet, but the observed differences in energy intake were in the opposite direction.

Perhaps the greater dietary fiber and substantially lower nonbeverage energy density of the LF diet promoted a reduction in energy intake compared to the LC diet. Indeed, the LC diet was at the 75th percentile for US population nonbeverage energy density, whereas the LF diet was below the 25th percentile. However, the determinants of ad libitum energy intake and overall energy balance are likely to be quite complex and unlikely to be explained by dietary fiber and energy density alone. Previous ad libitum feeding studies employing high-fat diets with somewhat lower energy densities than our LC diet (but higher in carbohydrate and lower in fat and protein) resulted in positive energy balance and weight gain, whereas our LC diet led to weight loss. Our previous ad libitum feeding study used an ultra-processed diet that closely matched the nonbeverage energy density of the LC diet and had more dietary fiber, but only the ultra-processed diet led to gain of weight and body fat. The LF diet and the unprocessed diet also had matching nonbeverage energy densities and both contained high amounts of fiber, but more body fat was lost with the LF diet, indicating a greater degree of negative energy balance despite the higher glycemic load of the LF diet compared with the unprocessed diet. Both the LC and LF diets contained few ultra-processed foods, with both diets having a percentage of total calories from ultra-processed food that was within the lowest 20% of the population distribution. Although such cross-study comparisons are obviously imperfect, they suggest that the determinants of energy intake and body fat change cannot be adequately explained by individual factors such as glycemic load, protein intake, dietary fiber or energy density. A more comprehensive model incorporating multiple factors, including eating rate, is likely required.

The main limitation of our study is that the inpatient environment makes it difficult to generalize our results to real-world settings. The participants were told that this was not a weight-loss study, were instructed not to attempt to change their weight and were blinded to their body weight measurements as well as the primary purpose of the study. Whether our results would have been different in free-living people actively trying to lose weight is unknown.

The passive overconsumption model of obesity predicts that consuming a diet with high energy density results in excess energy intake and weight gain. The carbohydrate–insulin model predicts that consuming a diet with high-glycemic carbohydrates results in increased postprandial insulin that drives body fat accumulation, thereby increasing energy intake. While our LF diet contained foods with high glycemic load that substantially increased postprandial glucose and insulin levels compared to the LC diet, the LF diet led to less energy intake compared to the LC diet, which contradicts the predictions of the carbohydrate–insulin model. While the LC diet was high in energy density, it did not result in body fat gain, which challenges the validity of the passive overconsumption model. Our results suggest that regulation of energy intake is more complex than these and other simple models propose.",https://www.nature.com/articles/s41591-020-01209-1,
An isocaloric moderately high-fat diet extends lifespan in male rats and Drosophila,"The take-home message of this recent paper is when rats are fed slightly less calories than ad lib high fat food and the food is given at a fixed time, they live longer and have less tumors. Low- IL6, -ROS and -TG predicted lifespan.","The health effect of dietary fat has been one of the most vexing issues in the field of nutrition. Few animal
studies have examined the impact of high-fat diets on lifespan by controlling energy intake. In this study,
we found that compared to a normal diet, an isocaloric moderately high-fat diet (IHF) significantly prolonged
lifespan by decreasing the profiles of free fatty acids (FFAs) in serum and multiple tissues via downregulating
FFA anabolism and upregulating catabolism pathways in rats and flies. Proteomics analysis in rats identified
PPRC1 as a key protein that was significantly upregulated by nearly 2-fold by IHF, and among the FFAs, only
palmitic acid (PA) was robustly and negatively associated with the expression of PPRC1. Using PPRC1 transgenic RNAi/overexpression flies and in vitro experiments, we demonstrated that IHF significantly reduced
PA, which could upregulate PPRC1 through PPARG, resulting in improvements in oxidative stress and
inflammation and prolonging the lifespan.","The health effects of dietary fat have been among the most vexing issues in public health. Although dietary recommendations
have universally advocated that energy provided by dietary fat
should not exceed 30% for maintaining health, the recent US dietary guideline has canceled the upper limit of fat intake based
on evidence showing that replacing total fat with other macronutrients does not lower the risk of cardiometabolic diseases (Mozaffarian and Ludwig, 2015; Siri-Tarino et al., 2010). Most
recently, in 2017, pioneering research further pointed out that a
high-fat diet decreases the risk of mortality and vascular disease
(Dehghan et al., 2017). Indeed, the health effects exerted by dietary fat could be modified by additional factors, such as calories, as the higher energy provision of fat is accompanied by
higher energy intake, delineating the complexity of the mixed effect of dietary composition on health (DeClerck, 2016; Piper and
Bartke, 2008; Wu et al., 2019). Therefore, to elucidate the health
effects of dietary fat, it is necessary to design a rigorous longterm experiment controlling the total energy intake. However,
such experiments are difficult and infeasible to perform in
humans.
In these circumstances, animal studies controlling energy
intake and feeding high fat throughout adulthood may provide
important evidence for the health effects of dietary fat.
Currently, few studies have investigated the health effects of
dietary fat while controlling the total energy intake; among
these, two studies are worth emphasizing. One study adopted
an intervention of an isocaloric diet with 60% of the energy
from fat (Lundsgaard et al., 2019), and the other study adopted
an intervention of an isocaloric diet with 90% of the energy
from fat (Roberts et al., 2018), both compared to a control
diet. Although both the studies have demonstrated the potential beneficial effects of dietary fat, the durations of the experiments were relatively short, or the intervention doses were
extremely high. The long-term health effects of more moderate
high-fat percentages under the condition of controlling the total energy intake remain largely unknown. In this study, we
performed a series of long-term rat and Drosophila experiments with rigorous feeding of an isocaloric moderately
high-fat (IHF) diet (35% energy from fat) throughout the life cycle. The energy intake was based on the control group, which
was fed a normal diet ad libitum during the experiment. We
aimed to elucidate whether and how an IHF diet would impact
lifespan.
Our data showed that an IHF diet significantly prolonged lifespan and delayed age-associated physiological declines in
both rats and flies. Unexpectedly, an IHF diet significantly
decreased free fatty acids (FFAs) in both serum and peripheral
tissues, including liver, kidney, and muscle, by downregulating FFA anabolism and upregulating catabolism pathways, and
these observations were consistently repeated in the flies. Using
a proteomics approach, PPRC1 was identified as a key protein
that was significantly upregulated by the IHF diet by nearly 2-
fold. Among the decreased profiles of FFAs, only palmitic acid
(PA) was robustly and negatively associated with the expression
of PPRC1. Using PPRC1 transgenic RNAi/overexpression flies
and a series of in vitro experiments, we first demonstrated that
the IHF diet significantly reduced PA, which could upregulate
PPRC1 through PPARG, prolong lifespan, and improve oxidative
stress and inflammation.","In conclusion, our longevity results provide evidence for the relationship between dietary fat and survival in both rats and
Drosophila and suggest that to attain the maximal benefits of
an IHF diet, rigorous control of energy intake is needed to reduce
PA concentrations and promote improvements in oxidative
stress and inflammation. More studies are needed to further
investigate the mechanisms underlying IHF diet-mediated improvements and to optimize diet composition and feeding approaches to extend lifespan.",https://www.cell.com/cell-metabolism/fulltext/S1550-4131(20)30672-0,
"Circadian Rhythm and Sleep Disruption: Causes, Metabolic Consequences, and Countermeasures",What is circadian rhythm disruption and what are the causes? This Endocrine Reviews article clearly defines types and causes of circadian rhythm disruptions.,"Circadian (∼24-hour) timing systems pervade all kingdoms of life and temporally optimize behavior and physiology in humans. Relatively recent changes to our environments, such as the introduction of artificial lighting, can disorganize the circadian system, from the level of the molecular clocks that regulate the timing of cellular activities to the level of synchronization between our daily cycles of behavior and the solar day. Sleep/wake cycles are intertwined with the circadian system, and global trends indicate that these, too, are increasingly subject to disruption. A large proportion of the world's population is at increased risk of environmentally driven circadian rhythm and sleep disruption, and a minority of individuals are also genetically predisposed to circadian misalignment and sleep disorders. The consequences of disruption to the circadian system and sleep are profound and include myriad metabolic ramifications, some of which may be compounded by adverse effects on dietary choices. If not addressed, the deleterious effects of such disruption will continue to cause widespread health problems; therefore, implementation of the numerous behavioral and pharmaceutical interventions that can help restore circadian system alignment and enhance sleep will be important.

","Mankind's historic fascination with the temporal world has taken many forms, from pilgrimages to Stonehenge at the time of the summer solstice for over 5000 years to fanciful notions about time travel. This world has shaped life by means of such rhythmic environmental stimuli as the 24-hour light/dark (LD) cycle, stimuli that have made organisms evolve their own timing systems to anticipate and adapt to daily and seasonal cycles. Thomas Edison's seminal invention of the electric light bulb in 1879 brought unprecedented possibilities, and the American inventor is attributed with once remarking, “The doctor of the future will give no medicine, but will instruct his patient in the care of the human frame, in diet, and in the cause and prevention of disease.” Little was he aware that mistimed use of his great gift to the world is now one of several human-imposed environmental changes that predispose us to many diseases by way of circadian rhythm and sleep disruption. The purposes of this review are to detail our current knowledge about the causes and metabolic consequences of such disruption and to highlight strategies to counteract these consequences. Although studies of other animals have been pivotal in furthering our understanding of the regulation of the circadian system and sleep, it may be premature to extrapolate findings from commonly studied model organisms, particularly nocturnal ones, to our own diurnal species. Therefore, this review focuses on human studies of healthy participants where possible, beginning with observational studies that provide insights into the prevalence of circadian rhythm and sleep disruption. Because shift workers often work during the night (the rest phase for humans, as diurnal organisms), they are at particular risk of circadian rhythm and sleep disruption. Shift workers are also predisposed to other health disorders, such as gastrointestinal issues, and shift work exposure is related to risk of some diseases in a dose-response fashion, including breast cancer and metabolic syndrome. Findings from observational studies also suggest that circadian rhythm and sleep disruption are intertwined with some of these disorders: compared to day shift workers matched for body mass index (BMI), for example, some of the adverse metabolic consequences experienced by night shift workers are coincident with sleep disturbances. Because shift workers make up approximately 17% of the European workforce and approximately 15% of the U.S. workforce, the societal implications of the health consequences of shift work are substantial. Like shift work, jetlag induces circadian rhythm and sleep disruption. Although the health consequences of frequent jetlag are equivocal, any deleterious effects of jetlag-induced circadian rhythm and sleep disruption will become more widespread because it has been estimated that there will be approximately 831 million more air-bound passengers globally in 2016 compared to 2011.

Whereas shift work and jetlag entail overt disruption to the circadian system and sleep, even “normal” working hours can result in more subtle circadian rhythm misalignment and sleep restriction, particularly among evening chronotypes. This is because many individuals use alarms to produce wakefulness when sleep would otherwise occur. Hence, bedtimes tend to differ between workdays and non-workdays, and a discrepancy of at least 1 hour between midsleep time on workdays and non-workdays affects approximately 87% of Northern Europeans. This phenomenon is often termed “social jetlag” and is associated with obesity and behavioral ramifications, such as increased alcohol consumption and smoking. Similarly, greater intraindividual sleep timing variability has been linked to higher fat mass and lower lean mass, as well as cardiometabolic disease risk factors like insulin resistance.

Alarm clock use contributes to pervasive short sleep duration among adults, and analysis of approximately 250 000 self-reports of sleep worldwide suggests that sleep duration on workdays has declined by approximately 3.7 minutes per year in the last decade, the significance of which is that sleep duration is associated with numerous chronic diseases. For example, sleep duration has a U-shaped association with type 2 diabetes mellitus (T2DM) prevalence. The mechanisms underlying the association between short sleep and increased T2DM risk will be discussed subsequently; the reason why long sleep is associated with increased T2DM risk is more contentious, but low socioeconomic status, depression, and other comorbidities are thought to contribute to this relationship.

An inverse association between sleep duration and adiposity is evident in observational studies. In addition, findings from a recent meta-analysis indicate that a negative association between sleep duration and waist circumference is apparent. Importantly, fat mass distribution is particularly salient to metabolic health, with central obesity linked to increased risk of several diseases. The relationship between sleep duration and adiposity is not limited to adults. Because chronotype delays during growth and is latest on reaching physical maturity, enforcing early school starts disrupts sleep timing and duration during adolescence, a critical developmental period and, once more, short sleep during this period is prospectively associated with obesity development. Relationships between sleep and adiposity are related to eating behaviors, and links between circadian rhythm and sleep disruption, dietary habits, and fat mass are further apparent in less common disorders like night eating syndrome.

In contrast to effects of jetlag and working demands on the circadian system and sleep, some individuals are at increased risk of circadian rhythm and sleep disruption regardless of cultural changes. At times this is environmentally driven. All 24 time zones converge at the Poles, for example, where low amplitude LD cycles and extreme temperatures are associated with health ramifications. In other instances, underlying pathologies are at fault. This is true in the case of circadian rhythm disruption in blind individuals without light perception, most of whom experience non-24-hour sleep/wake rhythm disorder in which sleep quality is highly variable. Sleep quality also deteriorates with advancing age, as do other circadian rhythms; thus, more of our ageing population is likely to experience circadian rhythm disruption. Furthermore, with senescence comes a predisposition to various diseases also characterized by circadian system disorganization, one of which is Alzheimer's disease.

Together, it appears that the pervasiveness of circadian rhythm and sleep disruption is rising, and observational evidence implicates this disruption in adverse health effects. Our understanding of the mechanisms underlying these consequences provides the foundation from which to intervene appropriately, and disorganization of the circadian system is at the center of many of these health problems.

","In modern societies, circadian rhythm and sleep disruption are perhaps more pervasive than ever. There is increasing evidence of detrimental effects on metabolic function and dietary choices, emphasizing the importance of bolstering circadian system function and addressing sleep disruption. Because an appreciation of the importance of circadian system entrainment and sleep may significantly enhance health and productivity for many individuals, educating key personnel has great potential to benefit society.

The circadian system optimizes behavior and physiology according to the time of day and is organized in a hierarchical manner with a central clock in the SCN that is primarily entrained by light. Nowadays, we are commonly exposed to less light during the day and more light at night because of artificial lighting, which may impair circadian system organization and disrupt sleep, resulting in widespread adverse effects on metabolic health. Disrupted sleep, for example, promotes increased energy intake, reduced energy expenditure, and insulin resistance in many individuals, consequences that may be compounded by an increased propensity to make less healthy dietary choices. Careful experiments have also shown that circadian misalignment produces an array of metabolic abnormalities.

Future research should continue to study factors influencing individual variation in the risk of and responses to circadian rhythm and sleep disruption, such as sex differences in circadian rhythms, associations between ethnicity and sleep variability, and other factors that contribute to differences in metabolic and behavioral responses to circadian rhythm and sleep disruption between individuals. It may not always be possible to extrapolate findings from animal studies to humans, indicating a need for continued human research, especially in populations experiencing frequent circadian rhythm and sleep disruption. Furthermore, little research has explored such disruption in diseases like T2DM. There has also been little research on large populations likely experiencing circadian rhythm and sleep disruption. People living in China, for example, may be of particular interest because the country spans five geographical time zones, yet the entire nation follows Beijing time. It is plausible that chronic circadian rhythm and sleep disruption may incur some adaptations in the affected, although little research has addressed this to date.

Finally, behavioral and pharmaceutical interventions show promise in offsetting the adverse effects of circadian rhythm and sleep disruption. Some of the beneficial effects of these interventions may be independent of the circadian system and sleep, however. Because our understanding of the range of healthy phase relationships between the SCN and peripheral clock systems is poorly characterized, clarifying these relationships could help personalize prescription of chronobiotics, some of which still require human safety and efficacy studies. Thereafter, comparisons of these interventions are needed to evaluate which are most effective and in what circumstances.

Ultimately, we hope that mankind's historic fascination with the temporal world will continue to draw interest to the importance of the human timing system in all facets of health.

",https://academic.oup.com/edrv/article/37/6/584/2691715?login=false,
COVID-19 and diabetes mellitus: from pathophysiology to clinical management,Diabetes exacerbates COVID-19. This article explains potential mechanism and suggests diabetes management to lessen the burden of COVID-19.,"Initial studies found increased severity of coronavirus disease 2019 (COVID-19), caused by infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), in patients with diabetes mellitus. Furthermore, COVID-19 might also predispose infected individuals to hyperglycaemia. Interacting with other risk factors, hyperglycaemia might modulate immune and inflammatory responses, thus predisposing patients to severe COVID-19 and possible lethal outcomes. Angiotensin-converting enzyme 2 (ACE2), which is part of the renin–angiotensin–aldosterone system (RAAS), is the main entry receptor for SARS-CoV-2; although dipeptidyl peptidase 4 (DPP4) might also act as a binding target. Preliminary data, however, do not suggest a notable effect of glucose-lowering DPP4 inhibitors on SARS-CoV-2 susceptibility. Owing to their pharmacological characteristics, sodium–glucose cotransporter 2 (SGLT2) inhibitors might cause adverse effects in patients with COVID-19 and so cannot be recommended. Currently, insulin should be the main approach to the control of acute glycaemia. Most available evidence does not distinguish between the major types of diabetes mellitus and is related to type 2 diabetes mellitus owing to its high prevalence. However, some limited evidence is now available on type 1 diabetes mellitus and COVID-19. Most of these conclusions are preliminary, and further investigation of the optimal management in patients with diabetes mellitus is warranted.","Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the novel coronavirus that causes coronavirus disease 2019 (COVID-19), was first reported in Wuhan, China, in December 2019 and has spread worldwide. As of 29 October 2020, 44,351,506 globally confirmed cases of COVID-19 have been reported on the World Health Organization COVID-19 dashboard, including 1,171,255 deaths. The fatality rate for COVID-19 has been estimated to be 0.5–1.0%1,2,3. From 1 March to 30 May 2020, 122,300 excess all-cause deaths occurred in the USA, of which 95,235 (79%) were officially attributed to COVID-19 (ref.4). Of note, mortality from COVID-19 and seasonal influenza is not equivalent, as deaths associated with these diseases do not reflect frontline clinical conditions in the same way. For example, COVID-19 pandemic-hit areas have been facing critical shortages in terms of access to supplies such as ventilators and intensive care unit (ICU) facilities5.

SARS-CoV-2 is a positive-stranded RNA virus that is enclosed by a protein-decorated lipid bilayer containing a single-stranded RNA genome; SARS-CoV-2 has 82% homology with human SARS-CoV, which causes severe acute respiratory syndrome (SARS)6. In human cells, the main entry receptor for SARS-CoV-2 is angiotensin-converting enzyme 2 (ACE2)7, which is highly expressed in lung alveolar cells, cardiac myocytes, vascular endothelium and various other cell types8. In humans, the main route of SARS-CoV-2 transmission is through virus-bearing respiratory droplets9. Generally, patients with COVID-19 develop symptoms at 5–6 days after infection. Similar to SARS-CoV and the related Middle Eastern respiratory syndrome (MERS)-CoV, SARS-CoV-2 infection induces mild symptoms in the initial stage for 2 weeks on average but has the potential to develop into severe illness, including a systemic inflammatory response syndrome, acute respiratory distress syndrome (ARDS), multi-organ involvement and shock10. Patients at high risk of severe COVID-19 or death have several characteristics, including advanced age and male sex, and have underlying health issues, such as cardiovascular disease (CVD), obesity and/or type 1 diabetes mellitus (T1DM) or type 2 diabetes mellitus (T2DM)11,12,13. A few early studies have shown that underlying CVD and diabetes mellitus are common among patients with COVID-19 admitted to ICUs14,15. T2DM is typically a disease of advanced age, and, therefore, whether diabetes mellitus is a COVID-19 risk factor over and above advanced age is currently unknown.

The basic and clinical science of the potential inter-relationships between diabetes mellitus and COVID-19 has been reviewed16. However, knowledge in this field is emerging rapidly, with numerous publications appearing frequently. This Review summarizes the new advances in diabetes mellitus and COVID-19 and extends the focus towards clinical recommendations for patients with diabetes mellitus at risk of or affected by COVID-19. Most available research does not distinguish between diabetes mellitus type and is mainly focused on T2DM, owing to its high prevalence. However, some limited research is available on COVID-19 and T1DM, which we highlight in this Review.","During the COVID-19 pandemic, patients with diabetes mellitus should be aware that COVID-19 can increase blood levels of glucose and, as such, they should follow clinical guidelines for the management of diabetes mellitus more strictly, as described here. We provide the following general guidance for patients and health-care providers: patients should be extra vigilant regarding their adherence to prescribed medications (including insulin injections) and their blood levels of glucose, which should be checked more frequently than previously. If blood concentrations of glucose are consistently higher than usual, patients should consult their physician. In the light of current global quarantine policies, more emphasis needs to be placed by health-care providers on healthy food intake and physical activity in patients with diabetes mellitus. If patients experience symptoms such as a dry cough, excessive sputum production or fever, or show a sudden rise in glucose level, they should be advised to consult their physician immediately. Furthermore, it is strongly recommended that patients should strictly adhere to the recommendations of their doctor and be wary of messages communicated by various types of media (including the internet), which often might not stand scientific scrutiny. Most importantly, general precautions should be strictly followed by both health-care providers and their patients, such as social distancing, wearing a mask, washing hands and using disinfectants, to reduce the risk of infection in patients with diabetes mellitus. Telehealth or remote consultations might help reduce the risk posed by direct physical contact between patients and medical personnel. These could be further ways to minimize the risk of SARS-CoV-2 transmission and at the same time provide continued and safe medical care to the general public.

Coronavirus infections are proven to have a huge effect on the management of diabetes mellitus because they aggravate inflammation and alter immune system responses, leading to difficulties in glycaemic control. SARS-CoV-2 infection also increases the risk of thromboembolism and is more likely to induce cardiorespiratory failure in patients with diabetes mellitus than in patients without diabetes mellitus. All of these mechanisms are now believed to contribute to the poor prognosis of patients with diabetes mellitus and COVID-19. During the COVID-19 pandemic, tight glycaemic control and management of cardiovascular risk factors are crucial for patients with diabetes mellitus. Medications used for both diabetes mellitus and CVD should be adjusted accordingly for people at high risk of SARS-CoV-2 infection. Two experimental agents (dexamethasone and hydroxychloroquine) have shown some promise as treatment agents62,183,207. Based on these results, combined treatment with these two agents might be more beneficial than either agent alone. However, it should be kept in mind that the efficacy of dexamethasone in treating COVID-19 was proven in well-designed RCTs such as the RECOVERY study62, whereas no such compelling RCTs have been performed for hydroxychloroquine.

In conclusion, the COVID-19 global pandemic poses considerable health hazards, especially for patients with diabetes mellitus. A definitive treatment or vaccine for COVID-19 has yet to be discovered. Therefossre, preventing infection in the first place is still the best solution. Under these circumstances, patients with diabetes mellitus should make a determined effort to maintain a healthy lifestyle and to decrease potential risk factors. The optimal management strategy for such patients, such as the choice of glucose-lowering, antihypertensive and lipid-lowering medications, is an important topic for current and future research.",https://www.nature.com/articles/s41574-020-00435-4#Sec26,
Mechanisms by which adiponectin reverses high fat diet-induced insulin resistance in mice,"Adiponectin reduces insulin resistance, but the mechanism is not clear. This paper shows adiponectin acts on both muscle and fat cells to improve insulin sensitivity. ","Adiponectin has emerged as a potential therapy for type 2
diabetes mellitus, but the molecular mechanism by which adiponectin reverses insulin resistance remains unclear. Two weeks of
globular adiponectin (gAcrp30) treatment reduced fasting plasma
glucose, triglyceride (TAG), and insulin concentrations and reversed whole-body insulin resistance, which could be attributed to
both improved insulin-mediated suppression of endogenous glucose production and increased insulin-stimulated glucose uptake
in muscle and adipose tissues. These improvements in liver and
muscle sensitivity were associated with ∼50% reductions in liver
and muscle TAG and plasma membrane (PM)-associated diacylglycerol (DAG) content and occurred independent of reductions in
total ceramide content. Reductions of PM DAG content in liver
and skeletal muscle were associated with reduced PKCe translocation in liver and reduced PKCθ and PKCe translocation in skeletal
muscle resulting in increased insulin-stimulated insulin receptor
tyrosine1162 phosphorylation, IRS-1/IRS-2–associated PI3-kinase
activity, and Akt-serine phosphorylation. Both gAcrp30 and fulllength adiponectin (Acrp30) treatment increased eNOS/AMPK activation in muscle and muscle fatty acid oxidation. gAcrp30 and
Acrp30 infusions also increased TAG uptake in epididymal white
adipose tissue (eWAT), which could be attributed to increased lipoprotein lipase (LPL) activity. These data suggest that adiponectin and adiponectin-related molecules reverse lipid-induced liver and muscle insulin resistance by reducing ectopic lipid storage in
these organs, resulting in decreased plasma membrane sn-1,2-
DAG–induced nPKC activity and increased insulin signaling. Adiponectin mediates these effects by both promoting the storage of
TAG in eWAT likely through stimulation of LPL as well as by stimulation of AMPK in muscle resulting in increased muscle fat
oxidation.","Type 2 diabetes mellitus (T2DM) is one of the leading causes
of morbidity and mortality in the adult population worldwide
(1, 2) and is associated with disease in many organ systems, including nonalcoholic fatty liver disease (NAFLD) and atherosclerotic vascular disease (ASCVD) (3–6). Insulin resistance
plays a critical role in the pathogenesis of T2DM and the metabolic syndrome. The adipokine adiponectin has emerged as a
potential antidiabetic, antiinflammatory, and antiatherogenic factor (7, 8). Unlike adipokines such as leptin, plasma adiponectin
levels are inversely correlated with adiposity and decreased in
obesity, insulin resistance, and T2DM (9, 10). Adiponectin is
present in human plasma as full-length adiponectin (Acrp30) and
as a C-terminal globular fragment (gAcrp30) (11–13). The
C-terminal globular fragment is produced by proteolytic cleavage
and is thought to be the pharmacologically active moiety (11). A
wide variety of explanations for adiponectin’s glucose lowering
and insulin sensitizing properties has been proposed, which have
been derived predominantly from in vitro and ex vivo studies,
including: suppression of gluconeogenesis (14–16), increased AMPK/ACC-dependent fatty acid oxidation in liver and muscle
(7, 12, 14, 17), and reduced hepatic ceramide content by activation
of hepatic ceramidase (18). A clear, consistent model for adiponectin’s action in vivo is lacking, and the mechanisms by which
adiponectin ameliorates insulin resistance are a matter of active
debate.
The association between ectopic lipid and insulin resistance in
liver and skeletal muscle is widely recognized (19–21). Diacylglycerols (DAGs) and ceramides are the two best-studied
mediators of lipid-induced insulin resistance. Ceramides have
been shown to impair insulin action at the level of protein kinase
B (Akt) phosphorylation, through activation of protein kinase Cζ
(PKCζ) and/or protein phosphatase 2A (22–24). In contrast,
plasma membrane sn-1,2-DAGs, which has been shown to be the
key DAG stereoisomer, impair insulin action via activation of
novel PKCs (nPKCs), including PKCe in liver (25–27) and both
PKCθ and PKCe in skeletal muscle (28, 29). PKCe activation
subsequently impairs insulin receptor kinase (IRK) tyrosine kinase activity, and PKCθ activation impairs insulin signaling at the
level of IRS-1/IRS-2–associated PI3-kinase activity (20, 30, 31).
Insulin resistance in the liver leads to reduced insulin-stimulated
hepatic glycogen synthesis and defects in insulin suppression of
hepatic glucose production, while insulin resistance in the skeletal muscle leads to reduced insulin-stimulated muscle glucose
transport. In the setting of white adipose tissue (WAT) insulin resistance, WAT lipolysis is resistant to suppression by insulin,
leading to increased nonesterified fatty acid (NEFA) delivery to
the liver and muscle, which may further promote increased liver
and muscle ectopic lipid content (4, 5, 32, 33).
Given that prior studies have demonstrated that increased
plasma adiponectin concentrations lead to accretion of WAT
and improved glycemia in mice (34, 35), we hypothesized that the
insulin-sensitizing properties of adiponectin might be due to protection against ectopic lipid deposition in insulin-responsive tissues. To address this hypothesis, we performed a comprehensive
series of studies to assess the effects of 2-wk gAcrp30 and Acrp30
treatment on multiple metabolic fluxes using a combination of
stable- and radio-labeled isotopic tracers, in a high fat diet (HFD)-
fed mouse model of lipid-induced insulin resistance. Here, we
demonstrate that 2 wk of gAcrp30 treatment reverses whole-body
insulin resistance in HFD-fed mice by reducing plasma membrane
DAG content, resulting in decreased translocation of PKCe to the
plasma membrane in liver and decreased PKCe/PKCθ translocation in skeletal muscle, leading to increased insulin signaling in
both of these tissues. This reduction in ectopic lipid storage in liver
and muscle could be attributed to increased lipoprotein lipase
activity in epididymal WAT (eWAT), resulting in increased lipid
uptake in eWAT, as well as activation of AMPK in muscle, which,
in turn, promoted increased fatty acid oxidation in skeletal muscle.
Taken together these results provide insights into the mechanisms
by which adiponectin reverses insulin resistance in vivo.","WAT is not only a critical energy storage depot, but it also acts as
an endocrine organ sensing metabolic signals and secreting hormones and adipocytokines (e.g., leptin and adiponectin) that
regulate whole-body energy homeostasis (50–53). Consistent with
previous reports (14, 15, 17), we have demonstrated that administration of globular adiponectin results in an improvement in
whole-body glucose homeostasis. Despite great interest in adiponectin, the mechanism by which adiponectin reverses insulin resistance remains unclear. To address this question, we performed
a comprehensive series of studies including hyperinsulinemiceuglycemic clamp studies combined with stable-labeled and
radio-labeled isotopic tracers to characterize adiponectin’s effects
on endogenous glucose production and tissue-specific insulin
sensitivity and followed these studies up by measuring bioactive
lipid metabolites and cellular insulin signaling phosphorylation
events in liver, skeletal muscle, and WAT.
Adiponectin receptor associated ceramidase activity, promoting decreased total hepatic ceramide content and ceramideinduced insulin resistance, has been proposed to mediate adiponectin’s insulin-sensitizing properties (18). However, in contrast to this hypothesis, we dissociated changes in total ceramide
content in the liver and skeletal muscle from gAcrp30-induced
improvements in liver and muscle insulin sensitivity. We also did
not observe any significant differences in the content of specific
ceramide species (C16:0 and C18:0), which have been specifically
hypothesized to mediate insulin resistance in rodents (42, 43).
While gAcrp30 treatment did not cause a reduction in total tissue ceramide content or in changes in C16:0 or C18:0 ceramides,
it did result in reductions in several hepatic ceramide species
(C16:0, C20:0, C22:0, C24:0, and C24:1) in the plasma membrane, which correlated with improved insulin sensitivity in liver.
Whether these specific plasma membrane-associated ceramide
species also contributed to alterations in insulin action will need
to be examined in future studies.
Nevertheless, ceramide-induced insulin resistance is thought
to alter downstream insulin signaling at the level of Akt; however, we observed that gAcrp30 improved insulin action at the
level of the insulin receptor, which is not compatible with the
putative mechanisms by which adiponectin is thought to mediate
insulin resistance at the level of AKT2 phosphorylation.
In contrast with ceramide-induced insulin resistance, DAGPKCe–induced insulin resistance can explain improved insulin
signaling at the level of the insulin receptor. By this mechanism,
sn-1,2-DAG accumulation in the plasma membrane of liver and
muscle results in nPKC translocation from the cytoplasm to the
plasma membrane, leading to decreased insulin signaling at the
level of the insulin receptor due to PKCe activation and at the
level of IRS-1–associated and IRS-2–associated PI3-kinase due
to PKCθ activation (25, 26, 30). We observed that 2 wk of
gAcrp30 treatment reduced plasma membrane sn-1,2-DAG in
liver and membrane-associated DAG in muscle, leading to decreased PKCe activity in liver and both PKCθ and PKCe activity
in skeletal muscle. As a result, insulin signaling at the level of
insulin receptor kinase increased in both of these tissues. As
such, the effect of globular adiponectin on tissue-specific insulin
action appears to occur through reductions in liver and muscle
plasma membrane DAG content, resulting in reduced PKCe
activation in liver and reduction in both PKCe and PKCθ activation in skeletal muscle.
In both in vitro and ex vivo studies, adiponectin has been
suggested to reduce TAG content in the liver and muscle by
enhancing fatty acid oxidation in an AMPK-dependent manner
(7, 12, 14, 54, 55). However, Yamauchi et al. found that globular
adiponectin cannot activate hepatic AMPK signaling pathways
(48). No competing hypothesis has yet been published, and so
the underlying physiological mechanisms by which gAcrp30 reduces hepatic TAG are still debated. Further complicating this
question, most mechanistic studies examining adiponectin’s
mechanism of action have been performed purely in vitro and
ex vivo, whereas in vivo studies are critical to understand the
complex interorgan cross-talk that regulates metabolic physiology. Reduced ectopic lipid content in liver and skeletal muscle
may be due to several factors including 1) decreased NEFA flux
to these tissues from reduced WAT lipolysis; 2) increased rates of
tissue mitochondrial fatty acid oxidation; and 3) decreased lipid
delivered to tissues from circulating lipoproteins. We evaluated
each of these potential mechanisms for the gAcrp30-induced reductions in ectopic lipids in HFD-fed mice using a comprehensive
series of in vivo metabolic studies. While gAcrp30 appeared to
suppress rates of WAT lipolysis, as reflected by reduced rates of
glycerol turnover and increased WAT insulin sensitivity, as reflected
by increased insulin-stimulated glucose uptake, it did not affect
whole-body fatty acid turnover potentially due to compensatory
changes in reesterification. Additionally, hepatic mitochondrial fatty
acid oxidation and the regulation of fat oxidation in liver were
unchanged. In gastrocnemius and soleus muscle, gAcrp30 treatment
increases muscle fatty oxidation in vivo and ex vivo, an effect that
was correlated with increased phosphorylation of ACC in a manner
consistent with previously described eNOS/AMPK-dependent regulation of ACC (46). This increase in skeletal muscle fatty acid
oxidation could account, in part, for the reduced ectopic lipid deposition seen in several tissues in gAcrp30-treated mice and the
improvement in muscle insulin sensitivity.
In addition to promoting increased muscle fatty acid oxidation, we also found that both gAcrp30 and Acrp30 treatment
reduces ectopic lipid (TAG/plasma membrane DAG) accumulation in liver and skeletal muscle by improving WAT TAG uptake and further increasing WAT storage capacity. Adiponectintreated mice displayed increased LPL activity in postheparin
plasma and eWAT and improved adipose postprandial triacylglycerol uptake. These results are consistent with our observations that 2 wk of gAcrp30 or Acrpt30 treatment increased
eWAT mass but did not change total fat mass, as assessed by
1
H NMR.
Our findings also imply an important role for decreased
plasma adiponectin in the development of lipid-induced liver
and skeletal muscle insulin resistance. In humans and monkeys,
plasma adiponectin levels correlate significantly with whole-body
insulin sensitivity (56, 57). Overexpression or administration of
adiponectin in mice results in a decrease in hyperglycemia and
improvement in systemic insulin sensitivity (7, 58), whereas
adiponectin-deficient mice exhibit impaired insulin sensitivity
and are prone to diabetes (8, 59). Tying all of this together,
circulating adiponectin may be a reflection of the presence of
functioning adipose tissue, a part of the machinery the WAT
uses in its fat-storing operation. In normal physiology, healthy adipose tissue secretes sufficient adiponectin to promote storage
of circulating TAG in WAT and signal a shift to increase fatty
acid oxidation in skeletal muscle. However, in obesity, as adipose
tissue has limited storage capacity, WAT secretion of adiponectin decreases. This derangement in fat storage and muscle fat
oxidation may then lead to increased ectopic lipid (TAGs/plasma
membrane DAGs) accumulation in liver and skeletal muscle and
the subsequent development of insulin resistance in these organs
leading to the metabolic syndrome, hepatic steatosis/NASH, and
atherosclerosis.
Taken together, these results suggest that chronic adiponectin
administration ameliorates insulin resistance in an HFD-fed
mouse model of obesity, NAFLD, and insulin resistance by two
major mechanisms. First, adiponectin treatment promotes increased WAT LPL activity, which may lead to increased uptake
of TAG into WAT, thus diverting circulating TAG away from
storage in liver and skeletal muscle. Second, adiponectin treatment promotes increased fatty acid oxidation in skeletal muscle,
which, in turn, may be attributed to the activation of AMPK and
eNOS. These two effects of adiponectin, in turn, lead to reductions in liver and muscle plasma membrane-associated sn-1,2-
DAG content, resulting in decreased PKCe activity in liver and
decreased PKCe and PKCθ in muscle resulting in increased insulin signaling and insulin action in these tissues. Furthermore,
adiponectin-induced improvement in liver and muscle insulin
sensitivity in insulin-resistant, HFD-fed mice occurred independently of changes in total ceramide content in these tissues.
Taken together, these studies provide insights into the mechanisms by which adiponectin reverses HFD-induced liver and
muscle insulin resistance in mice.",https://www.pnas.org/doi/full/10.1073/pnas.1922169117,
"Exercise mitigates sleep-loss-induced changes
in glucose tolerance, mitochondrial function,
sarcoplasmic protein synthesis, and diurnal
rhythms",Reduced sleep can increase blood glucose level. But exercise during sleep loss can sustain normal glucose level.,"Sleep loss has emerged as a risk factor for the development of impaired glucose tolerance. The mechanisms underpinning this
observation are unknown; however, both mitochondrial dysfunction and circadian misalignment have been proposed. Because exercise improves
glucose tolerance and mitochondrial function, and alters circadian rhythms, we investigated whether exercise may counteract the effects induced
by inadequate sleep. To minimize between-group differences of baseline characteristics, 24 healthy young males were allocated into one of the three
experimental groups: a Normal Sleep (NS) group (8 h time in bed (TIB) per night, for five nights), a Sleep Restriction (SR) group (4 h TIB per night,
for five nights), and a Sleep Restriction and Exercise group (SRþEX) (4 h TIB per night, for five nights and three high-intensity interval exercise
(HIIE) sessions). Glucose tolerance, mitochondrial respiratory function, sarcoplasmic protein synthesis (SarcPS), and diurnal measures of pe-
ripheral skin temperature were assessed pre- and post-intervention. We report that the SR group had reduced glucose tolerance post-intervention (mean change  SD, P value, SR glucose AUC: 149  115 A.U., P ¼ 0.002), which was also associated with reductions in mitochondrial respiratory function (SR: -15.9  12.4 pmol
O2.s1.mg1, P ¼ 0.001), a lower rate of SarcPS (FSR%/day SR: 1.11  0.25%, P < 0.001), and reduced amplitude of diurnal rhythms. These
effects were not observed when incorporating three sessions of HIIE during this period (SRþEX: glucose AUC 67  57, P ¼ 0.239, mitochondrial
respiratory function: 0.6  11.8 pmol O2.s1.mg1, P ¼ 0.997, and SarcPS (FSR%/day): 1.77  0.22%, P ¼ 0.971). A five-night period of sleep restriction leads to reductions in mitochondrial respiratory function, SarcPS, and amplitude of skin
temperature diurnal rhythms, with a concurrent reduction in glucose tolerance. We provide novel data demonstrating that these same detrimental
effects are not observed when HIIE is performed during the period of sleep restriction. These data therefore provide evidence in support of the use
of HIIE as an intervention to mitigate the detrimental physiological effects of sleep loss.","The detrimental effects of sleep loss on glucose tolerance are now well
established, and insufficient sleep is a risk factor for the development of
type 2 diabetes (T2D) [1]. In fact, sleep loss is comparable with other
more traditional risk factors that are associated with the development of
T2D, such as physical inactivity [1]. Several studies have shown that
periods of sleep restriction, or reduced time in bed (TIB), typically with a
sleep opportunity of 4e5 h per night, cause significant reductions in a
range of indices related to glucose metabolism [2e5]. The severity of
this effect can be seen with only one night of either sleep restriction (4 h
TIB) or sleep deprivation (e.g., no sleep), which can reduce insulin sensitivity [6e9]. Despite these findings, there are limited data
explaining the physiological and molecular changes that underpin these
effects. As a large proportion of the population do not meet the current
sleep recommendations (7e9 h per night) [10,11], and inadequate
sleep is a consequence of many occupations [12,13], gaining a better
understanding of these mechanisms may help to tailor specific in-
terventions aimed at counteracting the detrimental effects of sleep loss.
The physiological mechanisms that underpin the impairment of
glucose tolerance following sleep restriction are likely multifactorial.
While not previously investigated in the context of sleep loss, the
development of insulin resistance has been associated with a reduc-
tion in mitochondrial content and impaired mitochondrial respiratory
function [14,15]. Furthermore, reductions in citrate synthase activity (a
surrogate marker for mitochondrial content [16]) and mitochondrial
respiratory function has also been reported in T2D patients, compared
to obese non-diabetics, suggesting a link between mitochondrial
changes and the development of insulin resistance and T2D [14,15].
Therefore, sleep-loss-induced reductions in glucose tolerance may, in
part, be a consequence of changes in mitochondrial content, function,
or the processes that regulate these properties, including mitochon-
drial dynamics and mitochondrial protein synthesis [17]. In support of
this, 120 h of sleep deprivation was associated with a 24% reduction in
citrate synthase activity in human skeletal muscle [18]. However, how
these results translate to the context of the sleep loss commonly
experienced in society, such as repeated nights of partial sleep loss,
has not been determined and remains a critical gap in the literature.
The detrimental effect of sleep loss on glucose metabolism may also
be associated with the misalignment of circadian rhythms [7,8]. One
night of sleep deprivation (commonly experienced by 20% of the
world’s population who perform shift-work) leads to a reduction in
glucose tolerance and concomitant alterations in the expression of
skeletal muscle clock genes (i.e., Bmal1 and Cry1 gene expression [7])
and the content of clock proteins (i.e., BMAL1) [8], which are known to
regulate circadian rhythms at a molecular level [19]. The functional
significance of disrupting the molecular clock has been shown in
genetic mouse models (i.e., Clock mutant mice and the Bmal1 KO
mouse), which display reduced glucose tolerance, mitochondrial res-
piratory function, and skeletal muscle contractile function [20,21].
However, the effect of sleep restriction on markers of diurnal rhythms
(e.g., skeletal muscle clock gene expression) and the potential impli-
cations of such changes have not previously been examined.
One approach to mitigate or ablate the impact of reduced sleep
duration on glucose tolerance is via exercise [17]. Regular endurance
exercise has been shown to exert beneficial effects on glycemic control
via the activation of the contraction-mediated (insulin independent)
glucose uptake signaling pathway [22]. High-intensity interval exercise
(HIIE) is a time-efficient format of endurance exercise and also a potent
stimulus for the induction of mitochondrial biogenesis [23], with in-
crease in sarcoplasmic and mitochondrial protein synthesis and
mitochondrial content and respiratory function, occurring concomi-
tantly with improvements in glucose tolerance [24e27]. This raises the
intriguing hypothesis that exercise may also be useful to combat sleep-
loss-induced impairments to glucose tolerance, which are not
necessarily reversed by a period of recovery sleep alone [28e30].
Furthermore, the same detrimental metabolic changes that occur in
response to circadian misalignment and altered expression of clock
genes may also be ameliorated by performing exercise [20,31,32].
Consequently, HIIE may mitigate the detrimental effects of sleep loss
on glucose metabolism by increasing mitochondrial content and
function and preventing changes in circadian rhythmicity [17]. Accordingly, the aim of this study was to investigate the effect of sleep
restriction on glucose tolerance, and to examine the underlying
physiological alterations that might contribute to these changes;
specifically, we examined changes in mitochondrial content and
function and diurnal rhythms. Furthermore, we examined the role of
exercise as an intervention to mitigate the detrimental effects of sleep
restriction. We hypothesized that sleep restriction would reduce
mitochondrial content and respiratory function and disrupt diurnal
rhythms, with a concomitant reduction in glucose tolerance, but that
performance of HIIE would ameliorate these effects. ","In summary, we have provided the first direct evidence of a
concomitant decrease in mitochondrial respiratory function, SarcPS (of
which MitoPS is a contributor), diurnal rhythms, and glucose tolerance,
following sleep restriction in otherwise healthy young men. Despite
this, we did not observe any alterations in mRNA or protein content
associated with the regulation of these processes. Collectively, these
results highlight several potential mechanisms by which sleep re-
striction may lead to reduction in glucose tolerance. Importantly, these
same detrimental effects to mitochondrial function and SarcPS were
not observed when HIIE was performed during the period of sleep
restriction. While further research is still required, these data provide a
basis for the development of evidence-based health guidelines and
recommendations for those experiencing inadequate sleep by high-
lighting some of the underlying biological mechanisms that can be
targeted by therapeutic interventions such as exercise.",https://www.sciencedirect.com/science/article/pii/S2212877820301848?via%3Dihub,
"Inhibition of prostaglandin-degrading enzyme
15-PGDH rejuvenates aged muscle mass and strength","In the muscles of old mice, a prostaglandin degrading enzyme builds up. Inhibiting this enzyme can improve muscle mass, strength and exercise performance in mice.","Treatments are lacking for sarcopenia, a debilitating age-related skeletal muscle wasting syndrome. We identifed increased amounts of 15-hydroxyprostaglandin dehydrogenase (15-PGDH), the prostaglandin E2 (PGE2)–degrading enzyme, as a hallmark of aged tissues, including skeletal muscle. The consequent reduction in PGE2 signaling contributed to muscle atrophy in aged mice and results from 15-PGDH–expressing myofibers and interstitial cells, such as macrophages, within muscle. Overexpression of 15-PGDH in young muscles induced atrophy. Inhibition of 15-PGDH, by targeted genetic depletion or a small-molecule inhibitor, increased aged muscle mass, strength, and exercise performance. These benefits arise from a physiological increase in PGE2 concentrations, which augmented mitochondrial function and autophagy and decreased transforming growth factor–β signaling and activity of ubiquitin-proteasome pathways. Thus, PGE2 signaling ameliorates muscle atrophy and rejuvenates muscle function, and 15-PGDH may be a suitable therapeutic target for countering sarcopenia.","Currently there are no approved treatments for sarcopenia, the age-dependent loss of skeletal muscle mass and strength that constitutes a major public health problem affecting ~15% of individuals aged 65 or older. This dysfunction is due to aberrant protein and organelle turnover, inflammation, neuromuscular degeneration, and reduced mitochondrial function. Owing to its multifactorial etiology, untangling the causal molecular pathways to identify therapeutic targets to delay or reverse sarcopenia has proven challenging. Here we identify increased accumulation of the prostaglandin-degrading enzyme, 15- hydroxyprostaglandin dehydrogenase (15- PGDH), as a hallmark of aged tissues, including skeletal muscle, and show that it can be therapeutically targeted to enhance muscle mass and strength.","We identified the prostaglandindegrading enzyme, 15-PGDH, as a driver of
muscle atrophy. Overexpression of this enzyme
in young mice induced muscle loss, and shortterm inhibition overcame the muscle wasting
associated with aging. A major advantage of
our approach is that it restores PGE2 in aged
muscles to physiological levels characteristic of young muscles. The augmented PGE2
orchestrates several complementary signaling pathways, leading to increased mitochondrial numbers and function. Our findings
have broad relevance, as elevated 15-PGDH
expression is detected in a multiplicity of
aged tissues. The pleiotropic beneficial effects leading to the muscle rejuvenation
seen upon inhibition of 15-PGDH identify
the enzyme as a pivotal molecular determinant of aging that can be therapeutically
targeted to surmount the muscle weakness
associated with sarcopenia",https://www.science.org/doi/full/10.1126/science.abc8059,
"Chocolate for breakfast prevents
circadian desynchrony in
experimental models of jet-lag and
shift-work","Chocolate for breakfast supports a rat's circadian clock - but the same chocolate at dinner time can disrupt clocks. Btw, a prior study on coffee implied similar effect. ","Night-workers, transcontinental travelers and individuals that regularly shift their sleep timing, sufer
from circadian desynchrony and are at risk to develop metabolic disease, cancer, and mood disorders,
among others. Experimental and clinical studies provide evidence that food intake restricted to the
normal activity phase is a potent synchronizer for the circadian system and can prevent the detrimental
metabolic efects associated with circadian disruption. As an alternative, we hypothesized that a timed
piece of chocolate scheduled to the onset of the activity phase may be sufcient stimulus to synchronize
circadian rhythms under conditions of shift-work or jet-lag. In Wistar rats, a daily piece of chocolate
coupled to the onset of the active phase (breakfast) accelerated re-entrainment in a jet-lag model by
setting the activity of the suprachiasmatic nucleus (SCN) to the new cycle. Furthermore, in a rat model
of shift-work, a piece of chocolate for breakfast prevented circadian desynchrony, by increasing the
amplitude of the day-night c-Fos activation in the SCN. Contrasting, chocolate for dinner prevented
re-entrainment in the jet-lag condition and favored circadian desynchrony in the shift-work models.
Moreover, chocolate for breakfast resulted in low body weight gain while chocolate for dinner boosted
up body weight. Present data evidence the relevance of the timing of a highly caloric and palatable meal
for circadian synchrony and metabolic function.","Modern human society is exposed to a 7/24 activity schedule, leading individuals to low sleep quality and disrupted daily sleep-activity rhythms. It is known from shif-workers and frequent travelers ailing from jet-lag, that disturbed sleep-wake schedules create a confict between the circadian system and the temporal signals derived
from the cyclic environment, such as the light-dark cycle1. This is further supported by clinical and experimental findings indicating that a confict between external time signals and the internal temporal order transmitted by the suprachiasmatic nucleus (SCN) can lead to internal desynchrony2. Circadian organization is necessary to prepare the organisms for the daily challenges and requires a well-coordinated synchrony with the day-night cycle. Epidemiological and experimental studies indicate that constant exposure to situations that induce circadian disruption, increase the risk to develop overweight, metabolic diseases, cardiovascular problems and cancer. Diverse strategies are used to prevent circadian disruption, including scheduled melatonin administration, scheduled dexamethasone administration, exercise or scheduled feeding. Scheduled feeding has shown to be a
strong entraining signal for the body; when food intake is synchronized with the activity phase it exerts benefcial effects on the circadian system and metabolism. In experimental studies, timed food access restricted to the active phase accelerates resynchronization in a jet-lag model, prevents circadian desynchrony in a shif-work
model10 and induces healthy efects in metabolism. Conversely, food scheduled to the sleep/rest phase, exerts a disruptive infuence on circadian synchrony, altering metabolism and behavior. We have previously reported that scheduled access to chocolate, entrains brain areas involved in motivation and in the metabolic response to food. Scheduled chocolate also entrains the circadian system, enhancing the amplitude of neuronal activation in the SCN15. In the present study we hypothesized that a daily piece of chocolate scheduled in synchrony with the onset of the normal activity phase (breakfast) would be a powerful stimulus to prevent circadian disruption in experimental models of jet-lag and shif-work. Terefore, experimental models of jet-lag and shif-work were used to test the synchronizing efects of a piece of chocolate for breakfast or dinner on general activity, body temperature, daily c-Fos activation and metabolic indicators.","Te present study explored the efects of a piece of chocolate as a synchronizing factor to prevent circadian disruption under conditions of shif-work or jet-lag. We provide evidence that a piece of chocolate, a high caloric and
palatable meal, exerts benefcial efects on circadian function and body weight, when it is timed to the beginning of
the active phase (breakfast). Te circadian synchrony was associated with benefcial efects on metabolic rhythms
and body weight, thus may be an importat factor for maintaining circadian ftness in spite of the 24/7 modern
lifestyle. Further studies will need to demonstrate whether chocolate for breakfast can prevent comorbidities of
internal desynchronization such as the development of tumors, depression, metabolic syndrome, and others.",https://www.nature.com/articles/s41598-020-63227-w.pdf,
Circadian control of lung inflammation in influenza infection,"This mouse study shows mice are more susceptible to influenza related symptoms and death if they are exposed to the virus after waking up. If this holds in humans (BIG if), essential outdoor trips in the afternoon may be less harmful than in the morning. ","Influenza is a leading cause of respiratory mortality and morbidity. While inflammation is
essential for fighting infection, a balance of anti-viral defense and host tolerance is necessary
for recovery. Circadian rhythms have been shown to modulate inflammation. However, the
importance of diurnal variability in the timing of influenza infection is not well understood.
Here we demonstrate that endogenous rhythms affect survival in influenza infection. Circadian control of influenza infection is mediated by enhanced inflammation as proven by
increased cellularity in bronchoalveolar lavage (BAL), pulmonary transcriptomic profile and
histology and is not attributable to viral burden. Better survival is associated with a time
dependent preponderance of NK and NKT cells and lower proportion of inflammatory
monocytes in the lung. Further, using a series of genetic mouse mutants, we elucidate cellular
mechanisms underlying circadian gating of influenza infection.","Circadian rhythms constitute an innate anticipatory system
with a 24-h periodicity that improves survival by helping
the organism adapt to its surroundings. At the molecular
level, circadian rhythms are controlled by oscillating core-clock
genes, which regulate rhythmic expression of their downstream
targets. Many physiological processes, including immune
responses, are subject to circadian regulation. Inflammation is
a critical part of the immune response to influenza. While an
ineffective inflammatory response impedes viral clearance,
enhanced inflammation injures the host. Due to its role in
maintaining overall homeostasis, the circadian regulatory system
may act to balance antiviral resistance with host tolerance, in a
way that is favorable to overall survival.
Although the role of circadian regulation in systemic viral
infections8 has been described using Murid herpes virus (MHV)
and Vesicular Stomatis Virus (VSV), information on
respiratory viruses is limited. Some in vitro work9 and results of
a cluster-randomized study testing the efficacy of influenza vaccine in older adults are consistent with a role for circadian
rhythms in influenza infection. However, the importance of circadian rhythms in modulating lung inflammation induced by
influenza infection has yet to be systematically evaluated.
In this study, we test whether the outcome of influenza A
infection (IAV), viral burden, and pulmonary inflammation were
regulated by circadian rhythms. We utilize a series of genetic
mouse mutants to understand the cellular mechanism underlying
this regulation.","In conclusion, our work demonstrates that time-dependent
regulation of influenza infection and its consequences are mediated by circadian regulation of host tolerance pathways and not
directly through viral replication. This temporal difference in
outcomes based on time of inoculation is consistent with recent
trials of vaccination that demonstrate that time of day affects
antibody responses. However, the findings should have
broader relevance for other respiratory pathogens and circadian
regulation of host–pathogen interaction. A body of work, both
mechanistic and epidemiological have shown that shift workers,
who experience circadian disruption are at increased risk for
health issues, including metabolic syndrome, cardiac diseases, and cancer. Based on our results here, we would
extend these possibilities to more acute conditions such as
respiratory infections. We also suspect that the role of understanding and harnessing circadian regulation in disease states is
further underscored by deliberate changes in our lifestyle wherein
social jet lag is the normative, rather than the deviant. Finally, we speculate that perturbation of circadian rhythms in intensive care
units (whether through lighting, noise, disruptive timing of food,
clinical assessments, or medications) may all potentially worsen
the inflammation in patients afflicted with respiratory pathogens.",https://www.nature.com/articles/s41467-019-11400-9.pdf,
Circadian control of the secretory pathway maintains collagen homeostasis,"A sacrificial pool of collagen is maintained by circadian clock. This pool is synthesized, assembled and degraded every 24h. This may help recovery from damage to collagen.","Collagen is the most abundant secreted protein in vertebrates and persists throughout life without renewal. The permanency of collagen networks contrasts with both the continued synthesis of collagen throughout adulthood and the conventional transcriptional/translational homeostatic mechanisms that replace damaged proteins with new copies. Here, we show circadian clock regulation of endoplasmic reticulum-to-plasma membrane procollagen transport by the sequential rhythmic expression of SEC61, TANGO1, PDE4D and VPS33B. The result is nocturnal procollagen synthesis and daytime collagen fibril assembly in mice. Rhythmic collagen degradation by CTSK maintains collagen homeostasis. This circadian cycle of collagen synthesis and degradation affects a pool of newly synthesized collagen, while maintaining the persistent collagen network. Disabling the circadian clock causes abnormal collagen fibrils and collagen accumulation, which are reduced in vitro by the NR1D1 and CRY1/2 agonists SR9009 and KL001, respectively. In conclusion, our study has identified a circadian clock mechanism of protein homeostasis wherein a sacrificial pool of collagen maintains tissue function.","One-third of the eukaryote proteome enters the secretory pathway1, including the collagens that assemble into centimetre-long fibrils in the extracellular matrix (ECM)2. These fibrils account for one-third of the mass of vertebrates3 and are the sites of attachment for a wide range of macromolecules, including integrins, making them essential for metazoan development3. A remarkable feature of collagen fibrils is that they are formed during embryogenesis4 and remain without turnover for the life of the animal5,6,7,8. This has led to the idea that collagen fibrils are static and unchanging. However, the difficulty with zero turnover is that it does not explain the absence of fatigue failure, which would be expected in the face of life-long cyclic loading. In contrast to the evidence of zero replacement, fibroblasts synthesize collagen in response to mechanical loading9, and microdialysis of human Achilles tendon shows elevated levels of the C-propeptides of procollagen-I (PC-I; the precursor of collagen-I) after moderate exercise10.

These opposing observations led to the alternative hypothesis presented in this study, in which zero turnover and continued synthesis can coexist. We hypothesized that a pool of ‘persistent’ collagen coexists with a pool of ‘sacrificial’ collagen, in which the latter is synthesized and removed on a daily basis under the control of the circadian clock. Support for this alternative hypothesis comes from observations of a circadian oscillation in the serum concentrations of the C-propeptides of PC-I11 and of collagen degradation products in bone12. However, despite physiological and clinical observations, direct mechanistic support for these observations was lacking. Although the suprachiasmatic nucleus of the hypothalamus is the master circadian pacemaker, almost all tissues have self-sustaining circadian pacemakers that synchronize rhythmic tissue-specific gene expression in anticipation of environmental cycles of light and dark13. Disruption of the circadian clock leads to musculoskeletal abnormalities—for example, chondrocyte-specific disruptions of the circadian clock result in progressive degeneration of articular cartilage14 and fibrosis in the intervertebral disc15, and mice with a global knockout of Bmal1 (ref. 16) or the ClockΔ19 mutation17 develop thickened and calcified tendons with associated immobilization. These observations are indicative of circadian control of ECM homeostasis.

Here, we performed time-series electron microscopy, transcriptomics and proteomics over day–night cycles, which showed that the synthesis and transport of PC-I by the protein secretory pathway in fibroblasts is regulated by the circadian clock. We show that SEC61, TANGO1, PDE4D and VPS33B regulate collagen secretion, are 24-h rhythmic, and are located at the entry and exit points of the endoplasmic reticulum (ER), Golgi and post-Golgi compartments, respectively. CTSK is a collagen-degrading proteinase, which is rhythmic in-phase with collagen degradation to maintain collagen homeostasis. The result is nocturnal PC-I synthesis and a daily wave of collagen-I with no net change in the total collagen content of the tissue. Crucially, we discovered that arrhythmic ClockΔ19 and Scleraxis–Cre-dependent Bmal1-deletion mutant mice accumulate collagen and have a disorganized and structurally abnormal collagen matrix that is mechanically abnormal. Finally, we show that ClockΔ19 fibroblasts in vitro amass collagen fibres compared with control cells and treatment of ClockΔ19 fibroblasts with the NR1D1 agonist SR9009 (ref. 18) or the cryptochrome (CRY1/2) agonist KL001 (ref. 19) reduces the number of collagen fibres. Wild-type fibroblasts treated with KL001 lose their circadian rhythm and generate more collagen fibres. Together, these results provide insights into the importance of the circadian clock in maintaining collagen homeostasis.

","In this study we have identified a sacrificial pool of collagen that, via circadian clock regulation of the protein secretory pathway, is synthesized and removed while leaving the bulk collagen unchanged. Together with rhythmic variation of the collagen fibril diameters, this illustrates a surprisingly dynamic matrix and a selective mechanism of protein homeostasis that maintains tissue function and organization.

Intracellular proteins are constantly being synthesized and degraded to replace old and damaged polypeptides with newly synthesized copies44. However, the extreme longevity of collagen in tissues would suggest that these conventional transcriptional and translational homeostatic mechanisms do not apply to collagen. Presumably, the energy and time required to establish extensive networks of collagen fibrils (which is approximately 17 yr in humans7) rule out a homeostatic mechanism in which all of the collagen is turned over. Collagen-rich tissues are exposed to repeated cycles of stress and strain. Collagen molecules that are never replaced would thus be expected to undergo mechanical damage, leading to fatigue failure. A mechanism of protein homeostasis must therefore exist to protect, rather than replace, the collagen that was assembled during growth. Such a mechanism must be able to account for the observed continued synthesis in the face of zero turnover of bulk collagen. The circadian clock mechanism described here would seem to fulfil these requirements.

Our study showed that a population of thin (D1) collagen fibrils are interspersed between thick (D2 and D3) fibrils and makes frequent contact with the surfaces of the thick fibrils. It has been suggested that the thin fibrils are part of a sheer-stress loading mechanism in tendons45. Our data would agree with this suggestion, particularly as the D1 fibrils do not seem to be under tensile strain. However, the fact that the proportion of D1 fibrils varies with the time of day might suggest an additional function in maintaining matrix homeostasis in the face of bouts of activity and long-term wear and tear on the tissue. In ClockΔ19 and Scx::Bmal1 lox/lox mice with a defective circadian clock, the D2 and D3 fibrils had irregular profiles and the tendons were thicker, weaker and had reduced elastic modulus. Thus, D1 fibrils, their interaction with D2 and D3 fibrils, and their regulation by the circadian clock, are important determinants of tendon-tissue integrity and function.

Collagen fibrils are ‘molecular alloys’ that comprise a variety of collagen types46 and associated molecules, including small leucine-rich proteoglycans47. We noted that collagen α2(V) was rhythmic in-phase with collagen α1(I) and α2(I). Collagen-V controls the initiation of collagen fibril assembly, and haplo-insufficient mice have fewer, large and irregular collagen fibrils48. In addition, decorin (a small leucine-rich proteoglycan found in a wide range of tissues) was rhythmic, peaking at CT21 (ProteomeXchange accession no. PXD013450). Mice deficient in decorin have weakened tissues and abnormal collagen fibrils with irregular profiles49. This raises the possibility that the rhythmic collagen-I-containing D1 fibrils are part of a multi-component chronomatrix.

Finally, we propose that extrinsic control of the secretory pathway and the circadian clock may be effective at regulating collagen homeostasis in the treatment of disease, such as promoting collagen synthesis during wound healing or inhibiting collagen synthesis in fibroproliferative diseases. KL001 nullified the circadian clock in wild-type fibroblasts, which was associated with increased steady-state levels of Sec61a2, Mia3, Pde4d and Vps33b transcripts as well as increased numbers of collagen fibres per cell in vitro. Conversely, KL001 resulted in decreased levels of Sec61a2, Pde4d and Vps33b transcripts and fewer collagen fibres per arrhythmic ClockΔ19 cell. Thus, the targeted control of the secretory pathway and circadian clock may provide new opportunities to modulate ECM secretion in the maintenance of long-term tissue health.",https://www.nature.com/articles/s41556-019-0441-z#Abs1,
Association Between Push-up Exercise Capacity and Future Cardiovascular Events Among Active Adult Men,How many push-ups you can do can predict future heart diseases. Being able to do >10 push-ups makes a big difference. ,"Cardiovascular disease (CVD) remains the leading cause of mortality worldwide. Robust evidence indicates an association of increased physical fitness with a lower risk of CVD events and improved longevity; however, few have studied simple, low-cost measures of functional status. To evaluate the association between push-up capacity and subsequent CVD event incidence in a cohort of active adult men. Retrospective longitudinal cohort study conducted between January 1, 2000, and December 31, 2010, in 1 outpatient clinics in Indiana of male firefighters aged 18 years or older. Baseline and periodic physical examinations, including tests of push-up capacity and exercise tolerance, were performed between February 2, 2000, and November 12, 2007. Participants were stratified into 5 groups based on number of push-ups completed and were followed up for 10 years. Final statistical analyses were completed on August 11, 2018. Cardiovascular disease–related outcomes through 2010 included incident diagnoses of coronary artery disease and other major CVD events. Incidence rate ratios (IRRs) were computed, and logistic regression models were used to model the time to each outcome from baseline, adjusting for age and body mass index (BMI) (calculated as weight in kilograms divided by height in meters squared). Kaplan-Meier estimates for cumulative risk were computed for the push-up categories. A total of 1562 participants underwent baseline examination, and 1104 with available push-up data were included in the final analyses. Mean (SD) age of the cohort at baseline was 39.6 (9.2) years, and mean (SD) BMI was 28.7 (4.3). During the 10-year follow up, 37 CVD-related outcomes (8601 person-years) were reported in participants with available push-up data. Significant negative associations were found between increasing push-up capacity and CVD events. Participants able to complete more than 40 push-ups were associated with a significantly lower risk of incident CVD event risk compared with those completing fewer than 10 push-ups (IRR, 0.04; 95% CI, 0.01-0.36). The findings suggest that higher baseline push-up capacity is associated with a lower incidence of CVD events. Although larger studies in more diverse cohorts are needed, push-up capacity may be a simple, no-cost measure to estimate functional status.","Cardiovascular disease (CVD) remains the leading cause of death worldwide. In addition to long-recognized risk factors for CVD, such as smoking, hypertension, and diabetes, the unfavorable health consequences of physical inactivity on cardiovascular health have been well established. Studies have suggested that physical activity provides cardiovascular benefits independent of other modifiable CVD risk factors associated with a lower incidence of multiple diseases, including CVD, diabetes, cancer, and Alzheimer disease. A recent US study further suggested that moderate to vigorous physical activity could significantly reduce premature mortality and prolong life expectancy.10 Given this robust scientific evidence, the American Heart Association added physical activity to its My Life Check—Life’s Simple 7 campaign to reduce the burden of CVD and improve overall health.

In multiple recent scientific and policy statements, the American Heart Association has promoted assessment of physical activity in clinical settings and workplaces. Ross et al and Golightly et al have also emphasized the growing evidence for objectively assessing cardiorespiratory fitness (CRF) as a vital sign in health care settings. However, unlike anthropometric measurements and serum biomarkers, physical activity and CRF assessments have largely been neglected by clinicians. The most commonly used physical activity assessments are the patient’s self-reported history and health and lifestyle questionnaires. However, objectively measured CRF levels are often significantly lower than expected based on self-reported physical activity. Although good performance on accurate and objective CRF assessment tools such as exercise tolerance tests has been inversely associated with future CVD, these examinations are expensive, time-consuming, and often require professional facilities and trained personnel to administer. The use of these tools remains limited to particular occupations and targeted patient populations. To our knowledge, no study has examined the association of push-up capacity, a simple, no-cost, surrogate measure of functional status, with future cardiovascular events. In this study, we examined baseline performance on commonly performed physical fitness assessments (push-up capacity and submaximal treadmill tests) and its association with subsequent incident CVD events in a cohort of occupationally active men. We hypothesized that higher fitness levels would be associated with lower rates of incident CVD.","In this 10-year longitudinal study, participants able to complete more than 40 push-ups were associated with a significant reduction in incident CVD event risk compared with those completing fewer than 10 push-ups, which may be explained by significant differences in recognized CVD risk factors at baseline among the groups. The findings suggest that being able to perform a greater number of push-ups at baseline is associated with a lower incidence of CVD events among active adult men. Thus, results from this study suggest that it is reasonable for clinicians to assess functional status during clinical evaluations by using basic questions regarding activity. Further research is warranted to determine the association of push-up capacity with CVD risk in the general population and the potential use of push-ups as a clinical assessment tool.",https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2724778,
Circadian Entrainment Triggers Maturation of Human In Vitro Islets,Even cells in culture need Time-Restricted Feeding or fasting feeding cycle to function properly. Human in vitro islets mature better under feeding-fasting cycle.  This may prompt cell biologists to use circadian entrainment in cell culture studies.,"Stem-cell-derived tissues could transform disease
research and therapy, yet most methods generate
functionally immature products. We investigate how
human pluripotent stem cells (hPSCs) differentiate
into pancreatic islets in vitro by profiling DNA methyl-
ation, chromatin accessibility, and histone modifica-
tion changes. We find that enhancer potential is reset
upon lineage commitment and show how pervasive
epigenetic priming steers endocrine cell fates.
Modeling islet differentiation and maturation regula-
tory circuits reveals genes critical for generating
endocrine cells and identifies circadian control as
limiting for in vitro islet function. Entrainment to circa-
dian feeding/fasting cycles triggers islet metabolic
maturation by inducing cyclic synthesis of energy
metabolism and insulin secretion effectors, including
antiphasic insulin and glucagon pulses. Following
entrainment, hPSC-derived islets gain persistent
chromatin changes and rhythmic insulin responses
with a raised glucose threshold, a hallmark of func-
tional maturity, and function within days of transplan-
tation. Thus, hPSC-derived tissues are amenable to
functional improvement by circadian modulation.","Generating functional stem-cell-derived tissues is a major goal of
regenerative medicine, yet current strategies yield products that
often fail to recapitulate endogenous cellular function (Sances
et al., 2016; Sneddon et al., 2018; Yang et al., 2014). Our group
and others advanced methods to differentiate human pluripotent
stem cells (hPSCs) into pancreatic islet cells (Pagliuca et al.,
2014; Rezania et al., 2014; Russ et al., 2015). These comprise
SC-b (hPSC-derived b) cells, among other cell types (Pagliuca
et al., 2014; Sharon et al., 2019; Veres et al., 2019), including
insulin+ glucagon+ polyhormonal cells (PH). Better control over
deriving all islet cell types is limited by incomplete understanding
of the mechanisms driving endocrine lineage specification.
hPSC-derived islets (SC-islets) show glucose-responsive in-
sulin release and cure diabetic rodents, yet they lack the full
magnitude of responsiveness shown by mature islets in vivo.
Mature responsiveness develops postnatally as insulin secretion
capacity and the glucose threshold for secretion increase
(Aguayo-Mazzucato et al., 2006; Blum et al., 2012; Stolovich-
Rain et al., 2015). While factors promoting islet maturation
have been described (e.g., MAFA and NEUROD1), the underly-
ing mechanisms remain unclear (Liu and Hebrok, 2017).
Here, we use epigenome analyses to better understand mech-
anisms driving human islet differentiation and maturation.
We define regulatory landscapes, the putative pioneer factors
that establish them, and their state dynamics throughout islet
development. We find that enhancer turnover occurs mainly
upon lineage commitment, which is foreshadowed by wide-
spread yet dynamic enhancer priming. Accordingly, we show
that priming of a-cell-specific enhancers steers PH toward an
a-cell fate. Modeling core regulatory circuits (CRCs) across islet
differentiation and maturation stages captures both known and
unexpected regulators such as LMX1B, which we validate as
critical for generating endocrine cells. Finally, contrasting regula-
tory landscapes in SC-b and mature b cells reveals a role for
circadian rhythms in fostering mature glucose responsiveness.
Entrainment to daily feeding/fasting cycles activates islet clocks
and elicits rhythmic transcription of energy metabolism and in-
sulin synthesis/transport/release genes, triggering metabolic
rhythms and cyclic insulin responses with a higher glucose
threshold, thus recapitulating an aspect of postnatal maturation.
Clock-entrained SC-islets gain stable epigenetic changes at
genes enabling mature insulin responses and sustain function
from as early as 3 days after in vivo transplantation. These
data, available via an online resource (http://meltonlab.rc.fas.
harvard.edu/data/pancreatic_enhancers/), reveal mechanisms
controlling human islet development and illustrate how clock
modulation can be harnessed to further functiona maturation
of a stem-cell-derived product.","Overall, this study provides insights that underpin human
islet development and function. These insights may be valu-
able in improving b cell programing strategies and under-
standing how disrupted genetic circuits contribute to meta-
bolic diseases, including diabetes. An intriguing possibility
is that circadian entrainment may be harnessed to further
functional maturation of other stem-cell-derived products
(Sances et al., 2016; Sneddon et al., 2018; Yang et al.,
2014), consistent with the ability of clock controllers to bind
distinct targets in distinct tissues (Koike et al., 2012; Perelis
et al., 2015). Thus, our general approach may inform attempts
to control the fate and function of any human cell type.",https://www.sciencedirect.com/science/article/pii/S1934590919304667?via%3Dihub,
Sleep Restriction Enhances the Daily Rhythm of Circulating Levels of Endocannabinoid 2-Arachidonoylglycerol," 4 nights of short sleep ramps up the body’s own (endogenous) production of cannabinoids. A new mechanism (alongside leptin & ghrelin) explaining why a lack of sleep = excessive food intake, and the link between short sleep & obesity","Increasing evidence from laboratory and epidemiologic studies indicates that insufficient sleep may be a risk factor for obesity. Sleep curtailment results in stimulation of hunger and food intake that exceeds the energy cost of extended wakefulness, suggesting the involvement of reward mechanisms. The current study tested the hypothesis that sleep restriction is associated with activation of the endocannabinoid (eCB) system, a key component of hedonic pathways involved in modulating appetite and food intake.","Rates of overweight and obesity have increased rapidly over the past three decades.1 Concurrently, average sleep times have decreased.2 Polls conducted by the Centers for Disease Control and Prevention (CDC) and National Sleep Foundation (NSF), as well as large-scale epidemiological studies, indicate that many Americans experience chronic partial sleep restriction because of voluntary bedtime curtailment.3 A growing body of evidence suggests a link between insufficient sleep duration and increased risk of obesity. A metaanalysis of cross-sectional epidemiologic studies including more than 600,000 adults from around the world found a significant pooled odds ratio for short duration of sleep and obesity of 1.55.4 The majority of prospective studies, whether in children or adults, found that short sleep predicts greater weight gain or the incidence of obesity, after controlling for multiple confounders.5–9 Experimental sleep restriction in the laboratory (4–6 h/night for multiple nights) has adverse effects on insulin sensitivity and glucose tolerance10–12 and increases hunger and food intake.13,14 Further, in a study where caloric intake was strictly controlled, plasma concentrations of the satiety hormone leptin were suppressed and concentrations of the appetite-stimulating hormone ghrelin were elevated, in association with a stimulation of hunger and appetite. Recent studies using whole room indirect calorimetry have shown that sleep restriction does increase energy need, but only modestly.16–18 By comparison, the increased hunger and appetite reported by sleep deprived subjects and their increased energy intake in the presence of ad libitum feeding appear to exceed the energy demands of extended  wakefulness under sedentary conditions.13–15,18,19 These findings suggest a role for alterations in hedonically driven hunger and appetite as a result of insufficient sleep. The endocannabinoid (eCB) system is involved in the control of feeding, appetite, and energy homeostasis. The eCB system is composed of cannabinoid CB1 and CB2 receptors, the endogenous agonists of these receptors 2-arachidonoylglycerol (2-AG) and N-arachidonylethanolamine (anandamide, AEA), and the enzymes required for the biosynthesis and degradation of the endogenous lipids. The appetite-enhancing effects of cannabinoids appear to be mediated mainly by the CB1 receptor. Production of 2-AG is more than 100-fold larger than that of AEA. The eCBs, measurable in plasma,20 are biosynthesized de novo and bind to CB1 receptors in brain and in peripheral organs involved in energy metabolism, including adipose tissue, endocrine pancreas, muscle, and liver.21–24 It is well established that eCB-dependent CB1 receptor activation is a potent orexigenic signal; agonists of CB1 receptors stimulate feeding, whereas antagonists result in appetite suppression.21 CB1 receptors are found in homeostatic pathways including hypothalamic nuclei known to modulate energy homeostasis via interaction with peripheral peptides (i.e., leptin and ghrelin).25,26 Moreover, activation of the eCB system affects hedonic (motivation and reward) circuits in the mesolimbic system, including the nucleus accumbens and ventral tegmental area. Here eCBs interact with dopamine and opioid pathways, eliciting a preference for highly palatable
rewarding food.27–29 There are remarkable parallels between the effects of activation of the eCB system and the effect of experimental sleep restriction. Just as in a state of sleep debt, higher CB1 receptor activity increases feeding behavior in excess of energy need,
reduces glucose tolerance, tends to reduce leptin levels and to promote ghrelin release, and stimulates reward and addiction. We therefore hypothesized that sleep restriction activates
the eCB system and assessed the 24-h profile of circulating concentrations of the most abundant eCB, 2-AG, and of its structural analog 2-OG, in nonobese healthy individuals who participated in a randomized crossover study comparing 4 days of bedtime restriction to 4.5 h per night to 4 d of 8.5 h bedtimes. We have previously reported the existence of a robust circadian rhythm in circulating eCB concentrations under normal sleep conditions.",: Our findings suggest that activation of the eCB system may be involved in excessive food intake in a state of sleep debt and contribute to the increased risk of obesity associated with insufficient sleep.,pubmed.ncbi.nlm.nih.gov/26612385/,
Sleep deprivation impairs molecular clearance from the human brain,"A night without sleep impairs clearance of molecular detritus within the human brain. This was not compensated for by a recovery night of sleep. Editorial note suggests that in this regard: ""humans do not catch up on lost sleep.","It remains an enigma why human beings spend one-third of their life asleep. Experimental data suggest that sleep is required for clearance of waste products from brain metabolism. This has, however, never been verified in humans. The primary aim of the present study was to examine in vivo whether one night of total sleep deprivation affects molecular clearance from the human brain. Secondarily, we examined whether clearance was affected by subsequent sleep. Multiphase MRI with standardized T1 sequences was performed up to 48 h after intrathecal administration of the contrast agent gadobutrol (0.5 ml of 1 mmol/ml), which served as a tracer molecule. Using FreeSurfer software, we quantified tracer enrichment within 85 brain regions as percentage change from baseline of normalized T1 signals. The cerebral tracer enrichment was compared between two cohorts of individuals; one cohort (n = 7) underwent total sleep deprivation from Day 1 to Day 2 (sleep deprivation group) while an age and gender-matched control group (n = 17; sleep group) was allowed free sleep from Day 1 to Day 2. From Day 2 to 3 all individuals were allowed free sleep. The tracer enriched the brains of the two groups similarly. Sleep deprivation was the sole intervention. One night of sleep deprivation impaired clearance of the tracer substance from most brain regions, including the cerebral cortex, white matter and limbic structures, as demonstrated on the morning of Day 2 after intervention (sleep deprivation/sleep). Moreover, the impaired cerebral clearance in the sleep deprivation group was not compensated by subsequent sleep from Day 2 to 3. The present results provide in vivo evidence that one night of total sleep deprivation impairs molecular clearance from the human brain, and that humans do not catch up on lost sleep.","Sleep is essential for human life, including cognitive function (Rasch and Born, 2013), but it remains a mystery why man spends about one-third of life asleep. Experimental evidence has shown that sleep has a restorative function by facilitating clearance of metabolic waste products from the brain that accumulate during wakefulness (Xie et al., 2013). With two-photon microscopy, it was found that sleep increased the brain interstitial volume fraction by 60%, allowing for a 2-fold faster clearance of amyloid-β from the cortex. The observations echoed with reports of disrupted sleep in the preclinical stage of Alzheimer’s disease (Moran et al., 2005), a disease in which amyloid-β and tau aggregation in susceptible brain areas develop long before onset of clinical dementia. Severe sleep disturbances also accompany traumatic brain injury (Mathias and Alvaro, 2012), where patients suffer from increased cerebral tau and amyloid-β burden and risk of Alzheimer’s disease (Johnson et al., 2012). It was previously demonstrated in a mouse Alzheimer’s disease model that interstitial levels of amyloid-β, a metabolic by-product of neuronal activity, increased after acute sleep deprivation, while chronic sleep deprivation increased amyloid-β formation (Kang et al., 2009). Others also reported that sleep deprivation increased the amount of soluble amyloid-β and the risk of amyloid-β plaque formation in mice (Roh et al., 2012). The levels of tau were also increased in the interstitial fluid of the hippocampus following sleep deprivation (Holth et al., 2019). In healthy humans, undisturbed sleep caused 6% reduction in CSF amyloid-β42 levels while remaining unchanged after one night of total sleep deprivation (Ooms et al., 2014). An amyloid-β PET study showed that one night of sleep deprivation increased parenchymal amyloid-β burden by 5% in 20 healthy individuals (Shokri-Kojori et al., 2018). More recently, a direct link between sleep-related neuronal activity and CSF and blood flow was indicated based on observations that slow-wave sleep was accompanied with large-amplitude CSF flow as compared with the awake state, and an inverse relationship between CSF flow and blood flow (Fultz et al., 2019). It has, however, never been demonstrated in vivo whether sleep, or sleep deprivation, affects molecular clearance from the human brain.

The present study was undertaken to examine the effect of one night of total sleep deprivation on molecular clearance from the human brain. The MRI contrast agent gadobutrol (Gadovist®, Bayer) was used as tracer molecule to enrich brain tissue via intrathecal administration in CSF (Ringstad et al., 2018). Gadobutrol is a highly hydrophilic molecule with molecular weight of 604 Da, hydraulic diameter of ∼2 nm; and distributes freely within the brain, confined to the extravascular compartment by the blood–brain barrier (Ringstad et al., 2018). Gadobutrol as a CSF tracer may therefore be considered a surrogate marker for assessing transport of water-soluble metabolites excreted along extravascular pathways within the brain, including amyloid-β and tau. The MRI research protocol included standardized T1-weighted MRI scanning before and after intrathecal gadobutrol at predefined time points during Day 1 and at 24 h (next morning), 48 h (the morning after) and at 4 weeks (Supplementary Fig. 1). Following completion of imaging, the entire brain was analysed in FreeSurfer, allowing for the assessment of 85 subregions.","In conclusion, we provide in vivo evidence that one night of total sleep deprivation impairs molecular clearance from the human brain, an effect not compensated for by another night’s sleep. The results support the hypothesis that the interstitial space increases in the sleeping human brain, as previously demonstrated in rodents. As such, the observations have implications for understanding the impact of disturbed sleep in the evolvement of neurodegenerative disease, and may point to avenues for enhancing endogenous molecular transport and delivery of intrathecal drugs.",https://academic.oup.com/brain/article/144/3/863/6214917?login=false,
Changing school start times: impact on sleep in primary and secondary school students,"Before delaying school start times: ave wake-up time for high schoolers: 5:46AM. Weekends: 09:16AM = Social jet lag similar to flying back-forth from LA to NYC every weekend. And, 3 out of every 4 kids were suffering clinical-grade daytime sleepiness. ",To examine the impact of changing school start times on sleep for primary (elementary school: ES) and secondary (middle and high school: MS/HS) students.,"Sleep is essential for optimal health and development, academic achievement, and social and emotional functioning [1–3], yet insufficient sleep is common among children and adolescents. To address this public health concern, the goal of increasing sleep duration for not only adolescents, but for children of all ages, was recently identified as a target for Healthy People 2030 [4]. For secondary school students, healthy school start times (no earlier than 8:30 am) have been identified by most major professional associations, including the American Academy of Pediatrics [5], American Academy of Sleep Medicine [6], American Medical Association [7], American Psychological Association [8], National Association of School Nurses [9], and National Parent Teacher Association [10], as a modifiable policy to improve adolescent health and well-being. The need for, and benefits of, later secondary school start times is well documented [1, 11, 12]. However, it is notable that <21% of middle schools and <18% of US high schools start at 8:30 am or later [13, 14]. Thus, further evidence supporting later start times for adolescents is needed.

In order to achieve later secondary school start times, it is often necessary for primary school students to start earlier, especially if the district has a staggered transportation schedule and has only one set of buses that transports all students [15]. Based on biological changes to human circadian rhythms during puberty, early sleep onset is difficult for adolescents, whose sleep is then truncated with early school start times, resulting in deficient sleep [16]. Primary school students, on the other hand, typically go to bed and wake earlier than secondary school students [17, 18]. Furthermore, biological changes in the circadian rhythm and self-reported delays in circadian preference (i.e. time one feels most alert and awake) typically begin between 11 and 13 years, when students in the United States are transitioning from elementary to middle school [19, 20]. Studies have shown an increase in sleep duration for primary school-age students based on changing to an earlier bed time and maintaining a consistent wake time [21–23]. Together, these studies suggest primary school students are able to fall asleep earlier if they have an earlier bedtime set for them, allowing them the opportunity to obtain sufficient sleep duration, even with an earlier school start time. However, bedtimes are often set by parents, and are part of greater family routines and schedules. Thus, even though primary school students are biologically able to fall asleep earlier than adolescents, few studies have considered whether the implementation of earlier primary school start times (as a result of delayed secondary school start times) results in changes to sleep routines and sleep duration.

Only two studies in the United States have prospectively examined how district changes to start times impact primary school students’ sleep [24, 25]. Qualitative findings from a large, diverse urban district in Minnesota reported that moving to later start times (8:40–9:40 am) negatively impacted transportation, student behavior, staff meetings, morning teaching and learning, and afternoon student fatigue. Schools that moved earlier (9:40 am to either 8:40 am or 7:40 am) reported students benefitting from fewer transitions before school, having fewer behavior problems, and being more alert and focused during the day [24]. In the northeast United States, minimal changes in sleep were found after moving start times earlier (3rd grade: 9:10–7:45 am, 4th and 5th grade: 8:20–7:45 am). Students reported earlier bedtimes and wake times, increased sleep duration for 3rd graders (24 min), minimally decreased sleep duration for 4th and 5th graders (4 and 9 min, respectively), and no change in daytime sleepiness [25]. However, the study sample included only one school that was predominantly white (97.8%) and not economically diverse. No studies have concurrently considered the impact on changing start times on sleep for students across grades from Kindergarten through 12th grade.

Finally, questions also remain about whether the impact of changing school start times is consistent across different racial and economic groups of students. Many studies that have looked at the impact of changes to school start times on sleep have focused on schools where the majority of students were non-Hispanic white and/or did not qualify for free or reduced lunch (FRL), a proxy schools often use for poverty (e.g. Refs. [25–28]). Across studies, even those with somewhat more diverse samples, the impact of changing start times on sleep by race or FRL status has not been considered. However, both racial minority and low socioeconomic status students at all levels may be disproportionately impacted by early start times (e.g. unable to get to school if they oversleep and miss the bus) [29], thus, it is important to consider these variables.

As a result, there remains a significant need for rigorous, longitudinal research that includes large, diverse samples, to demonstrate how this important health policy impacts all students from Kindergarten through 12th grade. In August 2017, the Cherry Creek School District (CCSD), a diverse district of ~55,000 students in suburban Denver, Colorado, changed school start times, delaying high schools (HS, grades 9–12, typical ages 14–18 years) by 70 min (to 8:20 am) and middle schools (MS, grades 6–8, typical ages 11–14 years) by 40–60 min (to 8:50 am), while moving elementary schools (ES, grades K-5, typical ages 5–11 years) 60 min earlier (to start at 8:00 am) [30]. To address knowledge gaps noted above, the Changing Start Times: Longitudinal Effects Study (CaSTLES) was developed to evaluate outcomes in the CCSD before and for 2 years after the start time change. The aim of this paper is to examine the impact of changing school start times on student sleep, including bedtime, wake time, sleep duration, sleep quality, and daytime sleepiness on students K-12.","CaSTLES, a longitudinal study that includes a large, diverse sample, provides both complementary and novel findings to the literature on changing school start times. For secondary school students, recommended healthy school start times (at or after 8:30 am) result in increased sleep duration and decreased daytime sleepiness. In order to achieve these outcomes however, for many districts, “flipping” primary and secondary school start times is required to accommodate transportation schedules. CaSTLES findings support the option of moving primary school start times earlier, although future studies are needed to determine the optimal start time for younger students. Furthermore, when logistically and financially feasible, a uniform later school start time would be ideal for students and families.

Although recently passed legislation (CA Senate Bill 328) will implement healthy start times for all California secondary school students (starting fall 2022), many districts across the United States are still determining whether they should also change their bell schedules. This study provides critical evidence of how a single policy initiative, healthy school start times, is a significant and effective way to improve sleep duration and daytime sleepiness for large numbers of secondary school students. However, additional rigorous studies are needed to continue answering the many questions raised by changing school start times, most notably, how early is too early for primary school students to start their school day. Finally, as a sufficient sleep opportunity is critical for all students, education about the importance of healthy sleep patterns needs to be developed and disseminated to all families. In order to ensure that education programs are sensitive to factors that may contribute to sleep disparities (e.g. activities, parent or student work schedules, shared bedrooms), these programs should be developed in partnership with diverse students and parents.",https://academic.oup.com/sleep/article/44/7/zsab048/6218366?login=false,
Obstructive sleep apnea treatment and dementia risk in older adults,"Treating sleep apnea associated with more than a 20% reduced risk of developing Alzheimer’s disease, and the progressive declining stages into Alzheimer’s disease.","To examine associations between positive airway pressure (PAP) therapy, adherence and incident diagnoses of Alzheimer’s disease (AD), mild cognitive impairment (MCI), and dementia not otherwise specified (DNOS) in older adults.","Obstructive sleep apnea (OSA) is a common disorder characterized by repeated episodes of upper airway obstruction, sleep fragmentation, and hypoxia during sleep. This disorder is associated with many serious health and socioeconomic consequences, including cognitive impairment [1].

The risk of OSA increases with age [2–4]. Up to 56% of older Americans may suffer from OSA, even though the majority remain undiagnosed [5, 6]. Older individuals are also more likely to experience many of the health consequences that are associated with OSA, including adverse neurocognitive outcomes [7]. Prior work has revealed compelling relationships between OSA and objective measures of cognitive performance (e.g. executive function, attention, and memory) [8–12], both incident dementia [13] and Alzheimer’s disease (AD) [14], as well as mild cognitive impairment (MCI) [15]. Still, data regarding the potential benefits of OSA treatment on cognitive performance are inconclusive [16–20], and any associations between OSA treatment and risk of incident neurodegenerative conditions, in particular, are lacking.

Although some evidence suggests a possible therapeutic benefit of positive airway pressure (PAP) therapy on cognitive performance in older individuals [21], advancement of knowledge on this topic has been limited by the use of small, single-center, or regional samples, heterogeneity in outcome measures, and limited enrollment of older individuals. Furthermore, the role of OSA treatment in the development of specific neurodegenerative conditions such as AD—the most frequent cause of dementia in older adults—has not been sufficiently studied. Specifically, whether the use of PAP therapy in older adults with OSA mitigates the risk of development of MCI or dementia syndromes remains unknown, as longitudinal, population-level data are lacking.

The purpose of this study was to determine, within a cohort of older Americans with OSA, longitudinal associations between OSA treatment and odds of new dementia diagnosis. We hypothesized that older adults who received and adhered to treatment for OSA (PAP therapy) would have a lower risk of incident diagnoses of MCI, AD, or other dementia subtypes compared to untreated or nonadherent individuals.","This population-based data highlight a potentially protective role for PAP therapy on short-term dementia risk in older adults with OSA. Additional research is necessary to explore mechanisms that may underlie this association; however, if a causal pathway exists, treatment of OSA may offer new opportunities to improve cognitive outcomes in older adults with OSA.",https://academic.oup.com/sleep/article/44/9/zsab076/6189102?login=false,
Examining sleep deficiency and disturbance and their risk for incident dementia and all-cause mortality in older adults across 5 years in the United States,"Simply associational, but older adults who slept 5hr a night or less were twice as likely to develop dementia across 5 years vs. those who slept 7-8hr. Snoring also a key player. I hope we can better address age-related sleep impairment in the clinic","Background: Sleep disturbance and deficiency are common among older adults and have been linked with dementia and all-cause mortality. Using nationally representative data, we examine the relationship between sleep disturbance and deficiency and their risk for incident dementia and all-cause mortality among older adults.

Methods: The National Health and Aging Trends Study (NHATS) is a nationally-representative longitudinal study of Medicare beneficiaries in the US age 65 and older. Surveys that assessed sleep disturbance and duration were administered at baseline. We examined the relationship between sleep disturbance and deficiency and incident dementia and all-cause mortality over the following 5 years using Cox proportional hazards modeling, controlling for confounders.

Results: Among the sample (n = 2,812), very short sleep duration (≤5 hours: HR = 2.04, 95% CI: 1.26 - 3.33) and sleep latency (>30 minutes: HR = 1.45, 95% CI: 1.03 - 2.03) were associated with incident dementia in adjusted Cox models. Difficulty maintaining alertness (“Some Days”: HR = 1.49, 95% CI: 1.13 - 1.94 and “Most/Every Day”: HR = 1.65, 95% CI: 1.17 - 2.32), napping (“Some days”: HR = 1.38, 95% CI: 1.03 - 1.85; “Most/Every Day”: HR = 1.73, 95% CI: 1.29 - 2.32), sleep quality (“Poor/Very Poor”: HR = 1.75, 95% CI: 1.17 - 2.61), and very short sleep duration (≤5 hours: HR = 2.38, 95% CI: 1.44 - 3.92) were associated with all-cause mortality in adjusted Cox models.

Conclusions: Addressing sleep disturbance and deficiency may have a positive impact on risk for incident dementia and all-cause mortality among older adults.

","Approximately 5.8 million adults in the U.S. are living with Alzheimer’s disease or related dementia, and 16 million are expected to be living with Alzheimer’s disease by 2050 [1]. Both sleep disturbances and abnormal sleep duration, although both often modifiable, are associated with the development and progression of Alzheimer’s disease [2]. Furthermore, recent work suggests that older adults (above age 65) who report signs of good sleep health (i.e., reports of waking and feeling refreshed) demonstrate better cognitive function [3], which may buffer against the development of Alzheimer’s disease and dementia. According to the National Sleep Foundation, there is more sleep disturbance reported among older adults than any other age group [4]. Data collected as part of a prospective observational study among older adults found over 50% of the participants reported at least one sleep difficulty “most of the time,” [5] whereas these complaints were reported by only 20% of younger adults (below age 65) [6].

Sleep disturbance and insufficiency have been shown to be associated with both the development and progression of Alzheimer’s disease and with all-cause mortality [7, 8]. Among a cohort of 737 older adults without dementia, adults with high sleep fragmentation had a 1.5-fold risk of developing Alzheimer’s Disease compared to those with low sleep fragmentation [9]. Similarly, in a prospective analysis, sleep disturbance was linked with incident cognitive impairment [10], while another prospective analysis found sleep disturbance was linked with both incident dementia as well as mortality [11]. In related studies, prospective analyses of 1,951 older adults found that difficulty maintaining alertness was associated with increased risk for dementia [12]; in a cohort of 1,245 older women, longer sleep latency was associated with higher risk of cognitive impairment over an average follow-up period of 4.9 years [13]. Furthermore, compared to cognitively normal individuals, those with either with self-reported obstructive sleep apnea (OSA) diagnosis [14] or physician-diagnosed OSA [15] developed more Alzheimer’s disease biomarkers, such as amyloid-beta plaques or tau proteins, over time compared to those without OSA. In addition, previous research has found associations between both long (>9 hours) and short (<7 hours) sleep duration and Alzheimer’s disease and dementia. Specifically, in research with data from the Framingham Heart Study, self-reports of longer sleep (>9 hours) were associated with all-cause dementia and clinical Alzheimer disease, but self-reports of short sleep (<6 hours) were not [16]. However, according to a meta-analysis of 27 studies, both short (< 7 hours) and long sleep duration (> 8 hours) were both associated with approximately 86% greater risk for Alzheimer’s disease and dementia [7].

Prior research has also examined the association among sleep characteristics, sleep deficiency, alertness and all-cause mortality [17–19]. In a prospective analysis, researchers found that older short (<6 hours) habitual sleepers were at approximately 50% greater risk for all-cause mortality after adjusting for confounders [18]. In research conducted by Chen and colleagues using the Pittsburgh Sleep Quality Index (PSQI), researchers found an association between high PSQI scores (indicative of poor sleep) and all-cause mortality, but this relationship disappeared after controlling for confounders such as depression [19]. In the same study, but with the sub-components of the PSQI (i.e., sleep medication use, sleep quality, and sleep duration), researchers found that those reporting long sleep (>9 hours) were at greater risk for all-cause mortality, but short sleepers were not [19]. Another prospective cohort study found an opposite result. Specifically, Bertisch and colleagues found an association between objectively measured short sleep duration and all-cause mortality, but not long sleep [17]. The same study examined the relationship between those reporting either short or long sleep duration and insomnia in relation to all-cause mortality, but did not find any significant associations [17]. According to a meta-analysis of prospective studies, both short and long sleep were associated with greater risk of all-cause mortality in adults [20, 21]. Another meta-analysis of 24-hour sleep duration found only long sleep (>9 hours) was associated with greater risk of mortality.

Research on sleep disturbance and deficiency and all-cause mortality therefore has shown conflicting results. Further, few studies have included a comprehensive set of sleep characteristics in a single examination of incident dementia and all-cause mortality. We address these gaps in the literature and examine the relationship between sleep disturbance, sleep duration, alertness and incident dementia and all-cause mortality across a five-year time interval using nationally representative data collected among older adults in the U.S.","Our study offers a contribution to the literature on sleep among aging populations in its assessment of incident dementia and all-cause mortality and a range of sleep characteristics among older adults. According to our findings from Cox proportional hazard models examining each sleep characteristic and outcome, we found that longer time to fall asleep and short sleep duration predicted incident dementia, while short sleep duration, difficulty maintaining alertness, napping, and poor sleep quality predict all-cause mortality. Short sleep duration was a strong predictor of both incident dementia and all-cause mortality, suggesting this may be a sleep characteristic that is important—over and above the other predictors—of adverse outcomes among older adults. Also, future research may consider the development of novel behavioral interventions to improve sleep among older adults.",https://www.aging-us.com/article/202591/text,
"The Association Between School Start Time and Sleep Duration, Sustained Attention, and Academic Performance","Delaying school start times by 1hr = 1) more sleep, 2) better attendance, and 3) superior academic performance. Conclusion: ""A delay in school start time should be seriously considered to improve sleep and academic achievements of students."" Quite","Purpose: In adolescence, physiological (circadiaPurpose: In adolescence, physiological  circadian and homeostatic regulation of sleep) and social habits contribute to delayed sleep onset, while social obligations impose early sleep offset. The effects of delayed school start time on the subjective/objective measures of sleep-wake patterns and academic achievement have not been established.

Methods: This pre-, post-, and longitudinal non-randomized study included an early (8:00 am; ESC=30 students) and the late (9:00 am; LSC=21 students) start class. Multiple sleep data included a weekly sleep diary, Karolinska Sleepiness Scale, Pittsburgh Sleep Quality Index, and Epworth Sleepiness Scale. Sustained attention was measured using the Psychomotor Vigilance Task. Academic performance was evaluated by two different mathematical and scientific standard tests (entrance and final) and by school attendance indicators. Data were collected at monthly intervals from October 2018 to May 2019 and the beginning and end of the academic year (pre/post).

Results: All students turned their lights off at similar times (LSC=11:21pm, ESC=11:11pm), but LSC students woke up later (7:23am) than ESC students (6:55am; F1,48=11.81, p=0.001) on school days. The groups did not differ in total sleep duration on non-school days. Longitudinal measures revealed a significant increase (8.9%, 34 min) in total sleep duration of LSC students across the academic year. ESC students maintained approximately the same sleep duration. Furthermore, changes in sleep duration had parallelled significant differences in sustained attention, with LSC students outperforming ESC students. Longitudinal changes of sleep and sustained attention were associated with a coherent pattern of changes in academic performance.

Conclusion: Findings indicate that a one-hour delay in school start time is associated with longer sleep, better diurnal sustained attention, attendance, and improved academic performance. Notably, sleep changes were limited to school days. A delay in school start time should be seriously considered to improve sleep and academic achievements of students.n and homeostatic regulation of sleep) and social habits contribute to delayed sleep onset, while social obligations impose early sleep offset. The effects of delayed school start time on the subjective/objective measures of sleep-wake patterns and academic achievement have not been established.","Due to specific developmental changes in both the circadian1 and homeostatic regulation2 of sleep, adolescents exhibit a natural tendency to remain active until late at night and awaken late in the morning.3–5 In adolescence, the circadian system’s period lengthens, delaying the onset of sleep. Furthermore, the homeostatic regulation of sleep is associated with decreased sleep pressure during waking
periods, which allows adolescents to stay awake longer than younger adolescents and adults. Behavioral and social factors, such as the nighttime use of electronic devices, contribute to delayed sleep onset.6 Social obligations (eg, school attendance) also impose an
early sleep offset, resulting in considerably less sleep than the eight to ten hours the American Academy of Sleep Medicine (AASM) recommends.7 Over the past 20 years, our knowledge about the crucial role of sleep in adolescents’ well-being has dramatically increased.8 Chronic sleep restriction among adolescents has been connected to many adverse outcomes affecting several daytime functioning areas (eg, mental and physical health, cognitive and academic performance, and risk-taking behaviors).9 Therefore, insufficient sleep in adolescents has become a major public health issue.10 Carskadon11 described the detrimental and cumulative
effects of biological, psychological, and social factors on adolescent sleep as a “perfect storm.” The American Academy of Pediatrics12 recommends delaying middle and high school start times to reduce sleep deprivation among adolescents. Consequently, many schools worldwide instituted later start times, and many non-experimental studies started advocating delayed school start times. A systematic review13 evaluated six pre/post design studies where school start times were delayed by 25–60 minutes, and it showed increases of
25–77 minutes in total sleep time (TST) per weeknight as a result. Postponing school start times also reduced daytime sleepiness, tardiness, and trouble staying awake. A subsequent review14 using empirical evidence from 11 studies (297,994 participants) proposed that delayed school start times may enhance physiological, academic, and psychosocial outcomes. Six studies reported significant positive relationships between later school start times and amount of sleep. Four studies, in contrast, reported mixed results regarding the association between later school start times and academic outcomes. Although these studies13,14 suggested several benefits of delaying
school start times, they highlighted the need for higherquality, longitudinal primary evidence. However, randomized controlled trials and high-quality primary studies are challenging. Indeed, school systems often refuse to allow researchers the necessary control over scheduling and data collection. Most studies also use a single-time survey not quantified by objective data (ie, behavioral or physiological measures). A recent study15 on secondary school start time delayed by 55 minutes overcame this limitation by using actigraphy to measure wake–sleep schedules. This pre/post study revealed a significant increase (4.5%, 34 min) in sleep duration and an improvement in attendance. However, the effect of increased nightly sleep was moderate in similar objective studies,16 and the later-start approach has not yet entirely convinced all school administrators. More consistent evidence and studies focusing on the process rather than solely the
effects are thus urgently needed.17 Chronic sleep deprivation critically impacts not only physical and mental health but also attention,18 learning, and memory consolidation,19 potentially affecting daytime academic performance.20 Therefore, we systematically collected subjective and objective data at one-month intervals throughout an entire academic year in an Italian pilot project studying delayed
school starts. To examine the relationships between later school start, increased nighttime sleep, and academic performance, we introduced a behavioral assessment of psychomotor vigilance using a well-established task21 that provided the most widely used metrics of sustained attention. Academic performance could be guided by sleep quality, which affects memory, learning consolidation, and more directly, the diurnal level of vigilance.8 According to the intrinsic nature of this field study, we used a non-randomized controlled trial design, prospectively assigning participants to 8:00 AM and 9:00 AM starts. Within the theoretical framework determined by existing
empirical studies, we hypothesized that delaying school start times would lead to primary effects on sleep (longer sleep duration) and secondary effects on daytime functioning (greater sustained attention and better academic performance).","Our findings indicate that delaying school start time to align with the natural wake-up rhythm of adolescents improves wake-up time, sleep duration, attentional performance, and academic success while decreasing absences and drop-outs, as reported by the teaching staff. Sleep deprivation degraded objective attentional performance measures in students who opted to start classes at 8:00 AM, suggesting that sleep loss reasonably affects other domains. These correspondences should not be ignored. Although the causal link has yet to
be demonstrated, these results strongly support an association between later school start times and physiological and academic outcomes, providing further support for policies formalizing delayed school start times. Future studies on larger samples of students are certainly necessary to check the robustness of these results, and follow-up studies are strongly recommended to test the long-term effectiveness of the intervention. Lastly, despite the well-documented benefits of delaying school start times, future research will have to consider the unresolved economic and logistic problems inherent in this kind of intervention.9",https://pubmed.ncbi.nlm.nih.gov/33328774/,
"The Big Three Health Behaviors and Mental Health and Well-Being Among Young Adults: A Cross-Sectional Investigation of Sleep, Exercise, and Diet","Sleep: a foundation, not pillar, of mental health? Associational study of +1,000 people. Sleep quality was the strongest predictor of depressive symptoms and flourishing, followed by sleep quantity. Next were physical activity then diet (fruit & veggies).","Sleep, physical activity, and diet have been associated with mental health and well-being individually in young adults. However, which of these “big three” health behaviors most strongly predicts mental health and well-being, and their higher-order relationships in predictive models, is less known. This study investigated the differential and higher-order associations between sleep, physical activity, and dietary factors as predictors of mental health and well-being in young adults.","Healthy lifestyles are important contributors to both physical and mental health. Getting high-quality sleep, engaging in physical activity, and eating well not only have advantages to physical health (Chin et al., 2019) but also have advantages to mental health such as reduced risk of depression (Raboch et al., 2017; Francis et al., 2019) and anxiety (Nho and Yoo, 2018) and increased psychological well-being (Pilcher et al., 1997; Mujcic and Oswald, 2016; Prendergast et al., 2016; Conner et al., 2017). Healthy lifestyles may be especially important for the mental health and well-being of young adults. Emerging adulthood is a time of both developmental and ecological changes, marked by increased responsibility, new roles, and changing life circumstances (Conley et al., 2020). This developmental period often coincides with a transition to work or university, with changing routines, academic demands, and living situations, which can disrupt health behaviors (Conley et al., 2014). Emerging adults also appear more vulnerable to poorer mental health (Adolescent Health Research Group, 2008; Stallman, 2010), which could suggest a role for unhealthy lifestyles contributing to poorer emotional functioning.

There is good evidence linking the “big three” health behaviors of sleep, physical activity, and diet individually to both mental health and well-being in emerging adults. Sleep is one modifiable health behavior that may become particularly disrupted in this population (Lev Ari and Shulman, 2012). Sleep plays a vital role in both mental and physical health across the lifespan, with approximately one third of each day dedicated to sleep (Samson and Nunn, 2015). It is recommended that healthy adults need approximately 7–8 h sleep per night, while healthy emerging adults need approximately 7–9 h sleep per night (Hirshkowitz et al., 2015). Both inadequate and disrupted sleep has been shown to negatively influence mental and physical health and are risk factors for both depression and anxiety (Alfano and Gamble, 2009; Harvey, 2011). However, while sleep quantity is associated with increased depression and negative affect among clinical populations, sleep quality appears to be a greater predictor of mental health and well-being among the general population (Pilcher et al., 1997; Bassett et al., 2015; Wallace et al., 2017; João et al., 2018) and young adults in particular (Buysse et al., 2008). Furthermore, while clear recommendations around sleep quantity are outlined and promoted, the importance of sleep quality in mental health and well-being receives less attention.

Physical activity is the second modifiable health behavior tied to better mental health and well-being in young adults. Physical activity releases endorphins within the body, which help to promote well-being and feelings of euphoria, and increase mood and energy (Fox, 1999). A recent meta-analysis of 16 randomized control trials (RCTs) suggests that regular physical activity at a moderate intensity may aid in the treatment of mental disorders such as depression (Bailey et al., 2018). Additionally, physical activity has been associated with improved well-being among non-clinical young adult populations (Penedo and Dahn, 2005) and associated with happiness more generally (Zhang and Chen, 2019). Conversely, low levels of physical activity and increased sedentary behavior have been associated with poorer mental well-being among adolescents (Ussher et al., 2007).

Diet is the third modifiable lifestyle behavior that contributes to mental health and well-being in young adults. Research has shown regular adherence to a healthy diet is associated with reduced risk of depression and improved mood (Molendijk et al., 2018). Intake of fruits and vegetables is a key aspect of a healthy diet linked to greater happiness and well-being in young adults (White et al., 2013; Conner et al., 2015), with further evidence that consumption of raw fruits and vegetables may be more beneficial than consumption of cooked or processed fruits and vegetables (Brookie et al., 2018). Conversely, regular consumption of a typical Western diet, categorized by consumption of refined grains, high sugar intake, and processed and fried foods, has been associated with increased perceived stress among female college students, increased depressive symptoms among both male and female students (El Ansari et al., 2014), poorer mental health outcomes (Jacka et al., 2010), and increased risk of depression (Akbaraly et al., 2009).

While extensive research has shown the mental health and well-being benefits of sleep, physical activity, and diet as individual predictors, research examining all three behaviors together, along with their possible higher-order relationships, is limited. Prior research has shown possible synergistic relationships between health behaviors in predicting well-being (Prendergast et al., 2016), which warrants replication. However, compensatory relationships could also be found, whereby the Compensatory Carry-Over Action Model suggests a good diet could compensate for low physical activity (Tan et al., 2018). Knowing the importance of each of these lifestyle behaviors, singularly or in combination with each other, and the hierarchical order of importance will inform mental health interventions at both the population and individual level.

Therefore, the current study investigated how self-reported sleep, physical activity, and dietary factors together predicted differences in mental health and well-being in young adults and whether there were any higher-order interactions among these behaviors in the prediction patterns.","This study highlighted the relative contribution of sleep, physical activity, and diet to the prediction of depressive symptoms and flourishing. Our findings suggest that future lifestyle interventions targeting sleep quality may be most beneficial at improving mental health and well-being. However, physical activity and diet should not be disregarded, particularly as they also uniquely predicted differences in depressive symptoms (physical activity) and well-being (physical activity and raw fruit and vegetable intake). Sleep, physical activity, and a healthy diet should be thought of as multiple tools for promoting optimal mental health and well-being, particularly among young adult populations where the prevalence of mental disorders is high and well-being is suboptimal.",https://www.frontiersin.org/articles/10.3389/fpsyg.2020.579205/full#B12,
Chocolate for breakfast prevents circadian desynchrony in experimental models of jet-lag and shift-work,"Lighthearted, I do hope British Airways reads this. 
Breakfast chocolate aids jet-lag reset (in rats, at least). 
Evening chocolate... not so much. ","Night-workers, transcontinental travelers and individuals that regularly shift their sleep timing, suffer from circadian desynchrony and are at risk to develop metabolic disease, cancer, and mood disorders, among others. Experimental and clinical studies provide evidence that food intake restricted to the normal activity phase is a potent synchronizer for the circadian system and can prevent the detrimental metabolic effects associated with circadian disruption. As an alternative, we hypothesized that a timed piece of chocolate scheduled to the onset of the activity phase may be sufficient stimulus to synchronize circadian rhythms under conditions of shift-work or jet-lag. In Wistar rats, a daily piece of chocolate coupled to the onset of the active phase (breakfast) accelerated re-entrainment in a jet-lag model by setting the activity of the suprachiasmatic nucleus (SCN) to the new cycle. Furthermore, in a rat model of shift-work, a piece of chocolate for breakfast prevented circadian desynchrony, by increasing the amplitude of the day-night c-Fos activation in the SCN. Contrasting, chocolate for dinner prevented re-entrainment in the jet-lag condition and favored circadian desynchrony in the shift-work models. Moreover, chocolate for breakfast resulted in low body weight gain while chocolate for dinner boosted up body weight. Present data evidence the relevance of the timing of a highly caloric and palatable meal for circadian synchrony and metabolic function.","Modern human society is exposed to a 7/24 activity schedule, leading individuals to low sleep quality and disrupted daily sleep-activity rhythms. It is known from shift-workers and frequent travelers ailing from jet-lag, that disturbed sleep-wake schedules create a conflict between the circadian system and the temporal signals derived from the cyclic environment, such as the light-dark cycle1. This is further supported by clinical and experimental findings indicating that a conflict between external time signals and the internal temporal order transmitted by the suprachiasmatic nucleus (SCN) can lead to internal desynchrony2.

Circadian organization is necessary to prepare the organisms for the daily challenges and requires a well-coordinated synchrony with the day-night cycle. Epidemiological and experimental studies indicate that constant exposure to situations that induce circadian disruption, increase the risk to develop overweight, metabolic diseases, cardiovascular problems and cancer3,4.

Diverse strategies are used to prevent circadian disruption, including scheduled melatonin administration, scheduled dexamethasone administration, exercise or scheduled feeding5,6,7,8. Scheduled feeding has shown to be a strong entraining signal for the body; when food intake is synchronized with the activity phase it exerts beneficial effects on the circadian system and metabolism8,9,10. In experimental studies, timed food access restricted to the active phase accelerates resynchronization in a jet-lag model, prevents circadian desynchrony in a shift-work model10 and induces healthy effects in metabolism11,12. Conversely, food scheduled to the sleep/rest phase, exerts a disruptive influence on circadian synchrony, altering metabolism and behavior11,13.

We have previously reported that scheduled access to chocolate, entrains brain areas involved in motivation and in the metabolic response to food14. Scheduled chocolate also entrains the circadian system, enhancing the amplitude of neuronal activation in the SCN15. In the present study we hypothesized that a daily piece of chocolate scheduled in synchrony with the onset of the normal activity phase (breakfast) would be a powerful stimulus to prevent circadian disruption in experimental models of jet-lag and shift-work. Therefore, experimental models of jet-lag and shift-work were used to test the synchronizing effects of a piece of chocolate for breakfast or dinner on general activity, body temperature, daily c-Fos activation and metabolic indicators.","The present study explored the effects of a piece of chocolate as a synchronizing factor to prevent circadian disruption under conditions of shift-work or jet-lag. We provide evidence that a piece of chocolate, a high caloric and palatable meal, exerts beneficial effects on circadian function and body weight, when it is timed to the beginning of the active phase (breakfast). The circadian synchrony was associated with beneficial effects on metabolic rhythms and body weight, thus may be an importat factor for maintaining circadian fitness in spite of the 24/7 modern lifestyle. Further studies will need to demonstrate whether chocolate for breakfast can prevent comorbidities of internal desynchronization such as the development of tumors, depression, metabolic syndrome, and others.",https://www.nature.com/articles/s41598-020-63227-w,
"Sleep quality, duration, and consistency are associated with better academic performance in college students","Associational study showing that nearly 25% of a student’s grade is predicted by sleep in the weeks before the exam. Suggests sleep hysteresis: it’s not simply sleep the night prior to the exam. Rather, it's your aggregated sleep history over many weeks.","Although numerous survey studies have reported connections between sleep and cognitive function, there remains a lack of quantitative data using objective measures to directly assess the association between sleep and academic performance. In this study, wearable activity trackers were distributed to 100 students in an introductory college chemistry class (88 of whom completed the study), allowing for multiple sleep measures to be correlated with in-class performance on quizzes and midterm examinations. Overall, better quality, longer duration, and greater consistency of sleep correlated with better grades. However, there was no relation between sleep measures on the single night before a test and test performance; instead, sleep duration and quality for the month and the week before a test correlated with better grades. Sleep measures accounted for nearly 25% of the variance in academic performance. These findings provide quantitative, objective evidence that better quality, longer duration, and greater consistency of sleep are strongly associated with better academic performance in college. Gender differences are discussed.","The relationship between sleep and cognitive function has been a topic of interest for over a century. Well-controlled sleep studies conducted with healthy adults have shown that better sleep is associated with a myriad of superior cognitive functions,1,2,3,4,5,6 including better learning and memory.7,8 These effects have been found to extend beyond the laboratory setting such that self-reported sleep measures from students in the comfort of their own homes have also been found to be associated with academic performance.9,10,11,12,13

Sleep is thought to play a crucial and specific role in memory consolidation. Although the exact mechanisms behind the relationship between sleep, memory, and neuro-plasticity are yet unknown, the general understanding is that specific synaptic connections that were active during awake-periods are strengthened during sleep, allowing for the consolidation of memory, and synaptic connections that were inactive are weakened.5,14,15 Thus, sleep provides an essential function for memory consolidation (allowing us to remember what has been studied), which in turn is critical for successful academic performance.

Beyond the effects of sleep on memory consolidation, lack of sleep has been linked to poor attention and cognition. Well-controlled sleep deprivation studies have shown that lack of sleep not only increases fatigue and sleepiness but also worsens cognitive performance.2,3,16,17 In fact, the cognitive performance of an individual who has been awake for 17 h is equivalent to that exhibited by one who has a blood alcohol concentration of 0.05%.1 Outside of a laboratory setting, studies examining sleep in the comfort of peoples’ own homes via self-report surveys have found that persistently poor sleepers experience significantly more daytime difficulties in regards to fatigue, sleepiness, and poor cognition compared with persistently good sleepers.18

Generally, sleep is associated with academic performance in school. Sleep deficit has been associated with lack of concentration and attention during class.19 While a few studies report null effects,20,21 most studies looking at the effects of sleep quality and duration on academic performance have linked longer and better-quality sleep with better academic performance such as school grades and study effort.4,6,9,10,11,12,13,22,23,24,25,26,27 Similarly, sleep inconsistency plays a part in academic performance. Sleep inconsistency (sometimes called “social jet lag”) is defined by inconsistency in sleep schedule and/or duration from day to day. It is typically seen in the form of sleep debt during weekdays followed by oversleep on weekends. Sleep inconsistency tends to be greatest in adolescents and young adults who stay up late but are constrained by strict morning schedules. Adolescents who experience greater sleep inconsistency perform worse in school.28,29,30,31

Although numerous studies have investigated the relationship between sleep and students’ academic performance, these studies utilized subjective measures of sleep duration and/or quality, typically in the form of self-report surveys; very few to date have used objective measures to quantify sleep duration and quality in students. One exception is a pair of linked studies that examined short-term benefits of sleep on academic performance in college. Students were incentivized with offers of extra credit if they averaged eight or more hours of sleep during final exams week in a psychology class32 or five days leading up to the completion of a graphics studio final assignment.33 Students who averaged eight or more hours of sleep, as measured by a wearable activity tracker, performed significantly better on their final psychology exams than students who chose not to participate or who slept less than eight hours. In contrast, for the graphics studio final assignments no difference was found in performance between students who averaged eight or more hours of sleep and those who did not get as much sleep, although sleep consistency in that case was found to be a factor.

Our aim in this study was to explore how sleep affects university students’ academic performance by objectively and ecologically tracking their sleep throughout an entire semester using Fitbit—a wearable activity tracker. Fitbit uses a combination of the wearer’s movement and heart-rate patterns to estimate the duration and quality of sleep. For instance, to determine sleep duration, the device measures the time in which the wearer has not moved, in combination with signature sleep movements such as rolling over. To determine sleep quality, the Fitbit device measures the wearer’s heart-rate variability which fluctuates during transitions between different stages of sleep. Although the specific algorithms that calculate these values are proprietary to Fitbit, they have been found to accurately estimate sleep duration and quality in normal adult sleepers without the use of research-grade sleep staging equipment.34 By collecting quantitative sleep data over the course of the semester on nearly 100 students, we aimed to relate objective measures of sleep duration, quality, and consistency to academic performance from test to test and overall in the context of a real, large university college course.

A secondary aim was to understand gender differences in sleep and academic performance. Women outperform men in collegiate academic performance in most subjects35,36,37,38 and even in online college courses.39 Most of the research conducted to understand this female advantage in school grades has examined gender differences in self-discipline,40,41,42 and none to date have considered gender differences in sleep as a mediating factor on school grades. There are inconsistencies in the literature on gender differences in sleep in young adults. While some studies report that females get more quantity43 but worse quality sleep compared with males,43,44 other studies report that females get better quality sleep.45,46 In the current study, we aim to see whether we would observe a female advantage in grades and clarify how sleep contributes to gender differences.",,https://www.nature.com/articles/s41539-019-0055-z,
"Short Sleep Duration in Working American Adults, 2010–2018",A jump in the number of people sleeping <6hr from 2010 (30%) to 2018 (35%). Hardest hit: those in healthcare (45%) and the military (50%). This is unsustainable chronic sleep deprivation for maintaining human health.,"Short sleep duration is detrimental to physical and mental health. In this study, we explored the epidemiology of short sleep duration (<7 h) in working American adults from 2010 to 2018. Data from the National Health Interview Survey (NHIS) were analyzed to describe the prevalence and trends of short sleep duration by demographic and employment characteristics of working American adults. Overall, the prevalence of short sleep duration in working American adults increased signifcantly from 2010 to 2018 (30.9% in 2010 to 35.6% in 2018). Across the 9-year study period, short sleep duration prevalence varied signifcantly by demographic characteristics (i.e. age, race, marital status, number of children in the household, residing region, level of education) and occupational characteristics. Compared to 2010, the odds of short sleep duration were statistically signifcantly higher in 2018 despite adjusting for demographic characteristics (25% higher) and occupational characteristics (22% higher). In 2018, the highest levels of short sleep duration were found for the following categories of jobs: protective service and military (50%), healthcare support occupations (45%), transport and material moving (41%), and production occupations (41%). Sleep hygiene education may be especially useful for those in occupations with high rates of short sleep duration.","Sleep problems are under-recognized as a major population health concern. Inadequate sleep (<7 h) is associated with mild to severe physical and mental health problems, injury, loss of productivity, and premature mortality [1–6]. Two meta-analyses that reviewed the impact of shortened sleep duration on adults found association with a 9% increase in the risk of type-2 diabetes (for each hour of decreased sleep duration) [1], a 15% increase in strokes, a 23% increase in hypertension risk, and a 48% increase in coronary heart disease [2, 3]. Inadequate sleep is also associated with impulsivity, anxiety, alcohol abuse, absenteeism (workplace absence), presenteeism (below par work performance), unstable moods, and suicidal ideation [4, 5]. In addition to the health impairments caused by sleep defciency, the economic costs of inadequate sleep in the U.S. have been estimated to be $411 billion annually [6]. A plethora of studies has estimated the prevalence, predictors, and correlates of sleep disturbances in the adult American population. However, these studies have major limitations [7–12]. First, a number of studies were conducted with populations via clinics or healthcare facilities.
Second, due to convenience sampling, the sample size of these studies is generally limited to a certain geographic region or population, which results in small sample sizes and biases inherent to such study designs. Third, the limited number of larger studies published on the adult American population are limited to durations of 1 year or less (i.e. one point in time, precluding trend analyses over a longer period). Fourth, the studies published often do not focus on working adult American populations that are unique groups (i.e. employment related confounding could limit the ability of published studies to elucidate the true extent of sleep problems in the working American adult population). Finally, published studies on inadequate sleep in adult American populations do not adequately characterize the afected population by a number of vital sociodemographic factors beyond age, race, and gender (e.g. geography, type of employment, education) [7–12]. These limitations can be overcome, in part, by utilizing large national random samples of adults in the U.S. that are representative of the working American population. Thus, the purpose of this study was to describe the trends, patterns, and prevalence of short
sleep durations in working American adults over a period of 9 years (2010–2018) utilizing the National Health Interview Surveys (NHIS) [13]. Specifcally, we wanted to explore: what are the trends in prevalence of short sleep duration in the working American adult population? How did the prevalence of short sleep duration in working American adults vary by sociodemographic characteristics from 2010 to 2018? What employment characteristics were related to short sleep duration in the working American adult population from 2010 to 2018?","Almost a third of working adults in the U.S. get inadequate quantities of sleep. Most likely, those who work long hours, engage in changing shifts, or those in high stress professions that have minimal control over their work and life schedules are at risk of short sleep duration and the subsequent social, physical and mental health consequences of sleep problems. Noteworthy groups are racial/ethnic minorities, the less educated, and those who are living alone are at a greater risk for reporting short sleep duration among working American adults. Employers that are willing to help employees develop adequate sleep times may increase the probability of workplace productivity, reduction in employee healthcare costs, and improving workplace safety and health. Sleep hygiene education may be one method to help employees optimize their levels of sleep and reduce a signifcant form of preventable harm. For individuals with pathologies and chronic comorbidities, early clinical interventions may help reap signifcant benefts.",https://link.springer.com/article/10.1007/s10900-019-00731-9,
Sleep Habits and Susceptibility to the Common Cold,People who had <7 hours of sleep per night were ~3x more likely to develop a cold than those with ≥ 8 hours after administration with nasal drops containing rhinovirus. Those with <92% sleep efficiency were 5.5x more likely to develop a cold than those with ≥ 98% efficiency.,"Sleep quality is thought to be an important predictor of immunity and, in turn, susceptibility to the common cold. This article examines whether sleep duration and efficiency in the weeks preceding viral exposure are associated with cold susceptibility. A total of 153 healthy men and women (age range, 21-55 years) volunteered to participate in the study. For 14 consecutive days, they reported their sleep duration and sleep efficiency (percentage of time in bed actually asleep) for the previous night and whether they felt rested. Average scores for each sleep variable were calculated over the 14-day baseline. Subsequently, participants were quarantined, administered nasal drops containing a rhinovirus, and monitored for the development of a clinical cold (infection in the presence of objective signs of illness) on the day before and for 5 days after exposure. There was a graded association with average sleep duration: participants with less than 7 hours of sleep were 2.94 times (95% confidence interval [CI], 1.18-7.30) more likely to develop a cold than those with 8 hours or more of sleep. The association with sleep efficiency was also graded: participants with less than 92% efficiency were 5.50 times (95% CI, 2.08-14.48) more likely to develop a cold than those with 98% or more efficiency. These relationships could not be explained by differences in prechallenge virus-specific antibody titers, demographics, season of the year, body mass, socioeconomic status, psychological variables, or health practices. The percentage of days feeling rested was not associated with colds. Poorer sleep efficiency and shorter sleep duration in the weeks preceding exposure to a rhinovirus were associated with lower resistance to illness.","It is commonly thought that poor sleep increases our susceptibility to the common cold. However, there is little direct evidence for this assertion. Experimental studies have demonstrated that sleep deprivation results in poorer immune function, such as reduced natural killer cell activity, suppressed interleukin-2 production, and increased levels of circulating proinflammatory cytokines.1-3 Sleep deprivation has also been found to attenuate antibody response to both hepatitis A4 and influenza immunizations.5 The only direct evidence that sleep habits are associated with cold susceptibility derives from a secondary analysis of data from a rhinovirus (RV)-challenge study in which a single retrospective questionnaire assessing sleep habits during the previous month was used to assess sleep efficiency (the percentage of time a person actually sleeps between lying down to sleep and waking up the next morning).6 Efficiencies below 80% predicted a greater risk for the development of verifiable illness.

In this study, we examined whether sleep habits are associated with resistance to a common cold. Instead of retrospective reports, we obtained estimates of sleep habits by averaging respondent reports of sleep duration, efficiency, and “feeling rested” across 14 consecutive days. After sleep assessments were completed, the participants were exposed to an RV and were monitored to see whether they developed clinical illness. Infection and signs and symptoms of illness were assessed the day before and for 5 days after the viral challenge. This design extends previous work by providing reliable (averaged over 14 days) online (collected daily) measures of baseline sleep; by allowing the comparison of the relative importance of sleep duration, efficiency, and feeling rested for cold susceptibility; and by providing the opportunity to test for graded relationships between sleep measures and disease susceptibility.","In sum, according to our study results, measures of sleep predicted susceptibility to the development of a cold. Although both shorter sleep duration and lower sleep efficiency were associated with risk for illness, duration did not predict independently of efficiency, which was a stronger overall correlate of illness. Although the prospective design does not allow causal inference, it does eliminate reverse causation as an explanation. Because of the prospective design and the controls for multiple confounding factors, these results strongly suggest the possibility of sleep playing a causal role in cold susceptibility. Moreover, the use of a maximally reliable multiple day assessment of sleep habits increases our confidence in the findings of this study.",https://twitter.com/foundmyfitness/status/1483924723901763584,
Dietary Omega-3 Polyunsaturated Fatty-Acid Supplementation Upregulates Protective Cellular Pathways in Patients with Type 2 Diabetes that reduces Painful Diabetic Neuropathy,"Interesting, even low-dose DHA significantly increased Glutathione activity/precursors in diabetic subjects, increased glycerol-3-phosphate by 78% & alpha-ketobutyrate (important for NAD+ regeneration) by 35% while lowering 2-hydroxybutyrate* by 18% etc.","t: Background: Omega-3 polyunsaturated fatty acids (PUFAs) are increasingly reported to improve chronic neuroinflammatory diseases in peripheral and central nervous systems. Specifically, docosahexaenoic acid (DHA) protects nerve cells from noxious stimuli in vitro and in vivo. Recent reports link PUFA supplementation to improving painful diabetic neuropathy (pDN) symptoms. However, the molecular mechanism behind omega-3 PUFAs ameliorating pDN symptoms is lacking. Therefore, we sought to determine the distinct cellular pathways that omega3 PUFAs dietary supplementation promotes in reducing painful neuropathy in type 2 diabetes mellitus (DM2) patients. Methods: Forty volunteers diagnosed with type 2 diabetes were enrolled in the ""En Balance-PLUS"" diabetes education study. The volunteers participated in weekly lifestyle/nutrition education and daily supplementation with 1,000 mg DHA and 200 mg eicosapentaenoic acid. The Short-Form McGill Pain Questionnaire validated clinical determination of baseline and post-intervention pain complaints. Laboratory and untargeted metabolomics analyses were conducted using blood plasma collected at baseline and after three months of participation in the dietary regimen. The metabolomics data was analyzed using random forest, hierarchical cluster, ingenuity pathway analysis, and metabolic pathway mapping. Results: We found that metabolites involved in oxidative stress and glutathione production shifted significantly to a more anti-inflammatory state post supplementation. Example of these metabolites include cystathionine (+90%), S-methylmethionine (+9%), glycine cysteine-glutathione disulfide (+157%) cysteinylglycine (+19%), glutamate (-11%), glycine (+11%) and arginine (+13.4%). In addition, the levels of phospholipids associated with improved membrane fluidity such as linoleoyldocosahexaenoyl-glycerol (18:2/22:6) (+253 %) were significantly increased. Ingenuity pathway analysis suggested several key bio functions associated with omega-3 PUFA supplementation such as formation of reactive oxygen species (p = 4.38 × 10-4, z-score = -1.96), peroxidation of lipids (p = 2.24 × 10-5, z-score = -1.944), Ca2+ transport (p = 1.55 × 10-4, z-score = -1.969), excitation of neurons (p = 1.07 ×10-4, z-score = -1.091), and concentration of glutathione (p = 3.06 × 10-4, z-score = 1.974). Conclusion: The reduction of pro-inflammatory and oxidative stress pathways following omega3 PUFAS supplementation is consistent with using omega-3 PUFAs as a complementary dietary strategy as part of the overall treatment of pDN.","Painful diabetic neuropathy (pDN) is a common comorbidity of DM2 and has a significant negative impact on both quality of life and productivity of patients with DM2. Effective therapeutic interventions are sorely lacking and there is a great unmet need for novel and safe therapeutics. Currently treatment is focused on symptoms rather than targeting the disease process resulting in only a third of treated patients achieving 50% pain relief, often complicated by side effects [1-3]. In part, the development of therapies is complicated because of the inherently variable and complex nature of pDN. Although progress has been made in understanding the pathophysiology of diabetic neuropathy, attempts to use various targeted therapies has not resulted in either consistent or sustained benefits in patients with neuropathy. Progress in alleviating painful neuropathy is critically dependent on an understanding of the pathogenesis of pDN which to date is incompletely defined and we will continue to struggle in developing new pharmacotherapies that address the underlying biological dysregulation associated with pDN. Pre-clinical models suggest that the pathophysiology of pDN involves both microvascular and metabolic changes, which ultimately lead to increased oxidative stress, inflammation, and mitochondrial dysfunction [4]. However, corroborating human data currently is insufficient in the context of pDN. This knowledge gap may be closing with the development and use of highthroughput metabolomics analysis as we can now identify critical biomarkers and elucidate major metabolic pathways involved in the pathophysiology of different disease states [5-10]. For instance, metabolomics analysis can provide a readout of interactions between an individual's genome, diet, and environment in clinical studies. Growing evidence has implicated PUFAs in diminishing the adverse effects of chronic neuroinflammatory diseases in both the peripheral and central nervous system [11, 12]. The treatment with docosahexaenoic acid (DHA) has shown significant protection in nerve cells both in cell culture and in vivo [13-16]. Furthermore, prophylactic dietary omega-3 PUFAs protect against spinal cord injury-induced chronic pain [7]. In agreement with these findings, pre-clinical models have found that oral administration of omega-3 PUFAs can regenerate and protect peripheral nerves from injury [11, 17]. Recently, PUFA supplementation has been linked to the amelioration of painful diabetic neuropathy symptoms within a clinical cohort [10]. However, the effects on a cellular basis that are related to any benefits within humans attributed to omega-3 PUFAs and pDN is lacking. The present study examines the underlying metabolome of participants with type 2 diabetes reporting pDN and the impact of DHA-rich supplementation on their metabolic profile. This study represents, to our knowledge, the first unbiased attempt, in humans with type-2 diabetes, to identify the biologically critical metabolic features that define pDN and how a dietary DHA-enriched supplementation alters pDN patient's metabolome during a three-month quasi experimental designed intervention. Of significance, our study identifies biochemical signatures associated with pDN and determines how a simple nutritional invention leads to significant reversal.","We demonstrated that dietary DHA-enriched supplementation has a wide-ranging impact on the antioxidant metabolomic profile of our participants. Overall, this study shows that untargeted metabolomics is sensitive for defining targeted alteration in metabolism secondary to a dietary intervention. Specifically, we found that a dietary DHA-enriched supplementation improved metabolic profiles regarding omega-3 PUFA metabolism, glycerolipid metabolism, cysteine, methionine, glutathione metabolism, and glucose and fatty acid homeostasis without changing clinical parameters. Furthermore, circulating plasma metabolites showed a significant shift toward decreased reactive oxygen species biosynthesis, lipid peroxidation, improved Ca2+ homeostasis, and increased glutathione activity. These changes may explain previously reported reduction in neuropathic pain symptoms in our En Balance plus cohort [10]. Since omega-3 FA supplementation is safe in patients with type 2 diabetes, future studies are warranted to define further its role in the possible improvement of pDN symptoms [89].",https://twitter.com/onelifemax/status/1483849703930355716,
Somnogenic Cytokines and Models Concerning Their Effects on Sleep,"From what we know about sleep, the idea that too much is deleterious has always felt jarring. Should we practice sleep restriction if we're sleeping too much? Reality: It may be a spurious association since cytokines can have a somnogenic quality.","All the sleep-promoting substances currently identified also have other biological activities. Despite years of effort, a single specific central nervous system sleep center has not been described. These observations led us to propose a biochemical model of a sleep activational system in which the effects of several sleep factors are integrated into a regulatory scheme. These sleep factors interact by altering the metabolism, production, or activity of each other and thereby result in multiple feedback loops. This web of interactions leads to sleep stability in that minor challenges to the system will not greatly alter sleep. The system, however, is responsive to strong perturbations, such as sleep deprivation and infectious disease. The sleep-promoting effects of cytokines and their interactions with prostaglandins and the neuroendocrine system are used to illustrate the functioning of a part of the sleep activational system under normal conditions and during infectious disease. Although the actions of individual sleep factors are not specific to sleep, their interactions at various levels of the neuraxis can mediate a specific sleep response. Such a system would also be responsive to the autonomic and environmental parameters that alter sleep.","The concept that sleep is regulated in part by humoral mechanisms is supported by experimental demonstrations of somnogenic substances in tissue fluids (reviewed, [1-4]). These experiments began at the turn of the century with Legendre and Pieron [5] and Ishimori [6], who described the accumulation during prolonged wakefulness of substances in cerebrospinal fluid (CSF) that induce excess sleep when transferred to recipient animals. Although the chemical identity of these substances was never established, today over 30 putative sleep factors (SFs) have been identified (reviewed, [2]). Most of these putative SFs are hormones, immunomodulators, and/or substances involved in endocrine and/or immune regulation. Until recently, it was postulated that a SF, herein defined as any substance found within the body that alters sleep, would have biological actions specific to sleep and would act on central nervous system (CNS) executive sleep centers, which were also assumed to be concerned primarily with sleep regulation. A single CNS center necessary for sleep has not, however, been demonstrated. Furthermore, all SFs identified to date have multiple biological activities. Thus, a revision of the original assumptions is necessary. This review describes a model that links the sleep effects of
many putative SFs in a sleep activational system. The sleep activational system presented is fundamentally a biochemical model and, like all models, is provisional; the cellular localization of many of the specific components that are germane to sleep regulation is not known. Nevertheless, it is assumed that specific components of the model ultimately interact with either glia and/or neurons at various levels of the CNS and that such interactions lead to altered sleep. We propose that such a system would be responsive to the many autonomic and environmental parameters that influence sleep, yet would retain the capacity to regulate sleep selectively. Rapid-eye-movement sleep (REMS) and non-rapid-eye-movement sleep (NREMS) are the two major types of sleep, although subdivisions of each of these classes have been made. To identify these states of vigilance, the electroencephalogram (EEG) and other physiological measurements, e.g., electromyogram, brain temperature, and motor activity, are recorded. REMS and NREMS states exist in most mammals. Sleep in animals, however, is slightly different from that of humans. Thus, species often used in sleep research, such as rats, cats, and rabbits, have sleep episodes that are relatively short, typically lasting only for a few minutes. These bouts of sleep occur sporadically throughout the 24-hour day, alternating with episodes of wakefulness, although the percentage of any given hour spent in sleep is strongly influenced by circadian rhythms.","From the discussions presented here, it is clear that cytokines and many other substances alter sleep. We have presented a model of how the regulation of many SFs may be interrelated and another model illustrating how SFs may interact with various neuronal sets to produce specific sleep responses. Ten years ago, it would have been unrealistic to present such models because the roster of putative SFs was much smaller, and there was very little information concerning how SFs might affect each other; however, despite the great number of advances within the field of SFs, many questions remain unanswered. Major challenges that need to be addressed include: (1) How do SF concentrations change with sleep/wake cycles and under pathological conditions? (2) What is the timing of these changes? (3) Where do these changes take place? (4) Is one change linked directly to another?",https://twitter.com/foundmyfitness/status/1450894696448815107,
"Vitamin C supplementation promotes mental vitality in healthy young adults: results from a cross‑sectional analysis and a randomized, double‑blind, placebo‑controlled trial",Young adults given 500 mg of vitamin C twice a day for 4 weeks had better cognitive performance requiring sustained attention and improved work motivation compared to those given a placebo.,"We aimed to investigate the link of vitamin C status with vitality and psychological functions in a cross-sectional study, and examine their causal relationship through a randomized controlled trial (RCT).We frst conducted a population-based cross-sectional investigation of healthy young adults (n=214, 20–39 years), and analyzed the associations of serum vitamin C concentrations with vitality (fatigue and attention) and mood status (stress, depression, and positive and negative afect) using Pearson’s correlation and multiple linear regression analyses. Next, we performed a double-blind RCT in healthy subjects whose serum vitamin C concentrations were inadequate (<50 μmol/L).Subjects were randomly allocated to receive 500 mg of vitamin C twice a day for 4 weeks (n=24) or a placebo (n=22). We assessed vitality, which included fatigue, attention, work engagement, and self-control resources, and measured mood status, including stress, depression, positive and negative afect, and anxiety. ELISA determined serum brain-derived neurotrophic factor (BDNF), and a Stroop color–word test evaluated attention capacity and processing speed. s In the cross-sectional data, the serum vitamin C concentration was positively associated with the level of attention (r=0.16, p=0.02; standardized β=0.21, p=0.003), while no signifcant associations with the levels of fatigue and mood variables being found. In the RCT, compared to the placebo, the vitamin C supplementation signifcantly increased attention (p=0.03) and work absorption (p=0.03) with distinct tendency of improvement on fatigue (p=0.06) and comprehensive work engagement (p=0.07). The vitamin C supplementation did not afect mood and serum concentrations of BDNF. However, in the Stroop color–word test, the subjects supplemented with vitamin C showed better performance than those in the placebo group (p=0.04).n Inadequate vitamin C status is related to a low level of mental vitality. Vitamin C supplementation efectively increased work motivation and attentional focus and contributed to better performance on cognitive tasks requiring sustained attention","Vitamin C (ʟ-ascorbic acid or ascorbate) is an essential nutrient in humans that functions as an indispensable electron donor and a cofactor in various biological reactions such as hydroxylation of collagen, biosynthesis of carnitine, and tyrosine metabolism [1]. Interestingly, vitamin C presents its highest concentrations in the brain [2], and animal model and in vitro studies have reported that vitamin C performs critical roles in brain functions. Vitamin C protects neurons from oxidative stress, induces diferentiation and maturation of neurons, and regulates the synthesis or release of   euromodulating factors including serotonin, catecholamines, and glutamate [3, 4]. Accordingly, vitamin C is inferred to be important for maintaining normal mental health. Humans rely on dietary supply to obtain vitamin C due to the absence of a gene encoding ʟ-gulonolactone oxidase, which is critical for vitamin C synthesis from glucose [5]. Although vitamin C defciency can be prevented by consuming one or two servings of citrus fruits or vegetables; reports state that an inadequate vitamin C status is prevalent among young adults even in industrialized countries [6, 7]. The poor vitamin C status in the young can be attributed to external factors such as smoking, excessive drinking, and unhealthy eating habits that fail to provide a fresh and balanced diet rich in vitamin C [8]. Thus, even healthy young individuals can be at risk of vitamin C defciency, and consequently, poor body functions due to these lifestyle-related factors.However, compared to the elderly, inadequate vitamin C status in the young is liable to remain undiagnosed or be considered as being of little importance. Vitality is defned as a subjective feeling of energy and aliveness, which highlights the psychological aspects [9]. Feeling vital is suggested as a key component in healthy psychological functioning, the ability of self-regulation, work performance, and goal achievement [10, 11]. Classically, vitality decline is known as the earliest sign of scurvy, a clinical symptom of severe vitamin C defciency; it manifests in fatigue, decreases in arousal and motivation, and cognitive impairment [12–15]. Considering that professional and social engagement is highest in the young population, it is necessary to investigate whether improvement of vitamin C status helps to promote their vitality and work performance. However, the link of vitamin C status with vitalityrelated psychological and cognitive functions at a young age is equivocal and their causal relationship has rarely been examined.
Hence, we frst investigated the associations between vitamin C status and subjective vitality in a healthy young population. Next, using a randomized, double-blind, placebocontrolled trial, we further explored the efects of vitamin C supplementation on vitality, such as work motivation and self-regulatory resources, and cognitive performance in young adults with inadequate vitamin C status.","In conclusion, this study is the frst, to our knowledge, to show the link between vitamin C status with mental functions in healthy young adults using both populationbased observational studies and randomized clinical trials. The cross-sectional study suggests inadequate vitamin C status is related to a low level of mental vitality. In the randomized clinical trial, vitamin C supplementation at 1000 mg/day for 4 weeks efectively increases serum vitamin C concentrations in subjects with suboptimal vitamin C status. The supplementation promotes their mental vitality, especially work motivation and attentional focus, contributing to better performance on cognitive tasks that require sustained attention.",https://twitter.com/foundmyfitness/status/1445106700164292608,
Magnesium intake and incidence of pancreatic cancer: the VITamins and Lifestyle study,"Large study including over 66,000 men and women found that for every 100 mg decrease in magnesium intake there was a 24% increase in pancreatic cancer independent of age, body mass, and gender. ~45% of the US population has insufficient magnesium intake.","Studies document that magnesium is inversely associated with the risk of diabetes, which is a risk factor of pancreatic cancer. However, studies on the direct association of magnesium with pancreatic cancer are few and findings are inconclusive. In this study, we aimed to investigate the longitudinal association between magnesium intake and pancreatic cancer incidence in a large prospective cohort study. A cohort of 66 806 men and women aged 50–76 years at baseline who participated in the VITamins And Lifestyle (VITAL) study was followed from 2000 to 2008. Multivariable-adjusted Cox regression models were used to estimate hazard ratios (HRs) and 95% confidence intervals (CIs) of pancreatic cancer incidence by magnesium intake categories. During an average of 6.8-year follow-up, 151 participants developed pancreatic cancer. Compared with those who met the recommended dietary allowance (RDA) for magnesium intake, the multivariable-adjusted HRs (95% CIs) for pancreatic cancer were 1.42 (0.91, 2.21) for those with magnesium intake in the range of 75–99% RDA and 1.76 (1.04, 2.96) for those with magnesium intake <75% RDA. Every 100 mg per day decrement in magnesium intake was associated with a 24% increase in the incidence of pancreatic cancer (HR: 1.24; 95% CI: 1.02, 1.50; Ptrend=0.03). The observed inverse associations appeared not to be appreciably modified by age, gender, body mass index, and non-steroidal anti-inflammatory drug use but appeared to be limited to those taking magnesium supplementation (from multivitamins or individual supplement). Findings from this prospective cohort study indicate that magnesium intake may be beneficial in terms of primary prevention of pancreatic cancer.

","Pancreatic cancer is the fourth leading cause of cancer-related death in both men and women in the United States . The overall incidence of pancreatic cancer has not significantly changed since 2002 but the mortality rate has increased an average of 0.4% annually from 2002-2011 (National Cancer Institute, 2014). It was estimated that about 46 420 people in the United States would be diagnosed with pancreatic cancer and about 39 590 would die of this disease in 2014 .
Approximately 80% of pancreatic cancer patients have concomitant diabetes  Studies show that pancreatic tumour cells have receptors for insulin and have high levels of insulin as in type-2 diabetes and insulin resistance. Insulin and insulin-like growth factor (IGF) promote pancreatic tumour cell growth. Because epidemiological studies suggest that magnesium intake is inversely associated with the risk of type-2 diabetes  – a risk factor of pancreatic cancer, it is reasonable to hypothesise that magnesium intake may decrease the risk of pancreatic cancer. However, data directly relating magnesium intake to the incidence of pancreatic cancer are sparse and the findings are inconsistent. Two case–control studies reported that magnesium intake was inversely associated with the risk of pancreatic cancer, whereas another case–control study  found no association. Two recent prospective cohort studies, one from the Health Professionals Follow-up Study conducted only in men and the other from the European Prospective Investigation of Cancer (EPIC) study,  found no association between magnesium intake and the incidence of pancreatic cancer in the total cohort, but both observed an inverse association (borderline in EPIC study) among overweight men. Therefore, we aimed to (1) investigate the longitudinal association between magnesium intake and the incidence of pancreatic cancer, and (2) explore whether age, gender, body mass index (BMI, kg m−2), non-steroidal anti-inflammatory drugs (NSAIDs) use and magnesium supplementation are effect modifiers in this large prospective cohort study.","In conclusion, a high level of magnesium intake (that meet RDA) may be beneficial in terms of primary prevention of pancreatic cancer. Adhering to the RDA for magnesium intake is recommended. To achieve that level, dietary magnesium intake alone may not be sufficient. Magnesium supplementation may help achieve the RDA for magnesium, especially for those who may have an elevated risk of pancreatic cancer, such as those with family history of pancreatic cancer or diabetes mellitus. Further research is needed to confirm our findings and to establish causal inference.",https://twitter.com/foundmyfitness/status/1440743052188913668,
Atherogenic Lipoprotein Particles in Atherosclerosis,"One serving (small handful) of walnuts per day lowered total LDL particle number by 4.3% and decreased small, dense LDL particle number by 6.1% (large intervention trial with over 700 participants). Small, dense LDL particles are the most atherogenic.","The importance of low-density lipoprotein (LDL) cholesterol in the development of atherosclerosis has long been recognized, and LDL cholesterol remains the primary target of therapy for the prevention of coronary heart disease. Nevertheless, increasing research attention over the past decade has been devoted to the heterogeneity of LDL particles and the atherogenicity of lipids and lipoproteins other than LDL. Particularly atherogenic forms of LDL include small, dense LDL particles and oxidized LDL. All lipoproteins that contain apolipoprotein B, such as LDL, very-low-density lipoprotein, and intermediate-density lipoprotein, tend to promote atherosclerosis; however, these particles differ in their apolipoprotein and triglyceride content. High levels of plasma triglycerides increase the risk of acute coronary events. Lipoprotein(a) is now considered an independent risk factor in both men and women. Ultimately, better understanding of the roles of these lipid particles and subfractions in the initiation and progression of atherosclerosis may affect treatment decisions.","Epidemiological studies have shown a positive relationship between total cholesterol concentrations and mortality from coronary heart disease (CHD). Total cholesterol does not accurately predict the risk of CHD in many patients, however, because it is the sum of all cholesterol carried not only by atherogenic lipoproteins (eg, very-low-density lipoprotein [VLDL], low-density lipoprotein [LDL], intermediate-density lipoprotein [IDL]) but also by antiatherogenic lipoproteins (ie, high-density lipoprotein [HDL]). Therefore, the decision to treat is based on LDL-cholesterol values. Yet, treatment decisions may also need to take into account LDL heterogeneity, which has been recognized for many years. Small, dense LDL particles are more atherogenic than large, buoyant LDL particles, and oxidation of LDL also increases its atherogenicity. In addition, LDL belongs to the group of lipoproteins that contain apolipoprotein (apo) B-100. Some of the particles in this highly heterogeneous group contain other apolipoproteins, such as apo C-II, apo C-III, and apo E. Furthermore, some particles are larger and rich in triglycerides (large VLDL), whereas others are smaller and rich in cholesteryl esters (small VLDL, IDL). It is now known that remnant lipoproteins containing apo C-III are highly atherogenic, as is lipoprotein(a) [Lp(a)], another member of the apo B-100 group. This article reviews recent studies involving LDL subclasses and atherogenic lipoproteins, many of which used novel methods of lipoprotein subfractioning.","Elevated LDL-cholesterol levels are associated with a high risk of CHD, and LDL cholesterol continues to be the primary target of therapy for the prevention of CHD. However, the heterogeneity of LDL particles and the increasing recognition of the atherogenicity of other lipoproteins, remnant lipoprotein particles, and certain apolipoproteins, demands attention. Physicians need to be aware of these other atherogenic lipoproteins because they may become the targets of therapeutic intervention in the future. Small, dense LDL particles are highly atherogenic, and high levels of circulating oxidized LDL increase the risk of CHD. Lipoproteins that contain apo B are highly heterogeneous in terms of chemical composition and size. Further fundamental and clinical studies are needed to characterize these lipoproteins and their capacity to induce atherosclerosis. For the time being, apo B might be of greatest value in the diagnosis and treatment of men and women with some common lipid abnormalities but normal or low concentrations of LDL cholesterol. The apo B/apo A-I ratio should also be regarded as highly predictive in evaluating cardiac risk. Elevated plasma triglyceride levels increase the risk of acute coronary events and are an independent risk factor, but the concentrations of remnant particles associated with apo C-III are more related to the development of atherosclerosis than are triglycerides per se. Lipoprotein(a) is now considered an independent risk factor for CHD in both men and women. An important challenge is to develop standardized and simple analytical methods to generalize the measurement of different atherogenic lipoprotein particles in clinical biochemistry and in clinical practice.",https://twitter.com/foundmyfitness/status/1437890400778215433,
Effects of Twelve Sessions of High-Temperature Sauna Baths on Body Composition in Healthy Young Men,"Anti-atrophy effects of sauna have been known for a while, but new research suggests it might actually promote muscle growth!
Men that took 12 sauna sessions over 12 weeks increased muscle, bone mineral content, and bone mineral density (vs controls).","The health benefits of sauna baths are attracting ever-increasing interest. Therefore, the purpose of this study was to evaluate the effects of 12 high-temperature (100 °C) sauna baths on body composition of 23 healthy young men, divided into a control group (CG) and a sauna group (SG). Both groups were initially evaluated by dual-energy X-ray absorptiometry (DXA), after which the SG experienced 12 sessions of sauna baths at high temperatures (100 °C). Initial measurements were carried out after the sauna sessions and after two weeks of decay in both groups. The muscle mass of the right leg (pre vs. decay: 9.50 (5.59) vs. 10.52 (5.15); p < 0.05; Δ 1.07%), bone mineral density (pre vs. post: 1.221 (0.35) vs. 1.315 (0.45); p < 0.05; Δ 7.7%) and bone mineral content (pre vs. post: 0.470 (0.21) vs. 0.499 (0.22); p < 0.05; Δ 6.17%) of the left leg increased in the SG after the sauna baths. It seems that exposure to heat at high temperatures could produce improvements in bone and muscle mass.","There has been growing interest in sauna baths over the last few years, and they are increasingly being installed in homes and sports centers. Currently, the uses of the sauna bath are sporting, recreational or rehabilitative . Several studies suggest that sauna baths could provide numerous health benefits , as their use is inversely related to mortality. Heat exposure at high temperatures causes stress in the organism. This stress is detected by peripheral receptors (thermoreceptors and hotoreceptors) and central receptors (hypothalamus), reacting to the magnitude of the stress . The organism’s responses are acute and chronic. The acute responses are promoted by the autonomic nervous system, by releasing catecholamines and glucocorticoids [8], eliciting a peripheral blood flow distribution to the skin, increasing sweating in order to remove heat, and provoking, in addition, a rise in heart rate and respiratory rate . The chronic response produces adaptations such as expansion of plasma volume, decreased heart rate, increased sweat rate, reduced mineral excretion through sweating, and improved heat tolerance . These adaptions enhance the cardiovascular system. However, the influence of sauna baths on body composition has not been studied in depth. It is known that sauna baths elicit a fall in body water due to the rise in sweating. Some authors have reported associations between body composition parameters and sauna baths . The changes observed in previous studies were related to the loss of body fluids through sweating. Nevertheless, the aforementioned studies carried out short acclimation protocols of 4 sessions or less. Thus, the investigation of the effects of high-temperature sauna baths on body composition is still incomplete. Likewise, to our knowledge, there are no studies which have investigated sauna baths at a temperature above 90 °C. Heat stress increases the synthesis of heat shock proteins (HSP). These proteins act as molecular chaperones that protect cells by binding to denatured proteins to prevent their aggregation, help transport repair proteins, and transport toxic metabolites for their degradation. Heat stress could promote muscle mass hypertrophy. reported that heat stress activates the kinase B protein/rapamycin target in mammals (Akt/mTOR), a signaling pathway, related to protein synthesis, in rat skeletal muscle. In addition, heat stress attenuates muscle atrophy in immobile persons . Moreover, it has been reported that heat stress could have benefits on lipid metabolism . Similarly, it has been observed that HSP could have a positive effect on bone metabolism . Previous investigations in animals suggest that heat stimulus could promote significant osteogenesis ; however, no human studies have been found in the scientific literature on this factor. Molecular changes in bone tissues reported in animals has prompted research in humans. Therefore, based on previous research, we hypothesized that the regular use of sauna baths could have positive effects on body composition. Hence, the aim of the present study was to analyze the effect of 12 sessions of sauna baths (100 ± 2 °C) on body composition, evaluated with DEXA, in healthy young men.","Twelve sessions of sauna baths at high temperatures (100 ± 2 °C) do not produce changes in fat parameters. However, in view of the results obtained in the present study, they could perhaps influence muscle and bone parameters. Our study shows that sauna baths could be a promising treatment option for diseases associated with bone mineral alterations and sarcopenia. Further studies in humans are needed to confirm these encouraging results.",https://twitter.com/foundmyfitness/status/1437486820015493120,
Sauna use as a lifestyle practice to extend healthspan,The sauna robustly elevates heat shock protein levels that can persist for up to 48hrs. Heat shock proteins play a preventative role in neurodegenerative diseases and sauna use is associated with a 66% lower risk of Alzheimer's,"Sauna use, sometimes referred to as “sauna bathing,” is characterized by short-term passive exposure to high temperatures, typically ranging from 45 ◦C to 100 ◦C (113 ◦ F to 212 ◦F), depending on modality. This exposure elicits mild hyperthermia, inducing a thermoregulatory response involving neuroendocrine, cardiovascular, and cytoprotective mechanisms that work in a synergistic fashion in an attempt to maintain homeostasis. Repeated sauna use acclimates the body to heat and optimizes the body's response to future exposures, likely due to the biological phenomenon known as hormesis. In recent decades, sauna bathing has emerged as a probable means to extend healthspan, based on compelling data from observational, interventional, and mechanistic studies. Of particular interest are the findings from large, prospective, population-based cohort studies of health outcomes among sauna users that identified strong dose-dependent links between sauna use and reduced morbidity and mortality. This review presents an overview of sauna practices; elucidates the body's physiological response to heat stress and the molecular mechanisms that drive the response; enumerates the myriad health benefits associated with sauna use; and describes sauna use concerns","The evolving field of aging research has undergone dramatic shifts in recent decades, as the prevailing view of aging as a non-modifiable inevitability has given way to the possibilities of extending lifespan and, even more promising, healthspan. A widely accepted definition of healthspan is the period of one's life spent in good health, free from the chronic diseases and disabilities that commonly accompany aging . Healthspan extension compresses the time spent in ill health, shifting it to one's later years. Sauna use has emerged as a probable means to increase lifespan and extend healthspan.

Bathing oneself in heat for the purposes of purification, cleansing, and healing is an ancient practice, observed for thousands of years across many cultures. Variations of its use appear today in the banyas of Russia, the sweat lodges of the American Indians, and the saunas of Finland. Sauna use, sometimes referred to as “sauna bathing,” is characterized by short-term passive exposure to high temperatures, typically ranging from 45 °C to 100 °C (113 °F to 212 °F), depending on modality. This exposure elicits mild hyperthermia, an increase in the body's core temperature that induces a thermoregulatory response involving neuroendocrine, cardiovascular, and cytoprotective mechanisms that participate in restoring homeostasis and conditioning the body for future stressors (Laukkanen et al., 2018a).

","Sauna bathing is associated with many health benefits, from cardiovascular and cognitive health to physical fitness and muscle maintenance. It is generally considered safe for healthy adults and may be safe for special populations with appropriate medical supervision. Heat stress via sauna use elicits hormetic responses driven by molecular mechanisms that protect the body from damage, similar to those elicited by moderate- to vigorous-intensity exercise, and may offer a means to forestall the effects of aging and extend healthspan.",https://twitter.com/foundmyfitness/status/1430938112369197061,
"Short Sleep Duration in Working American Adults, 2010–2018",A jump in the number of people sleeping <6hr from 2010 (30%) to 2018 (35%). Hardest hit: those in healthcare (45%) and the military (50%). This is unsustainable chronic sleep deprivation for maintaining human health.,"Short sleep duration is detrimental to physical and mental health. In this study, we explored the epidemiology of short sleep duration (<7 h) in working American adults from 2010 to 2018. Data from the National Health Interview Survey (NHIS) were analyzed to describe the prevalence and trends of short sleep duration by demographic and employment characteristics of working American adults. Overall, the prevalence of short sleep duration in working American adults increased signifcantly from 2010 to 2018 (30.9% in 2010 to 35.6% in 2018). Across the 9-year study period, short sleep duration prevalence varied signifcantly by demographic characteristics (i.e. age, race, marital status, number of children in the household, residing region, level of education) and occupational characteristics. Compared to 2010, the odds of short sleep duration were statistically signifcantly higher in 2018 despite adjusting for demographic characteristics (25% higher) and occupational characteristics (22% higher). In 2018, the highest levels of short sleep duration were found for the following categories of jobs: protective service and military (50%), healthcare support occupations (45%), transport and material moving (41%), and production occupations (41%). Sleep hygiene education may be especially useful for those in occupations with high rates of short sleep duration.","Sleep problems are under-recognized as a major population health concern. Inadequate sleep (<7 h) is associated with mild to severe physical and mental health problems, injury, loss of productivity, and premature mortality. Two meta-analyses that reviewed the impact of shortened sleep duration on adults found an association with a 9% increase in the risk of type-2 diabetes (for each hour of decreased sleep duration) [1], a 15% increase in strokes, a 23% increase in hypertension risk, and a 48% increase in coronary heart disease. Inadequate sleep is also associated with impulsivity, anxiety, alcohol abuse, absenteeism (workplace absence), presenteeism (below par work performance), unstable moods, and suicidal ideation. In addition to the health impairments caused by sleep deficiency, the economic costs of inadequate sleep in the U.S. have been estimated to be $411 billion annually. A plethora of studies have estimated the prevalence, predictors, and correlates of sleep disturbances in the adult American population. However, these studies have major limitations. First, a number of studies were conducted with populations via clinics or healthcare facilities. Second, due to convenience sampling, the sample size of these studies is generally limited to a certain geographic region or population, which results in small sample sizes and biases inherent to such study designs. Third, the limited number of larger studies published on the adult American population are limited to durations of 1 year or less (i.e. one point in time, precluding trend analyses over a longer period). Fourth, the studies published often do not focus on working adult American populations that are unique groups (i.e. employment related confounding could limit the ability of published studies to elucidate the true extent of sleep problems in the working American adult population). Finally, published studies on inadequate sleep in adult American populations do not adequately characterize the afected population by a number of vital sociodemographic factors beyond age, race, and gender (e.g. geography, type of employment, education). These limitations can be overcome, in part, by utilizing large national random samples of adults in the U.S. that are representative of the working American population. Thus, the purpose of this study was to describe the trends, patterns, and prevalence of short sleep durations in working American adults over a period of 9 years (2010–2018) utilizing the National Health Interview Surveys (NHIS) . Specifcally, we wanted to explore: what are the trends in prevalence of short sleep duration in the working American adult population? How did the prevalence of short sleep duration in working American adults vary by sociodemographic characteristics from 2010 to 2018? What employment characteristics were related to short sleep duration in the working American adult population from 2010 to 2018?","Almost a third of working adults in the U.S. get inadequate quantities of sleep. Most likely, those who work long hours, engage in changing shifts, or those in high stress professions that have minimal control over their work and life schedules are at risk of short sleep duration and the subsequent social, physical and mental health consequences of sleep problems. Noteworthy groups are racial/ethnic minorities, the less educated, and those who are living alone are at a greater risk for reporting short sleep duration among working American adults. Employers that are willing to help employees develop adequate sleep times may increase the probability of workplace productivity, reduction in employee healthcare costs, and improving workplace safety and health. Sleep hygiene education may be one method to help employees optimize their levels of sleep and reduce a signifcant form of preventable harm. For individuals with pathologies and chronic comorbidities, early clinical interventions may help reap signifcant benefts.",https://link.springer.com/article/10.1007/s10900-019-00731-9,
Sleep Habits and Susceptibility to the Common Cold,People who had <7 hours of sleep per night were ~3x more likely to develop a cold than those with ≥ 8 hours after administration with nasal drops containing rhinovirus. Those with <92% sleep efficiency were 5.5x more likely to develop a cold than those with ≥ 98% efficiency.,"Sleep quality is thought to be an important predictor of immunity and, in turn, susceptibility to the common cold. This article examines whether sleep duration and efficiency in the weeks preceding viral exposure are associated with cold susceptibility. A total of 153 healthy men and women (age range, 21-55 years) volunteered to participate in the study. For 14 consecutive days, they reported their sleep duration and sleep efficiency (percentage of time in bed actually asleep) for the previous night and whether they felt rested. Average scores for each sleep variable were calculated over the 14-day baseline. Subsequently, participants were quarantined, administered nasal drops containing a rhinovirus, and monitored for the development of a clinical cold (infection in the presence of objective signs of illness) on the day before and for 5 days after exposure. There was a graded association with average sleep duration: participants with less than 7 hours of sleep were 2.94 times (95% confidence interval [CI], 1.18-7.30) more likely to develop a cold than those with 8 hours or more of sleep. The association with sleep efficiency was also graded: participants with less than 92% efficiency were 5.50 times (95% CI, 2.08-14.48) more likely to develop a cold than those with 98% or more efficiency. These relationships could not be explained by differences in prechallenge virus-specific antibody titers, demographics, season of the year, body mass, socioeconomic status, psychological variables, or health practices. The percentage of days feeling rested was not associated with colds. Poorer sleep efficiency and shorter sleep duration in the weeks preceding exposure to a rhinovirus were associated with lower resistance to illness.","It is commonly thought that poor sleep increases our susceptibility to the common cold. However, there is little direct evidence for this assertion. Experimental studies have demonstrated that sleep deprivation results in poorer immune function, such as reduced natural killer cell activity, suppressed interleukin-2 production, and increased levels of circulating proinflammatory cytokines. Sleep deprivation has also been found to attenuate antibody response to both hepatitis A and influenza immunizations. The only direct evidence that sleep habits are associated with cold susceptibility derives from a secondary analysis of data from a rhinovirus (RV)-challenge study in which a single retrospective questionnaire assessing sleep habits during the previous month was used to assess sleep efficiency (the percentage of time a person actually sleeps between lying down to sleep and waking up the next morning). Efficiencies below 80% predicted a greater risk for the development of verifiable illness.In this study, we examined whether sleep habits are associated with resistance to a common cold. Instead of retrospective reports, we obtained estimates of sleep habits by averaging respondent reports of sleep duration, efficiency, and “feeling rested” across 14 consecutive days. After sleep assessments were completed, the participants were exposed to an RV and were monitored to see whether they developed clinical illness. Infection and signs and symptoms of illness were assessed the day before and for 5 days after the viral challenge. This design extends previous work by providing reliable (averaged over 14 days) online (collected daily) measures of baseline sleep; by allowing the comparison of the relative importance of sleep duration, efficiency, and feeling rested for cold susceptibility; and by providing the opportunity to test for graded relationships between sleep measures and disease susceptibility.","In sum, according to our study results, measures of sleep predicted susceptibility to the development of a cold. Although both shorter sleep duration and lower sleep efficiency were associated with risk for illness, duration did not predict independently of efficiency, which was a stronger overall correlate of illness. Although the prospective design does not allow causal inference, it does eliminate reverse causation as an explanation. Because of the prospective design and the controls for multiple confounding factors, these results strongly suggest the possibility of sleep playing a causal role in cold susceptibility. Moreover, the use of a maximally reliable multiple day assessment of sleep habits increases our confidence in the findings of this study.",https://twitter.com/foundmyfitness/status/1483924723901763584,
Dietary Omega-3 Polyunsaturated Fatty-Acid Supplementation Upregulates Protective Cellular Pathways in Patients with Type 2 Diabetes that reduces Painful Diabetic Neuropathy,"Interesting, even low-dose DHA significantly increased Glutathione activity/precursors in diabetic subjects, increased glycerol-3-phosphate by 78% & alpha-ketobutyrate (important for NAD+ regeneration) by 35% while lowering 2-hydroxybutyrate* by 18% etc.","t: Background: Omega-3 polyunsaturated fatty acids (PUFAs) are increasingly reported to improve chronic neuroinflammatory diseases in peripheral and central nervous systems. Specifically, docosahexaenoic acid (DHA) protects nerve cells from noxious stimuli in vitro and in vivo. Recent reports link PUFA supplementation to improving painful diabetic neuropathy (pDN) symptoms. However, the molecular mechanism behind omega-3 PUFAs ameliorating pDN symptoms is lacking. Therefore, we sought to determine the distinct cellular pathways that omega3 PUFAs dietary supplementation promotes in reducing painful neuropathy in type 2 diabetes mellitus (DM2) patients. Methods: Forty volunteers diagnosed with type 2 diabetes were enrolled in the ""En Balance-PLUS"" diabetes education study. The volunteers participated in weekly lifestyle/nutrition education and daily supplementation with 1,000 mg DHA and 200 mg eicosapentaenoic acid. The Short-Form McGill Pain Questionnaire validated clinical determination of baseline and post-intervention pain complaints. Laboratory and untargeted metabolomics analyses were conducted using blood plasma collected at baseline and after three months of participation in the dietary regimen. The metabolomics data was analyzed using random forest, hierarchical cluster, ingenuity pathway analysis, and metabolic pathway mapping. Results: We found that metabolites involved in oxidative stress and glutathione production shifted significantly to a more anti-inflammatory state post supplementation. Example of these metabolites include cystathionine (+90%), S-methylmethionine (+9%), glycine cysteine-glutathione disulfide (+157%) cysteinylglycine (+19%), glutamate (-11%), glycine (+11%) and arginine (+13.4%). In addition, the levels of phospholipids associated with improved membrane fluidity such as linoleoyldocosahexaenoyl-glycerol (18:2/22:6) (+253 %) were significantly increased. Ingenuity pathway analysis suggested several key bio functions associated with omega-3 PUFA supplementation such as formation of reactive oxygen species (p = 4.38 × 10-4, z-score = -1.96), peroxidation of lipids (p = 2.24 × 10-5, z-score = -1.944), Ca2+ transport (p = 1.55 × 10-4, z-score = -1.969), excitation of neurons (p = 1.07 ×10-4, z-score = -1.091), and concentration of glutathione (p = 3.06 × 10-4, z-score = 1.974). Conclusion: The reduction of pro-inflammatory and oxidative stress pathways following omega3 PUFAS supplementation is consistent with using omega-3 PUFAs as a complementary dietary strategy as part of the overall treatment of pDN.","Painful diabetic neuropathy (pDN) is a common comorbidity of DM2 and has a significant negative impact on both quality of life and productivity of patients with DM2. Effective therapeutic interventions are sorely lacking and there is a great unmet need for novel and safe therapeutics. Currently treatment is focused on symptoms rather than targeting the disease process resulting in only a third of treated patients achieving 50% pain relief, often complicated by side effects. In part, the development of therapies is complicated because of the inherently variable and complex nature of pDN. Although progress has been made in understanding the pathophysiology of diabetic neuropathy, attempts to use various targeted therapies has not resulted in either consistent or sustained benefits in patients with neuropathy. Progress in alleviating painful neuropathy is critically dependent on an understanding of the pathogenesis of pDN which to date is incompletely defined and we will continue to struggle in developing new pharmacotherapies that address the underlying biological dysregulation associated with pDN. Pre-clinical models suggest that the pathophysiology of pDN involves both microvascular and metabolic changes, which ultimately lead to increased oxidative stress, inflammation, and mitochondrial dysfunction. However, corroborating human data currently is insufficient in the context of pDN. This knowledge gap may be closing with the development and use of highthroughput metabolomics analysis as we can now identify critical biomarkers and elucidate major metabolic pathways involved in the pathophysiology of different disease states . For instance, metabolomics analysis can provide a readout of interactions between an individual's genome, diet, and environment in clinical studies. Growing evidence has implicated PUFAs in diminishing the adverse effects of chronic neuroinflammatory diseases in both the peripheral and central nervous system. The treatment with docosahexaenoic acid (DHA) has shown significant protection in nerve cells both in cell culture and in vivo. Furthermore, prophylactic dietary omega-3 PUFAs protect against spinal cord injury-induced chronic pain. In agreement with these findings, pre-clinical models have found that oral administration of omega-3 PUFAs can regenerate and protect peripheral nerves from injury. Recently, PUFA supplementation has been linked to the amelioration of painful diabetic neuropathy symptoms within a clinical cohort. However, the effects on a cellular basis that are related to any benefits within humans attributed to omega-3 PUFAs and pDN is lacking. The present study examines the underlying metabolome of participants with type 2 diabetes reporting pDN and the impact of DHA-rich supplementation on their metabolic profile. This study represents, to our knowledge, the first unbiased attempt, in humans with type-2 diabetes, to identify the biologically critical metabolic features that define pDN and how a dietary DHA-enriched supplementation alters pDN patient's metabolome during a three-month quasi experimental designed intervention. Of significance, our study identifies biochemical signatures associated with pDN and determines how a simple nutritional invention leads to significant reversal.","We demonstrated that dietary DHA-enriched supplementation has a wide-ranging impact on the antioxidant metabolomic profile of our participants. Overall, this study shows that untargeted metabolomics is sensitive for defining targeted alteration in metabolism secondary to a dietary intervention. Specifically, we found that a dietary DHA-enriched supplementation improved metabolic profiles regarding omega-3 PUFA metabolism, glycerolipid metabolism, cysteine, methionine, glutathione metabolism, and glucose and fatty acid homeostasis without changing clinical parameters. Furthermore, circulating plasma metabolites showed a significant shift toward decreased reactive oxygen species biosynthesis, lipid peroxidation, improved Ca2+ homeostasis, and increased glutathione activity. These changes may explain previously reported reduction in neuropathic pain symptoms in our En Balance plus cohort. Since omega-3 FA supplementation is safe in patients with type 2 diabetes, future studies are warranted to define further its role in the possible improvement of pDN symptoms .",https://twitter.com/onelifemax/status/1483849703930355716,
Somnogenic Cytokines and Models Concerning Their Effects on Sleep,"From what we know about sleep, the idea that too much is deleterious has always felt jarring. Should we practice sleep restriction if we're sleeping too much? Reality: It may be a spurious association since cytokines can have a somnogenic quality.","All the sleep-promoting substances currently identified also have other biological activities. Despite years of effort, a single specific central nervous system sleep center has not been described. These observations led us to propose a biochemical model of a sleep activational system in which the effects of several sleep factors are integrated into a regulatory scheme. These sleep factors interact by altering the metabolism, production, or activity of each other and thereby result in multiple feedback loops. This web of interactions leads to sleep stability in that minor challenges to the system will not greatly alter sleep. The system, however, is responsive to strong perturbations, such as sleep deprivation and infectious disease. The sleep-promoting effects of cytokines and their interactions with prostaglandins and the neuroendocrine system are used to illustrate the functioning of a part of the sleep activational system under normal conditions and during infectious disease. Although the actions of individual sleep factors are not specific to sleep, their interactions at various levels of the neuraxis can mediate a specific sleep response. Such a system would also be responsive to the autonomic and environmental parameters that alter sleep.","The concept that sleep is regulated in part by humoral mechanisms is supported by experimental demonstrations of somnogenic substances in tissue fluids (reviewed, ). These experiments began at the turn of the century with Legendre and Pieron and Ishimori, who described the accumulation during prolonged wakefulness of substances in cerebrospinal fluid (CSF) that induce excess sleep when transferred to recipient animals. Although the chemical identity of these substances was never established, today over 30 putative sleep factors (SFs) have been identified (reviewed, ). Most of these putative SFs are hormones, immunomodulators, and/or substances involved in endocrine and/or immune regulation. Until recently, it was postulated that a SF, herein defined as any substance found within the body that alters sleep, would have biological actions specific to sleep and would act on central nervous system (CNS) executive sleep centers, which were also assumed to be concerned primarily with sleep regulation. A single CNS center necessary for sleep has not, however, been demonstrated. Furthermore, all SFs identified to date have multiple biological activities. Thus, a revision of the original assumptions is necessary. This review describes a model that links the sleep effects of many putative SFs in a sleep activational system. The sleep activational system presented is fundamentally a biochemical model and, like all models, is provisional; the cellular localization of many of the specific components that are germane to sleep regulation is not known. Nevertheless, it is assumed that specific components of the model ultimately interact with either glia and/or neurons at various levels of the CNS and that such interactions lead to altered sleep. We propose that such a system would be responsive to the many autonomic and environmental parameters that influence sleep, yet would retain the capacity to regulate sleep selectively. Rapid-eye-movement sleep (REMS) and non-rapid-eye-movement sleep (NREMS) are the two major types of sleep, although subdivisions of each of these classes have been made. To identify these states of vigilance, the electroencephalogram (EEG) and other physiological measurements, e.g., electromyogram, brain temperature, and motor activity, are recorded. REMS and NREMS states exist in most mammals. Sleep in animals, however, is slightly different from that of humans. Thus, species often used in sleep research, such as rats, cats, and rabbits, have sleep episodes that are relatively short, typically lasting only for a few minutes. These bouts of sleep occur sporadically throughout the 24-hour day, alternating with episodes of wakefulness, although the percentage of any given hour spent in sleep is strongly influenced by circadian rhythms.","From the discussions presented here, it is clear that cytokines and many other substances alter sleep. We have presented a model of how the regulation of many SFs may be interrelated and another model illustrating how SFs may interact with various neuronal sets to produce specific sleep responses. Ten years ago, it would have been unrealistic to present such models because the roster of putative SFs was much smaller, and there was very little information concerning how SFs might affect each other; however, despite the great number of advances within the field of SFs, many questions remain unanswered. Major challenges that need to be addressed include: (1) How do SF concentrations change with sleep/wake cycles and under pathological conditions? (2) What is the timing of these changes? (3) Where do these changes take place? (4) Is one change linked directly to another?",https://twitter.com/foundmyfitness/status/1450894696448815107,
"Vitamin C supplementation promotes mental vitality in healthy young adults: results from a cross‑sectional analysis and a randomized, double‑blind, placebo‑controlled trial",Young adults given 500 mg of vitamin C twice a day for 4 weeks had better cognitive performance requiring sustained attention and improved work motivation compared to those given a placebo.,"We aimed to investigate the link of vitamin C status with vitality and psychological functions in a cross-sectional study, and examine their causal relationship through a randomized controlled trial (RCT).We frst conducted a population-based cross-sectional investigation of healthy young adults (n=214, 20–39 years), and analyzed the associations of serum vitamin C concentrations with vitality (fatigue and attention) and mood status (stress, depression, and positive and negative afect) using Pearson’s correlation and multiple linear regression analyses. Next, we performed a double-blind RCT in healthy subjects whose serum vitamin C concentrations were inadequate (<50 μmol/L).Subjects were randomly allocated to receive 500 mg of vitamin C twice a day for 4 weeks (n=24) or a placebo (n=22). We assessed vitality, which included fatigue, attention, work engagement, and self-control resources, and measured mood status, including stress, depression, positive and negative afect, and anxiety. ELISA determined serum brain-derived neurotrophic factor (BDNF), and a Stroop color–word test evaluated attention capacity and processing speed. s In the cross-sectional data, the serum vitamin C concentration was positively associated with the level of attention (r=0.16, p=0.02; standardized β=0.21, p=0.003), while no signifcant associations with the levels of fatigue and mood variables being found. In the RCT, compared to the placebo, the vitamin C supplementation signifcantly increased attention (p=0.03) and work absorption (p=0.03) with distinct tendency of improvement on fatigue (p=0.06) and comprehensive work engagement (p=0.07). The vitamin C supplementation did not afect mood and serum concentrations of BDNF. However, in the Stroop color–word test, the subjects supplemented with vitamin C showed better performance than those in the placebo group (p=0.04).n Inadequate vitamin C status is related to a low level of mental vitality. Vitamin C supplementation efectively increased work motivation and attentional focus and contributed to better performance on cognitive tasks requiring sustained attention","Vitamin C (ʟ-ascorbic acid or ascorbate) is an essential nutrient in humans that functions as an indispensable electron donor and a cofactor in various biological reactions such as hydroxylation of collagen, biosynthesis of carnitine, and tyrosine metabolism. Interestingly, vitamin C presents its highest concentrations in the brain, and animal model and in vitro studies have reported that vitamin C performs critical roles in brain functions. Vitamin C protects neurons from oxidative stress, induces diferentiation and maturation of neurons, and regulates the synthesis or release of   euromodulating factors including serotonin, catecholamines, and glutamate. Accordingly, vitamin C is inferred to be important for maintaining normal mental health. Humans rely on dietary supply to obtain vitamin C due to the absence of a gene encoding ʟ-gulonolactone oxidase, which is critical for vitamin C synthesis from glucose. Although vitamin C defciency can be prevented by consuming one or two servings of citrus fruits or vegetables; reports state that an inadequate vitamin C status is prevalent among young adults even in industrialized countries . The poor vitamin C status in the young can be attributed to external factors such as smoking, excessive drinking, and unhealthy eating habits that fail to provide a fresh and balanced diet rich in vitamin C. Thus, even healthy young individuals can be at risk of vitamin C defciency, and consequently, poor body functions due to these lifestyle-related factors.However, compared to the elderly, inadequate vitamin C status in the young is liable to remain undiagnosed or be considered as being of little importance. Vitality is defned as a subjective feeling of energy and aliveness, which highlights the psychological aspects [9]. Feeling vital is suggested as a key component in healthy psychological functioning, the ability of self-regulation, work performance, and goal achievement. Classically, vitality decline is known as the earliest sign of scurvy, a clinical symptom of severe vitamin C defciency; it manifests in fatigue, decreases in arousal and motivation, and cognitive impairment. Considering that professional and social engagement is highest in the young population, it is necessary to investigate whether improvement of vitamin C status helps to promote their vitality and work performance. However, the link of vitamin C status with vitalityrelated psychological and cognitive functions at a young age is equivocal and their causal relationship has rarely been examined.Hence, we frst investigated the associations between vitamin C status and subjective vitality in a healthy young population. Next, using a randomized, double-blind, placebocontrolled trial, we further explored the efects of vitamin C supplementation on vitality, such as work motivation and self-regulatory resources, and cognitive performance in young adults with inadequate vitamin C status.","In conclusion, this study is the frst, to our knowledge, to show the link between vitamin C status with mental functions in healthy young adults using both populationbased observational studies and randomized clinical trials. The cross-sectional study suggests inadequate vitamin C status is related to a low level of mental vitality. In the randomized clinical trial, vitamin C supplementation at 1000 mg/day for 4 weeks efectively increases serum vitamin C concentrations in subjects with suboptimal vitamin C status. The supplementation promotes their mental vitality, especially work motivation and attentional focus, contributing to better performance on cognitive tasks that require sustained attention.",https://twitter.com/foundmyfitness/status/1445106700164292608,
Magnesium intake and incidence of pancreatic cancer: the VITamins and Lifestyle study,"Large study including over 66,000 men and women found that for every 100 mg decrease in magnesium intake there was a 24% increase in pancreatic cancer independent of age, body mass, and gender. ~45% of the US population has insufficient magnesium intake.","Studies document that magnesium is inversely associated with the risk of diabetes, which is a risk factor of pancreatic cancer. However, studies on the direct association of magnesium with pancreatic cancer are few and findings are inconclusive. In this study, we aimed to investigate the longitudinal association between magnesium intake and pancreatic cancer incidence in a large prospective cohort study. A cohort of 66 806 men and women aged 50–76 years at baseline who participated in the VITamins And Lifestyle (VITAL) study was followed from 2000 to 2008. Multivariable-adjusted Cox regression models were used to estimate hazard ratios (HRs) and 95% confidence intervals (CIs) of pancreatic cancer incidence by magnesium intake categories. During an average of 6.8-year follow-up, 151 participants developed pancreatic cancer. Compared with those who met the recommended dietary allowance (RDA) for magnesium intake, the multivariable-adjusted HRs (95% CIs) for pancreatic cancer were 1.42 (0.91, 2.21) for those with magnesium intake in the range of 75–99% RDA and 1.76 (1.04, 2.96) for those with magnesium intake <75% RDA. Every 100 mg per day decrement in magnesium intake was associated with a 24% increase in the incidence of pancreatic cancer (HR: 1.24; 95% CI: 1.02, 1.50; Ptrend=0.03). The observed inverse associations appeared not to be appreciably modified by age, gender, body mass index, and non-steroidal anti-inflammatory drug use but appeared to be limited to those taking magnesium supplementation (from multivitamins or individual supplement). Findings from this prospective cohort study indicate that magnesium intake may be beneficial in terms of primary prevention of pancreatic cancer.","Pancreatic cancer is the fourth leading cause of cancer-related death in both men and women in the United States . The overall incidence of pancreatic cancer has not significantly changed since 2002 but the mortality rate has increased an average of 0.4% annually from 2002-2011 (National Cancer Institute, 2014). It was estimated that about 46 420 people in the United States would be diagnosed with pancreatic cancer and about 39 590 would die of this disease in 2014 .Approximately 80% of pancreatic cancer patients have concomitant diabetes  Studies show that pancreatic tumour cells have receptors for insulin and have high levels of insulin as in type-2 diabetes and insulin resistance. Insulin and insulin-like growth factor (IGF) promote pancreatic tumour cell growth. Because epidemiological studies suggest that magnesium intake is inversely associated with the risk of type-2 diabetes  – a risk factor of pancreatic cancer, it is reasonable to hypothesise that magnesium intake may decrease the risk of pancreatic cancer. However, data directly relating magnesium intake to the incidence of pancreatic cancer are sparse and the findings are inconsistent. Two case–control studies reported that magnesium intake was inversely associated with the risk of pancreatic cancer, whereas another case–control study  found no association. Two recent prospective cohort studies, one from the Health Professionals Follow-up Study conducted only in men and the other from the European Prospective Investigation of Cancer (EPIC) study,  found no association between magnesium intake and the incidence of pancreatic cancer in the total cohort, but both observed an inverse association (borderline in EPIC study) among overweight men. Therefore, we aimed to (1) investigate the longitudinal association between magnesium intake and the incidence of pancreatic cancer, and (2) explore whether age, gender, body mass index (BMI, kg m−2), non-steroidal anti-inflammatory drugs (NSAIDs) use and magnesium supplementation are effect modifiers in this large prospective cohort study.","In conclusion, a high level of magnesium intake (that meet RDA) may be beneficial in terms of primary prevention of pancreatic cancer. Adhering to the RDA for magnesium intake is recommended. To achieve that level, dietary magnesium intake alone may not be sufficient. Magnesium supplementation may help achieve the RDA for magnesium, especially for those who may have an elevated risk of pancreatic cancer, such as those with family history of pancreatic cancer or diabetes mellitus. Further research is needed to confirm our findings and to establish causal inference.",https://twitter.com/foundmyfitness/status/1440743052188913668,
Potential reversal of epigenetic age using a diet and lifestyle intervention: a pilot randomized clinical trial,A new study found that some people that previously had mild COVID-19 illness develop antibody-producing cells in the bone marrow that can last a lifetime. Antibody-producing cells against SARS-CoV2 were also found in people 11 months after first symptoms.,"Manipulations to slow biological aging and extend healthspan are of interest given the societal and healthcare costs of our aging population. Herein we report on a randomized controlled clinical trial conducted among 43 healthy adult males between the ages of 50-72. The 8-week treatment program included diet, sleep, exercise and relaxation guidance, and supplemental probiotics and phytonutrients. The control group received no intervention. Genome-wide DNA methylation analysis was conducted on saliva samples using the Illumina Methylation Epic Array and DNAmAge was calculated using the online Horvath DNAmAge clock (2013). The diet and lifestyle treatment was associated with a 3.23 years decrease in DNAmAge compared with controls (p=0.018). DNAmAge of those in the treatment group decreased by an average 1.96 years by the end of the program compared to the same individuals at the beginning with a strong trend towards significance (p=0.066). Changes in blood biomarkers were significant for mean serum 5-methyltetrahydrofolate (+15%, p=0.004) and mean triglycerides (-25%, p=0.009). To our knowledge, this is the first randomized controlled study to suggest that specific diet and lifestyle interventions may reverse Horvath DNAmAge (2013) epigenetic aging in healthy adult males. Larger-scale and longer duration clinical trials are needed to confirm these findings, as well as investigation in other human populations.","Advanced age is the largest risk factor for impaired mental and physical function and many non-communicable diseases including cancer, neurodegeneration, type 2 diabetes, and cardiovascular disease . The growing health-related economic and social challenges of our rapidly aging population are well recognized and affect individuals, their families, health systems and economies. Considering economics alone, delaying aging by 2.2 years (with associated extension of healthspan) could save $7 trillion over fifty years. This broad approach was identified to be a much better investment than disease-specific spending. Thus, if interventions can be identified that extend healthspan even modestly, benefits for public health and healthcare economics will be substantial.DNA methylation is the addition of a methyl group to cytosine residues at selective areas on a chromosome (e.g. CpG islands, shelf/shore, exons, open sea). Methylation constitutes the best-studied, and likely most resilient of many mechanisms controlling gene expression. Unique among epigenetic markers, DNA methylation can readily and cheaply be mapped from tissue samples. Of 20+ million methylation sites on the human genome, there are a few thousand at which methylation levels are tightly correlated with age. Currently, the best biochemical markers of an individual’s age are all based on patterns of methylation. This has led some researchers to propose that aging itself has its basis in epigenetic changes (including methylation changes) over time.As of this writing, the best-studied methylation-based clock is the multi-tissue DNAmAge clock . At the time this study design was approved, there were few viable alternatives. Horvath’s DNAmAge clock predicts all-cause mortality and multiple morbidities better than chronological age. Methylation clocks (including DNAmAge) are based on systematic methylation changes with age. DNAmAge clock specifically demonstrates about 60% of CpG sites losing methylation with age and 40% gaining methylation. This is distinct from stochastic changes, “methylation drift”, unpredictable changes which vary among individuals and cell-by-cell within individuals. Systematic methylation changes include hypermethylation in promotor regions of tumor suppressor genes (inhibiting expression) and hypomethylation promoting inflammatory cytokines (promoting expression). Saliva can be considered a good source of high-quality DNA, containing both white blood cells and buccal cells, and is a suitable tissue type to be assessed for the DNAmAge clock.","The significance of these findings is multi-factorial, but primarily as the first demonstration of potential reversal of epigenetic age in a randomized, controlled clinical trial, accounting for any normal variability in epigenetic methylation. This is the second report of a diet and lifestyle intervention reducing biologic aging in individuals otherwise known to be healthy. Notably, the shorter timeframe of this study and the scale of potential reduction, while modest in magnitude, may correlate with meaningful socioeconomic benefits, and appears to have the potential to be broadly achievable.Vitamin D3 at a dose of 4,000 IU/d for 16 weeks has previously been shown to decrease the DNAmAge clock measurement by 1.85 years in overweight/obese African Americans with a serum 25-hydroxyvitamin D [25(OH)d] <50 nmol/L . Subsequently, a one-year regimen of daily injection of growth hormone plus one prescription drug and three nutritional supplements was shown to set back the DNAmAge clock by 1.5 years in 9 middle-aged men (plus the 1-year study duration = 2.5 years) . More recently, a 1-year non controlled pilot trial involving 120 participants aged 65-79 years (including 60 Italians, 60 Poles) drawn from the larger NU-AGE cohort found a non-significant trend towards reversal of the DNAmAge clock after 1 year of a Mediterranean diet plus 400IU of vitamin D3. However, subgroup analysis did reveal a significant 1.47-year age decreases in female Polish participants (n=36) and in individuals with a baseline higher epigenetic age. It was noted in the study that Poland is a country with a non-Mediterranean baseline diet. In the present study, biological age set-back was achieved in eight weeks, using similarly non-invasive, and otherwise generally beneficial interventions known to have mechanistic plausibility for affecting methylation pathways.The increase in circulating folate demonstrates that dietary sources and folate-producing probiotics can be an effective method of nutrient repletion. The reduction in serum triglycerides might be expected with a diet that lowered carbohydrate intake and glycemic response, plus exercise [32]. While we expected to see a decrease in homocysteine with an intervention that supplied additional dietary B vitamins and betaine, as well as exercise, the average starting homocysteine value of the treatment group was 10.9 umol/L, already within a range typically identified as “normal” (<15 umol/L).",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8064200/,
"Total and added sugar intakes, sugar types, and cancer risk: results from the prospective NutriNet-Santé cohort","Total sugar intake is associated with a higher overall cancer risk independent of body fat. The associations were more pronounced for sucrose, non–fruit-derived sugars, and added and natural sugars present in sugary drinks.","Excessive sugar intake is now recognized as a key risk factor for obesity, type 2 diabetes, and cardiovascular diseases. In contrast, evidence on the sugar–cancer link is less consistent. Experimental data suggest that sugars could play a role in cancer etiology through obesity but also through inflammatory and oxidative mechanisms and insulin resistance, even in the absence of weight gain.The objective was to study the associations between total and added sugar intake and cancer risk (overall, breast, and prostate), taking into account sugar types and sources.  In total, 101,279 participants aged >18 y (median age, 40.8 y) from the French NutriNet-Santé prospective cohort study (2009–2019) were included (median follow-up time, 5.9 y). Sugar intake was assessed using repeated and validated 24-h dietary records, designed to register participants’ usual consumption for >3500 food and beverage items. Associations between sugar intake and cancer risk were assessed by Cox proportional hazard models adjusted for known risk factors (sociodemographic, anthropometric, lifestyle, medical history, and nutritional factors). Total sugar intake was associated with higher overall cancer risk (n = 2503 cases; HR for quartile 4 compared with quartile 1: 1.17; 95% CI: 1.00, 1.37; Ptrend = 0.02). Breast cancer risks were increased (n = 783 cases; HRQ4vs.Q1 = 1.51; 95% CI: 1.14, 2.00; Ptrend = 0.0007). Results remained significant when weight gain during follow-up was adjusted for. In addition, significant associations with cancer risk were also observed for added sugars, free sugars, sucrose, sugars from milk-based desserts, dairy products, and sugary drinks (Ptrend ≤ 0.01). These results suggest that sugars may represent a modifiable risk factor for cancer prevention (breast in particular), contributing to the current debate on the implementation of sugar taxation, marketing regulation, and other sugar-related policies.","Despite public health recommendations and dietary guidelines of the WHO advising to limit “free sugars” [including both added and natural sugars present in fruit juices, honey, and syrups ] up to 5% of calories, consumption remains excessive in most occidental countries. For instance, in the United States, 13% of total energy is brought by free sugars, and in France, sugars from sugary products contribute to 11% of energy intake and 20% of adults exceed 100 g of sugar per day. While the associations of sugars on cardiometabolic outcomes have been extensively studied , associations with cancer risk have been much less investigated. Given the limited and inconsistent existing literature, the World Cancer Research Fund and American Institute for Cancer Research (WCRF/AICR) concluded, in their latest report, that the evidence base was not sufficient for a sugar and cancer association. Similarly, Makaren recently performed a systematic review on sugars and related exposures and overall and sitespecific cancers but concluded that, despite strong mechanistic plausibility, epidemiological data were still limited. Pancreatic and other gastrointestinal cancers have been the main cancer sites studied so far. In contrast, few studies have explored the associations between sugars and breast  and prostate cancers, which are the most prevalent cancers in women and men, respectively, in several countries, and even fewer studies considered added sugars in relation with breast and prostate cancers. Literature on added sugars is even more limited due to methodologic limitations of dietary assessment tools in many cohort studies (i.e., FFQs), which do not adequately capture added sugar intake.","In conclusion, in this large population-based prospective cohort, higher sugar intake was associated with overall cancer and, more specifically, breast cancer risk, independently of weight gain and weight status. These findings need to be replicated in other large-scale prospective studies (notably involving more overweight/obese participants) and supported by experimental data to clarify underlying mechanisms. Nonetheless, they suggest that sugars may represent a modifiable risk factor for cancer prevention. Globally, the implementation of tax on sugary drinks and foods, as well as other sugar-related policies (e.g., the regulation of food advertising and marketing, the fixation of reference standards limiting sugar content according to product categories), is currently debated. In a context where sugar consumption is increasing in Western countries, and adding to its well-established cardiometabolic detrimental effects, these results contribute to building the evidence base suggesting that public health policies addressing sugar intake should also consider their role in cancer etiology.",https://pubmed.ncbi.nlm.nih.gov/32936868/,
Atherogenic Lipoprotein Particles in Atherosclerosis,"One serving (small handful) of walnuts per day lowered total LDL particle number by 4.3% and decreased small, dense LDL particle number by 6.1% (large intervention trial with over 700 participants). Small, dense LDL particles are the most atherogenic.","The importance of low-density lipoprotein (LDL) cholesterol in the development of atherosclerosis has long been recognized, and LDL cholesterol remains the primary target of therapy for the prevention of coronary heart disease. Nevertheless, increasing research attention over the past decade has been devoted to the heterogeneity of LDL particles and the atherogenicity of lipids and lipoproteins other than LDL. Particularly atherogenic forms of LDL include small, dense LDL particles and oxidized LDL. All lipoproteins that contain apolipoprotein B, such as LDL, very-low-density lipoprotein, and intermediate-density lipoprotein, tend to promote atherosclerosis; however, these particles differ in their apolipoprotein and triglyceride content. High levels of plasma triglycerides increase the risk of acute coronary events. Lipoprotein(a) is now considered an independent risk factor in both men and women. Ultimately, better understanding of the roles of these lipid particles and subfractions in the initiation and progression of atherosclerosis may affect treatment decisions.","Epidemiological studies have shown a positive relationship between total cholesterol concentrations and mortality from coronary heart disease (CHD). Total cholesterol does not accurately predict the risk of CHD in many patients, however, because it is the sum of all cholesterol carried not only by atherogenic lipoproteins (eg, very-low-density lipoprotein [VLDL], low-density lipoprotein [LDL], intermediate-density lipoprotein [IDL]) but also by antiatherogenic lipoproteins (ie, high-density lipoprotein [HDL]). Therefore, the decision to treat is based on LDL-cholesterol values. Yet, treatment decisions may also need to take into account LDL heterogeneity, which has been recognized for many years. Small, dense LDL particles are more atherogenic than large, buoyant LDL particles, and oxidation of LDL also increases its atherogenicity. In addition, LDL belongs to the group of lipoproteins that contain apolipoprotein (apo) B-100. Some of the particles in this highly heterogeneous group contain other apolipoproteins, such as apo C-II, apo C-III, and apo E. Furthermore, some particles are larger and rich in triglycerides (large VLDL), whereas others are smaller and rich in cholesteryl esters (small VLDL, IDL). It is now known that remnant lipoproteins containing apo C-III are highly atherogenic, as is lipoprotein(a) [Lp(a)], another member of the apo B-100 group. This article reviews recent studies involving LDL subclasses and atherogenic lipoproteins, many of which used novel methods of lipoprotein subfractioning.","Elevated LDL-cholesterol levels are associated with a high risk of CHD, and LDL cholesterol continues to be the primary target of therapy for the prevention of CHD. However, the heterogeneity of LDL particles and the increasing recognition of the atherogenicity of other lipoproteins, remnant lipoprotein particles, and certain apolipoproteins, demands attention. Physicians need to be aware of these other atherogenic lipoproteins because they may become the targets of therapeutic intervention in the future. Small, dense LDL particles are highly atherogenic, and high levels of circulating oxidized LDL increase the risk of CHD. Lipoproteins that contain apo B are highly heterogeneous in terms of chemical composition and size. Further fundamental and clinical studies are needed to characterize these lipoproteins and their capacity to induce atherosclerosis. For the time being, apo B might be of greatest value in the diagnosis and treatment of men and women with some common lipid abnormalities but normal or low concentrations of LDL cholesterol. The apo B/apo A-I ratio should also be regarded as highly predictive in evaluating cardiac risk. Elevated plasma triglyceride levels increase the risk of acute coronary events and are an independent risk factor, but the concentrations of remnant particles associated with apo C-III are more related to the development of atherosclerosis than are triglycerides per se. Lipoprotein(a) is now considered an independent risk factor for CHD in both men and women. An important challenge is to develop standardized and simple analytical methods to generalize the measurement of different atherogenic lipoprotein particles in clinical biochemistry and in clinical practice.",https://twitter.com/foundmyfitness/status/1437890400778215433,
Effects of Twelve Sessions of High-Temperature Sauna Baths on Body Composition in Healthy Young Men,"Anti-atrophy effects of sauna have been known for a while, but new research suggests it might actually promote muscle growth! Men that took 12 sauna sessions over 12 weeks increased muscle, bone mineral content, and bone mineral density (vs controls).","The health benefits of sauna baths are attracting ever-increasing interest. Therefore, the purpose of this study was to evaluate the effects of 12 high-temperature (100 °C) sauna baths on body composition of 23 healthy young men, divided into a control group (CG) and a sauna group (SG). Both groups were initially evaluated by dual-energy X-ray absorptiometry (DXA), after which the SG experienced 12 sessions of sauna baths at high temperatures (100 °C). Initial measurements were carried out after the sauna sessions and after two weeks of decay in both groups. The muscle mass of the right leg (pre vs. decay: 9.50 (5.59) vs. 10.52 (5.15); p < 0.05; Δ 1.07%), bone mineral density (pre vs. post: 1.221 (0.35) vs. 1.315 (0.45); p < 0.05; Δ 7.7%) and bone mineral content (pre vs. post: 0.470 (0.21) vs. 0.499 (0.22); p < 0.05; Δ 6.17%) of the left leg increased in the SG after the sauna baths. It seems that exposure to heat at high temperatures could produce improvements in bone and muscle mass.","There has been growing interest in sauna baths over the last few years, and they are increasingly being installed in homes and sports centers. Currently, the uses of the sauna bath are sporting, recreational or rehabilitative . Several studies suggest that sauna baths could provide numerous health benefits , as their use is inversely related to mortality. Heat exposure at high temperatures causes stress in the organism. This stress is detected by peripheral receptors (thermoreceptors and hotoreceptors) and central receptors (hypothalamus), reacting to the magnitude of the stress . The organism’s responses are acute and chronic. The acute responses are promoted by the autonomic nervous system, by releasing catecholamines and glucocorticoids , eliciting a peripheral blood flow distribution to the skin, increasing sweating in order to remove heat, and provoking, in addition, a rise in heart rate and respiratory rate . The chronic response produces adaptations such as expansion of plasma volume, decreased heart rate, increased sweat rate, reduced mineral excretion through sweating, and improved heat tolerance . These adaptions enhance the cardiovascular system. However, the influence of sauna baths on body composition has not been studied in depth. It is known that sauna baths elicit a fall in body water due to the rise in sweating. Some authors have reported associations between body composition parameters and sauna baths . The changes observed in previous studies were related to the loss of body fluids through sweating. Nevertheless, the aforementioned studies carried out short acclimation protocols of 4 sessions or less. Thus, the investigation of the effects of high-temperature sauna baths on body composition is still incomplete. Likewise, to our knowledge, there are no studies which have investigated sauna baths at a temperature above 90 °C. Heat stress increases the synthesis of heat shock proteins (HSP). These proteins act as molecular chaperones that protect cells by binding to denatured proteins to prevent their aggregation, help transport repair proteins, and transport toxic metabolites for their degradation. Heat stress could promote muscle mass hypertrophy. reported that heat stress activates the kinase B protein/rapamycin target in mammals (Akt/mTOR), a signaling pathway, related to protein synthesis, in rat skeletal muscle. In addition, heat stress attenuates muscle atrophy in immobile persons . Moreover, it has been reported that heat stress could have benefits on lipid metabolism . Similarly, it has been observed that HSP could have a positive effect on bone metabolism . Previous investigations in animals suggest that heat stimulus could promote significant osteogenesis ; however, no human studies have been found in the scientific literature on this factor. Molecular changes in bone tissues reported in animals has prompted research in humans. Therefore, based on previous research, we hypothesized that the regular use of sauna baths could have positive effects on body composition. Hence, the aim of the present study was to analyze the effect of 12 sessions of sauna baths (100 ± 2 °C) on body composition, evaluated with DEXA, in healthy young men.","Twelve sessions of sauna baths at high temperatures (100 ± 2 °C) do not produce changes in fat parameters. However, in view of the results obtained in the present study, they could perhaps influence muscle and bone parameters. Our study shows that sauna baths could be a promising treatment option for diseases associated with bone mineral alterations and sarcopenia. Further studies in humans are needed to confirm these encouraging results.",https://twitter.com/foundmyfitness/status/1437486820015493120,
Sauna use as a lifestyle practice to extend healthspan,The sauna robustly elevates heat shock protein levels that can persist for up to 48hrs. Heat shock proteins play a preventative role in neurodegenerative diseases and sauna use is associated with a 66% lower risk of Alzheimer's,"Sauna use, sometimes referred to as “sauna bathing,” is characterized by short-term passive exposure to high temperatures, typically ranging from 45 ◦C to 100 ◦C (113 ◦ F to 212 ◦F), depending on modality. This exposure elicits mild hyperthermia, inducing a thermoregulatory response involving neuroendocrine, cardiovascular, and cytoprotective mechanisms that work in a synergistic fashion in an attempt to maintain homeostasis. Repeated sauna use acclimates the body to heat and optimizes the body's response to future exposures, likely due to the biological phenomenon known as hormesis. In recent decades, sauna bathing has emerged as a probable means to extend healthspan, based on compelling data from observational, interventional, and mechanistic studies. Of particular interest are the findings from large, prospective, population-based cohort studies of health outcomes among sauna users that identified strong dose-dependent links between sauna use and reduced morbidity and mortality. This review presents an overview of sauna practices; elucidates the body's physiological response to heat stress and the molecular mechanisms that drive the response; enumerates the myriad health benefits associated with sauna use; and describes sauna use concerns","The evolving field of aging research has undergone dramatic shifts in recent decades, as the prevailing view of aging as a non-modifiable inevitability has given way to the possibilities of extending lifespan and, even more promising, healthspan. A widely accepted definition of healthspan is the period of one's life spent in good health, free from the chronic diseases and disabilities that commonly accompany aging . Healthspan extension compresses the time spent in ill health, shifting it to one's later years. Sauna use has emerged as a probable means to increase lifespan and extend healthspan.Bathing oneself in heat for the purposes of purification, cleansing, and healing is an ancient practice, observed for thousands of years across many cultures. Variations of its use appear today in the banyas of Russia, the sweat lodges of the American Indians, and the saunas of Finland. Sauna use, sometimes referred to as “sauna bathing,” is characterized by short-term passive exposure to high temperatures, typically ranging from 45 °C to 100 °C (113 °F to 212 °F), depending on modality. This exposure elicits mild hyperthermia, an increase in the body's core temperature that induces a thermoregulatory response involving neuroendocrine, cardiovascular, and cytoprotective mechanisms that participate in restoring homeostasis and conditioning the body for future stressors.","Sauna bathing is associated with many health benefits, from cardiovascular and cognitive health to physical fitness and muscle maintenance. It is generally considered safe for healthy adults and may be safe for special populations with appropriate medical supervision. Heat stress via sauna use elicits hormetic responses driven by molecular mechanisms that protect the body from damage, similar to those elicited by moderate- to vigorous-intensity exercise, and may offer a means to forestall the effects of aging and extend healthspan.",https://twitter.com/foundmyfitness/status/1430938112369197061,
"Meat consumption and risk of incident dementia: cohort study of 493,888 UK Biobank participants",Unprocessed red meat consumption was associated with a lower dementia and Alzheimer's disease incidence while consumption of processed meat was associated with higher risks.,"Worldwide, the prevalence of dementia is increasing and diet as a modifiable factor could play a role. Meat consumption has been cross-sectionally associated with dementia risk, but specific amounts and types related to risk of incident dementia remain poorly understood. We aimed to investigate associations between meat consumption and risk of incident dementia in the UK Biobank cohort. Meat consumption was estimated using a short dietary questionnaire at recruitment and repeated 24-h dietary assessments. Incident all-cause dementia comprising Alzheimer disease (AD) and vascular dementia (VD) was identified by electronic linkages to hospital and mortality records. HRs for each meat type in relation to each dementia outcome were estimated in Cox proportional hazard models. Interactions between meat consumption and the apolipoprotein E (APOE) ε4 allele were additionally explored. Among 493,888 participants included, 2896 incident cases of all-cause dementia, 1006 cases of AD, and 490 cases of VD were identified, with mean ± SD follow-up of 8 ± 1.1 y. Each additional 25 g/day intake of processed meat was associated with increased risks of incident all-cause dementia (HR: 1.44; 95% CI: 1.24, 1.67; P-trend < 0.001) and AD (HR: 1.52; 95% CI: 1.18, 1.96; P-trend = 0.001). In contrast, a 50-g/d increment in unprocessed red meat intake was associated with reduced risks of all-cause dementia (HR: 0.81; 95% CI: 0.69, 0.95; P-trend = 0.011) and AD (HR: 0.70; 95% CI: 0.53, 0.92; P-trend = 0.009). The linear trend was not significant for unprocessed poultry and total meat. Regarding incident VD, there were no statistically significant linear trends identified, although for processed meat, higher consumption categories were associated with increased risks. The APOE ε4 allele increased dementia risk by 3 to 6 times but did not modify the associations with diet significantly.These findings highlight processed-meat consumption as a potential risk factor for incident dementia, independent of the APOE ε4 allele.","Dementia is a major public health concern with around 50 million cases globally and an incidence of nearly 10 million new cases per annum. It comprises Alzheimer disease (AD), which contributes to 50–70% of dementia cases, vascular dementia (VD), which contributes to ∼25%, and other forms of dementia. Dementia development and progression are associated with both genetic and environmental factors, including diet and lifestyle. Lifestyle-related and dietary factors associated with dementia are potentially modifiable and thus represent targets for primary prevention.Meat consumption has gained increasing interest in relation to health, since high consumption of processed meat and probably red meat were found to be consistently associated with an increased risk of colorectal cancer. In recent decades meat consumption has doubled or even tripled globally, especially in developing countries. This dietary transition has been associated with increasing AD prevalence in Japan, Peru, Cuba and other low- and middle-income countries in both ecological and cross-sectional studies. A study of cognitively healthy individuals in Sweden showed that low consumption of meat and meat products was associated with better cognitive performance in clinical dementia screening tests and greater total brain volume after a 5-y follow-up period. Our previous review on meat consumption and cognitive disorders including dementia showed that most meat-related studies were embedded in complex dietary patterns with considerable heterogeneity, and the evidence of associations between risk of dementia and specific types or amounts of meat consumption was limited.A consistent association has been established between carriage of the apolipoprotein E (APOE) ε4 allele and elevated risk of dementia or AD. Previous stratified analyses by APOE ε4 status showed that unfavourable lifestyle factors (e.g., less healthy dietary pattern, less physical activity, smoking, and social isolation) were associated with higher risk of dementia in APOE ε4 noncarriers but not in carriers. The discrepancy between carriers and noncarriers indicates that APOE genotype may modify associations between lifestyle factors and dementia risks, and might be explained by a potential masking of weak associations from lifestyle factors by the strongly associated APOE ε4 allele. However, at present whether APOE ε4 allele carriage interacts with lifestyle factors, such as diet, influencing risk of dementia remains unclear.In the present study we examined the hypothesis that high consumption of meat increases the incidence of dementia in the general population, which may be more pronounced among APOE ε4 noncarriers.","Our findings suggest that consumption of processed meat may increase risk of incident dementia, and unprocessed red meat intake may be associated with lower risks, independent of APOE ε4 carriage. On the basis of the findings of this study, more specific public health guidance could be indicated differentiating between types of meat. However further research is recommended to confirm these results. Overall, the research adds to the growing body of evidence linking meat, especially processed meat consumption, to increased risk of a range of noncommunicable diseases.",https://twitter.com/foundmyfitness/status/1417907227294191616,
Very-low-carbohydrate diet enhances human T-cell immunity through immunometabolic reprogramming,A ketogenic diet may improve immunity and reduce excessive inflammation. Participants on a ketogenic diet for 3 weeks had enhanced activity of antibody-producing B cells and increased number and activity of regulatory T cells which prevent autoimmunity.,"Very-low-carbohydrate diet triggers the endogenous production of ketone bodies as alternative energy substrates. There are as yet unproven assumptions that ketone bodies positively affect human immunity. We have investigated this topic in an in vitro model using primary human T cells and in an immuno-nutritional intervention study enrolling healthy volunteers. We show that ketone bodies profoundly impact human T-cell responses. CD4+, CD8+, and regulatory T-cell capacity were markedly enhanced, and T memory cell formation was augmented. RNAseq and functional metabolic analyses revealed a fundamental immunometabolic reprogramming in response to ketones favoring mitochondrial oxidative metabolism. This confers superior respiratory reserve, cellular energy supply, and reactive oxygen species signaling. Our data suggest a very-low-carbohydrate diet as a clinical tool to improve human T-cell immunity. Rethinking the value of nutrition and dietary interventions in modern medicine is required.","Western diet is increasingly seen as being at the root of many diseases, such as metabolic syndrome, autoimmune disorders, and cancer, and thus is suspected to limit life expectancy during the 21st century . It impairs cellular immunity and evokes systemic low-grade inflammation not only by causing obesity but also by direct reprogramming of immune cells toward a proinflammatory phenotype . Nutritional interventions may hold promise as a tool to prevent and even to treat disease. Unfortunately, most recommendations on food intake and dietary guidelines yet lack substantiated scientific background . Novel nutritional concepts promote a restriction of carbohydrates in favor of fat to ameliorate detrimental low-grade inflammation . However, large observational studies investigating this approach are highly controversial, and molecular data in humans are scarce.In this regard, the high-fat low-carbohydrate ketogenic diet (KD) is one highly discussed approach. Restriction of carbohydrate intake leads to the endogenous production of ketone bodies such as beta-hydroxybutyrate (BHB) as evolutionary conserved alternative metabolic substrates, which can be utilized via mitochondrial oxidative phosphorylation . In animal models, BHB has been shown to dampen inappropriate innate immune responses via suppression of the NLRP3 inflammasome, thus ameliorating chronic low-grade inflammation and associated diseases . Human adaptive immunity, however, has not yet been addressed. Here, we present the first study investigating the influence of KD on human immune responses in vitro and in a cohort of healthy subjects. Our results reveal profound beneficial effects of ketone bodies on human T-cell immunity. BHB improves effector and regulatory T-cell function and primes human T memory cell differentiation both in vitro and in vivo. These functional changes are based on a fundamental immunometabolic reprogramming, resulting in enhanced mitochondrial oxidative metabolism, thus conferring an increased immunometabolic capacity to human T cells. We provide molecular evidence that ketone bodies promptly improve human T-cell metabolism and immunity. By complementing classical approaches of modern medicine, nutritional interventions offer new perspectives for prevention and therapy of numerous diseases.","Our study demonstrates that KD induces a fundamental immunometabolic reprogramming in human T cells associated with profound transcriptomic changes. This leads to a balanced global enhancement of T-cell immunity, comprising enhanced cytokine production and secretion, strengthened cell lysis capacity, amplified Treg differentiation, and pronounced Tmem cell formation. KD thus holds promise as a feasible and effective clinical tool for a large range of conditions intimately associated with immune disorders. This provides the basis to proceed into further medical translation. Consequently, a clinical phase II study investigating KD in sepsis patients is currently recruiting (Rahmel et al, 2020). These new immunological aspects of KD might contribute to the modern concept of metabolic therapy of cancer . KD not only targets the Warburg effect, but could also strengthen anti-tumor immunity. Moreover, the different effects of BHB-induced ROS elevation on tumor cells and T cells might even lead to additive beneficial effects: While oxidative stress compromises cancer cell viability, mild increase in ROS enhances T-cell immune capacity which in turn further restrains tumor growth. However, these issues need to be addressed in future clinical studies.In conclusion, our study changes the perspective on nutrition as a clinical tool and could help to redefine the role of dietary interventions in modern medicine.",https://twitter.com/foundmyfitness/status/1410330583859830786,
Resveratrol and exercise combined to treat functional limitations in late life: a pilot randomized controlled trial,"A small pilot study found that exercise + resveratrol resulted in an increase of 449 meters in a 6-minute walk test and increased levels of citrate synthase, a common marker of mitochondrial volume in elderly people.","To evaluate the safety and feasibility of combining exercise (EX) and resveratrol to treat older adults with physical function limitations.Three-arm, two-site pilot randomized, controlled trial (RCT) for community-dwelling adults (N = 60), 71.8 ± 6.3 years of age with functional limitations. Participants were randomized to receive either 12 weeks of (1) EX + placebo [EX0], (2) EX + 500 mg/day resveratrol [EX500], or (3) EX + 1,000 mg/day resveratrol [EX1000]. EX consisted of two sessions a week for 12 weeks of center-based walking and whole-body resistance training. Safety was assessed through adverse events and feasibility through exercise session and supplement (placebo, or resveratrol) protocol adherence. Outcome measures included a battery of indices of physical function as well as skeletal muscle mitchondrial function. Data were adjusted for age and gender using the Intent-To-Treat approach. Adverse event frequency and type were similar between groups (n=8 EX0, n=12 EX500, and n=7 EX1000). Overall, 85% of participants met the supplement adherence via pill counts while 82% met the exercise session adherence. Adjusted within group mean differences (95% confidence interval) from week 0 to 12 for gait speed ranged from −0.04 (EX0: −0.1, 0.03) m/s to 0.04 (EX1000: −0.02, 0.11) and the six-minute walk test mean differences were 9.45 (EX0: −9.02, 27.7), 22.9 (EX500: 4.18, 41.6), and 33.1 (EX1000: 13.8, 52.4) meters. Unadjusted mean differences for citrate synthase were −0.80 (EX0: −15.45, 13.84), −1.38 (EX500: −12.16, 9.39), and 7.75 (EX1000: −4.68, 20.18) mU/mg. COX activity mean within group changes ranged from −0.05 (EX0) to 0.06 (EX500) k/sec/mg. Additional outcomes are detailed in the text. The pilot RCT indicated that combined EX + resveratrol was safe and feasible for older adults with functional limitations and may improve skeletal muscle mitochondrial function and mobility-related indices of physical function. A larger trial appears warranted and is needed to formally test these hypotheses.","Maintaining physical function during the aging process is critical to ward off disability and extend independence.However, despite the well-established relationship of decreased physical function with subsequent adverse health outcomes, few therapeutic strategies exist that are shown to improve these outcomes among older adults. To date, interventions incorporating physical exercise have demonstrated the most consistent results for improving physical function among older adults.  Still, despite the general benefits of exercise, physiologic responses to exercise can be quite variable [5 6] – thus adjuvant strategies may be useful to enhance the efficacy of exercise.One potential such adjuvant is the nutritional supplement resveratrol, a compound commonly found in red grapes. Resveratrol is a natural polyphenol with purported anti-oxidant, anti-inflammatory, and metabolic benefits. Preclinical studies have suggested greater potential benefits in combining exercise with resveratrol supplementation compared to exercise alone,  but existing data from humans are less supportive.  Still, the literature in humans remains sparse and novel interventions are needed to enhance the efficacy of exercise among older adults with physical limitations. Accordingly, the current pilot RCT expands on the limited literature to evaluate dose-dependent effects of resveratrol combined with physical exercise for older adults with functional limitations with an emphasis on safety and feasibility. As outlined previously, we designed a pilot, randomized controlled trial (RCT) to begin to test the hypothesis that resveratrol may potentiate the functional benefits of exercise among older adults at least partly by enhancing beneficial adaptations in skeletal muscle mitochondrial function in response to chronic exercise.The objective of this pilot trial was to collect data on the safety, feasibility, and potential efficacy of resveratrol supplementation combined with exercise training on indices of physical function and skeletal muscle mitochondrial function among older adults with functional limitations. The present manuscript provides findings from the pilot study designed to support or refute the need for a larger-scale trial necessary to fully test our central hypothesis. Notably, due to additional resources and tissue availability, were able to expand our initial battery of mitochondrial measures (mitochondrial DNA copy number, citrate synthase, cytochrome C oxidase) to include a more comprehensive evaluation of skeletal muscle mitochondrial function including indices of mitochondrial damage, fusion, fission, transcriptional regulation, as well as vascular inflammation and oxidative stress. These data provide important biologic insight, in addition to the clinical measures, to aid in evaluation of the potential need for a future, larger-scale trial in this area.","Aging of the worldwide population will have a massive burden of clinical and economic costs associated with age-related physical function. Moreover, declines in physical function occur with age and disablement. Research studies have demonstrated that exercise is a needed, yet insufficient, component of interventions to prevent age-related physical function. Therefore, it is critical to identify adjuvant therapies designed to optimize the efficacy of exercise which could ultimately have a positive impact on reducing functional limitations. Our pilot RCT has concluded that combined EX + resveratrol can be a safe and feasible for older adults with functional limitations. Moreover, EX + 1,000 mg/d resveratrol may have benefits for physical function and mitochondrial function. Ultimately, a fully powered trial is necessary to understand the effects of exercise and resveratrol combined for older adults with functional limitations.",https://twitter.com/foundmyfitness/status/1322611900815876097,
Improved metabolic function and cognitive performance in middle-aged adults following a single dose of wild blueberry,A wild blueberry extract improved memory and executive function in middle-aged adults compared to a placebo powder with similar macronutrient content. Similar cognitive improvements were found in children and older adults given blueberry powder.,"Research has demonstrated cognitive benefits following acute polyphenol-rich berry consumption in children and young adults. Berry intake also has been associated with metabolic benefits. No study has yet examined cognitive performance in middle-aged adults. We investigated the relationships among cognitive and metabolic outcomes in middle-aged adults following wild blueberry (WBB) consumption. Thirty-five individuals aged 40–65 years participated in a randomized, double blind, cross-over study. Participants consumed a breakfast meal and 1-cup equivalent WBB drink or matched placebo beverage on two occasions. Participants completed cognitive tasks and had blood drawn before and at regular intervals for 8 h after each meal/treatment. Changes in episodic memory and executive function (EF) were assessed alongside plasma levels of glucose, insulin, and triglyceride.Analysis of the memory-related Auditory Verbal Learning Task (AVLT) word recognition measure revealed a decrease in performance over the test day after placebo intake, whereas performance after WBB was maintained. For the AVLT word rejection measure, participants identified more foils following WBB in comparison to placebo. Benefits were also observed for EF on the Go/No-Go task with fewer errors following WBB intake on cognitively demanding invalid No-Go trials in comparison to placebo. Furthermore, in comparison to placebo, response times were faster for the Go/No-Go task, specifically at 4 h and 8 h following WBB treatment. We also observed reduced post-meal glucose and insulin, but not triglyceride, concentrations in comparison to placebo over the first 2 h following ingestion. Though the addition of Age, BMI, glucose and insulin as covariates to the analysis reduced the significant effect of beverage for AVLT word rejection, metabolic outcomes did not interact with treatment to predict cognitive performance with the exception of one isolated trend. This study indicated acute cognitive benefits of WBB intake in cognitively healthy middle-aged individuals, particularly in the context of demanding tasks and cognitive fatigue. WBB improved glucose and insulin responses to a meal. Further research is required to elucidate the underlying mechanism by which WBB improves cognitive function.","There is a substantial body of evidence demonstrating an association between habitual consumption of foods high in flavonoids and cognitive benefits including delayed cognitive decline with ageing . In addition, evidence from controlled intervention trials corroborate these findings, showing that supplementation with flavonoid-rich foods produce improvements in cognitive performance.The majority of human berry trials have investigated the effects of supplementation for periods of several weeks, although recent data suggest that effects of other flavonoid-rich food, such as cocoa, on brain function  and neurocognitive function can occur within hours of consumption. Likewise, in school-aged children, whole fruit blueberry powder delivered in a smoothie/juice drink was associated with improvements in executive function and memory performance 2–6 h following intake . In young adults, a smoothie of equal blueberry, strawberry, raspberry, and blackberry, was associated with improvements in executive function, again 2–6 h following intake. Furthermore, in older adults, global cognitive function was found to decline relative to baseline at 2 h following a control beverage whereas performance was maintained for a flavonoid-rich blueberry beverage . Importantly, these improvements in cognitive performance were demonstrated following berry intake at intervals similar to plasma peaks of blueberry anthocyanins and their metabolites 1–3 h after ingestion, as well as plasma peaks of different phenolic acid metabolites 2–3 h and 5 h post-consumption. Related to these timescales, reduced postprandial insulin (1–3 h) and attenuated postprandial inflammation have been shown to occur for up to 10 h in middle-aged, overweight and obese individuals consuming strawberries with a typical Western meal, and in a younger overweight group consuming strawberries 2 h before the meal, respectively . Similarly, red raspberry intake with a high carbohydrate breakfast meal reduced postprandial glycaemia and the concomitant insulin demand in overweight or obese individuals with pre-diabetes and insulin resistance across time frames that anthocyanin and phenolic metabolites were apparent in blood. Further, in a dose response study in individuals with obesity and insulin resistance, insulin and glucose responses after strawberry intake with a meal were associated with the main anthocyanin metabolite of strawberry, pelargonidin glucuronide.Collectively, there is evidence suggesting that cognitive benefits after a single administration of polyphenol-containing berry fruits occurs during a timeframe corresponding to both the pharmacokinetic profiles of berry (poly)phenols and biological effects associated with metabolic health. Notably, observational data suggest a strong link between metabolic syndrome and cognitive impairment, suggesting that dietary components and/or their metabolites that impact metabolic systems, also might impact cognitive function. However, these factors and their relationships have not been investigated in a clinical trial. Also, while evidence of immediate cognitive benefit following polyphenol intake has been observed in school-aged children, young adults, and older adults, there is limited research to date concerning such cognitive effects in middle-age, a period that is noteworthy because of the association of mid-life health conditions, particularly metabolic disturbance, with risk for late-life dementia. Therefore, the aim of this study was to examine relationships among cognitive performance and metabolic responses in middle-age adults following one-time intake of whole fruit wild blueberry powder","Here we have presented the effects of a polyphenol-rich, wild blueberry intervention on acute cognitive function and metabolic outcomes in middle-aged adults. The findings provide further support for the efficacy of wild blueberry on improving cognitive outcomes within this age group, particularly where there is increased cognitive demand. Wild blueberry was also found to reduce glucose and insulin concentrations in response to a meal over the initial 120 min having implications for post-meal metabolic control. Although there was little evidence of a direct relationship on cognition, these data have importance for structuring meal plans to reduce the metabolic burden in individuals with glucose homeostasis concerns.",https://twitter.com/foundmyfitness/status/1308496865382469633,
Effects of oily fish intake on cognitive and socioemotional function in healthy 8–9-year-old children: the FiSK Junior randomized trial,"Children given oily fish high in omega-3 fatty acids for 12 weeks had improved cognitive function, particularly attention and cognitive flexibility, and reduced socioemotional problems compared to children given poultry.","Long-chain n–3 PUFAs (n–3 LCPUFAs) accrete in the brain during childhood and affect brain development. Randomized trials in children show inconsistent effects of n–3 LCPUFAs on cognitive and socioemotional function, and few have investigated effects of fish per se. We aimed to investigate the effects of oily fish consumption on overall and domain-specific cognitive and socioemotional scores and explore sex differences. Healthy 8–9-y-old children (n = 199) were randomly allocated to receive ∼300 g/wk oily fish or poultry (control) for 12 ± 2 wk. At baseline and endpoint, we assessed attention, processing speed, executive functions, memory, emotions, and behavior with a large battery of tests and questionnaires and analyzed erythrocyte fatty acid composition. One hundred and ninety-seven (99%) children completed the trial. Children in the fish group consumed 375 (25th–75th percentile: 325–426) g/wk oily fish resulting in 2.3 (95% CI: 1.9, 2.6) fatty acid percentage points higher erythrocyte n–3 LCPUFA than in the poultry group. The overall cognitive performance score tended to improve by 0.17 (95% CI: −0.01, 0.35) points in children who received fish compared with poultry, supported by n–3 LCPUFA dose dependency. This was driven mainly by fewer errors [−1.9 (95% CI: −3.4, −0.3)] in an attention task and improved cognitive flexibility measured as faster reaction time [−51 ms (95% CI: −94, −7 ms)] in a complex relative to a simple task (“mixing cost”). The fish intervention furthermore reduced parent-rated Strength and Difficulties Questionnaire total difficulties by −0.89 (95% CI: −1.60, −0.18) points mainly due to a −0.63 (95% CI: −1.11, −0.16) points reduction in internalizing problems that was reflected in tendency to a decrease in the overall socioemotional problems score of −0.13 (95% CI: −0.26, 0.01) points. The overall effects were similar in boys and girls. Oily fish dose-dependently improved cognitive function, especially attention and cognitive flexibility, and reduced socioemotional problems. The results support the importance of n–3 LCPUFAs for optimal brain function and fish intake recommendations in children.","Nutrition plays an important role in brain development and cognitive function . Intake of n–3 long-chain PUFAs (n–3 LCPUFAs), especially DHA (22:6n–3), is considered particularly important for neuronal development during early infancy where a substantial amount of DHA is accreted in the brain. This accretion continues throughout childhood, especially in the frontal cortex involved in cognitive functions such as executive functions, attention, and memory as well as in socioemotional functions such as emotional regulation, impulse control, and social behavior, which continue to develop until early adulthood. Intake of n–3 LCPUFAs could therefore be important for cognitive and socioemotional development throughout childhood and adolescence.Some randomized controlled trials (RCTs) have shown effects of n–3 LCPUFA supplementation on cognitive and socioemotional outcomes in schoolchildren , but other studies have not found any effects , and meta-analyses of RCTs have not shown effects of n–3 LCPUFAs on any cognitive domains in healthy children. Oily fish are the primary dietary source of n–3 LCPUFAs, but fish also contain other nutrients that are considered important for the brain, for example, vitamin D, iodine, zinc, and vitamin B-12. Only a few RCTs have investigated the effect of fish per se. One trial in adolescents found that oily fish intake improved processing speed but reduced attention and did not affect mental health, and 2 trials in preschool children reported overall beneficial effects of oily fish consumption on cognitive performance but not mental health, and improvement in some tests of fluid intelligence but not in general intelligence. Furthermore, we previously showed that fish intake was associated with improved school performance but also with decreased attention in an RCT that compared fish-rich school meals with traditional Danish lunch packs in 8–11-y-old children.Most of the previous RCTs analyzed effects of fish or n–3 LCPUFAs without exploring potential sex differences, but few studies have reported sex-specific associations between n–3 LCPUFAs and cognitive outcomes. In the school meal intervention, we observed an increase in impulsivity only among boys, but their reading correctness also improved more than in girls. It is therefore important to focus on potential sex differences in the effects of n–3 LCPUFAs. The aim of this study was to investigate whether intake of oily fish compared with poultry affected cognitive function, focusing on measures of attention, processing speed, executive functions, and memory, as well as socioemotional function in healthy 8–9-y-old Danish children. Additionally, we examined whether the effects differed in boys and girls.","In conclusion, our findings support a dose-dependent beneficial effect of oily fish consumption on cognitive performance in healthy children and indicate improvements in attention, cognitive flexibility, and socioemotional problems. These results substantiate the importance of n–3 LCPUFAs for optimal brain function and recommendations of fish intake in children. Although a few sex-specific differences were indicated in attention and socioemotional measures, most effects were comparable in boys and girls. Future sufficiently powered studies are needed to understand the effects of fish consumption on specific cognitive and socioemotional domains and the potential influence of sex.",https://twitter.com/foundmyfitness/status/1296929813333078016,
Vitamin D deficiency as a predictor of poor prognosis in patients with acute respiratory failure due to COVID-19,"Severe vitamin D deficiency in people with COVID-19 was associated with a significantly higher mortality risk than COVID-19 patients with normal vitamin D levels (retrospective study in Bari, Italy).","Hypovitaminosis D is a highly spread condition correlated with increased risk of respiratory tract infections. Nowadays, the world is in the grip of the Coronavirus disease 19 (COVID 19) pandemic. In these patients, cytokine storm is associated with disease severity. In consideration of the role of vitamin D in the immune system, aim of this study was to analyse vitamin D levels in patients with acute respiratory failure due to COVID-19 and to assess any correlations with disease severity and prognosis.In this retrospective, observational study, we analysed demographic, clinical and laboratory data of 42 patients with acute respiratory failure due to COVID-19, treated in Respiratory Intermediate Care Unit (RICU) of the Policlinic of Bari from March, 11 to April 30, 2020. Eighty one percent of patients had hypovitaminosis D. Based on vitamin D levels, the population was stratified into four groups: no hypovitaminosis D, insufficiency, moderate deficiency, and severe deficiency. No differences regarding demographic and clinical characteristics were found. A survival analysis highlighted that, after 10 days of hospitalization, severe vitamin D deficiency patients had a 50% mortality probability, while those with vitamin D ≥ 10 ng/mL had a 5% mortality risk (p = 0.019). High prevalence of hypovitaminosis D was found in COVID-19 patients with acute respiratory failure, treated in a RICU. Patients with severe vitamin D deficiency had a significantly higher mortality risk. Severe vitamin D deficiency may be a marker of poor prognosis in these patients, suggesting that adjunctive treatment might improve disease outcomes.","Vitamin D deficiency and insufficiency is a worldwide condition, involving both adults and children, whose association with metabolic, autoimmune and infectious comorbidities has been extensively studied [1]. In particular, several studies highlighted a link between vitamin D deficiency and an increased risk of respiratory tract infections. Chalmers et al. demonstrated that patients with bronchiectasis and vitamin D deficiency were more likely to be chronically colonised with bacteria and had higher airway inflammation than patients with sufficient levels of vitamin D [2]. In a recent study conducted by Mamani et al., a low serum level of 25 hydroxyvitamin D (25(OH)D) was associated with a higher incidence of community-acquired pneumonia and more severe disease [3]. Moreover, vitamin D deficiency is common in critically ill patients and associates with adverse outcomes, as found by Dancer et al. in a cohort of patients with acute respiratory distress syndrome (ARDS). In that study, higher levels of vitamin D were detected in survivors of ARDS then in non-survivors, suggesting that supplementation of vitamin D might also have value as treatment for ARDS .Vitamin D is a fat-soluble vitamin produced from 7-dehydrocholesterol due to the action of UVB radiation and subsequently converted to 25(OH)D in the liver and then to the active form (calcitriol 1, 25(OH)D) in the kidneys or other organs. In addition to being involved in bone metabolism, facilitating the absorption of calcium and phosphorus from the intestinal tract, the role of vitamin D in the immune system has been studied in vitro [5–7]. A recent review analyzed the mechanisms by which vitamin D reduces the risk of microbial infections. It stimulates innate cellular immunity, through the induction of antimicrobial peptides, such as cathelicidins, IL-37 and defensins. It also inhibits the cytokine storm, reducing the production of pro-inflammatory cytokines such as IFNγ and TNFα. Finally, it modulates the adaptive immune response, suppressing the Th1 response and promoting cytokines production by Th2 cells. Nowadays, the world is experiencing a pandemic caused by infection with the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) [9]. Coronavirus disease 19 (COVID-19) is a respiratory tract infection whose clinical manifestations vary from mild to severe disease, sometimes requiring admission to intensive care (ICU) due to development of ARDS or sepsis [10, 11]. The cause of this extreme variability in clinical manifestations has been researched in disease pathogenesis. In a recent study conducted in a Wuhan Hospital in January 2020, it has been noted that COVID-19 patients had high concentrations of IL1B, IFNγ, IP10, and MCP1, probably leading to activated T-helper-1 (Th1) cell responses, and that these levels were higher in patients requiring ICU admission. These data suggest that the cytokine storm was associated with disease severity [12]. Therefore, the possibility of using anti-inflammatory and immunomodulating therapies in COVID 19 patients is gaining increasing attention. In our experience conducted in a Respiratory Intermediate Care Unit (RICU), we found a high prevalence of vitamin D deficiency and insufficiency in a cohort of patients with moderate-to-severe acute respiratory syndrome due to COVID-19. Thus, the aim of this study was to evaluate the possible correlation between vitamin D levels and disease severity in these patients. In consideration of evidence that hypovitaminosis D reduces innate cellular immunity and may stimulate the cytokine storm, which are involved in worsening COVID-19-related ARDS, we also aimed to assess if levels of vitamin D during COVID-19 could further influence the prognosis of those patients.","In conclusion, the results of our study show a high prevalence of hypovitaminosis D in COVID-19 patients treated in a RICU. Higher risk of mortality was found in patients with severe vitamin D deficiency. Further studies need to be conducted on a larger population, to demonstrate whether adjunctive treatment with vitamin D might be effective in improving disease outcomes and in reducing mortality risk.",https://twitter.com/foundmyfitness/status/1293620722074415106,
High sensitivity of melatonin suppression response to evening light in preschool‐aged children,Light exposure 1 hour before bedtime suppresses melatonin levels in pre-school children by as much as 98%. Children are much more sensitive to blue light than adults. Light dimmers or red lights at night are a good solution.,"Light at night in adults suppresses melatonin in a nonlinear intensity-dependent manner. In children, bright light of a single intensity before bedtime has a robust melatonin suppressing effect. To our knowledge, whether evening light of different intensities is related to melatonin suppression in young children is unknown. Healthy, good-sleeping children (n = 36; 3.0–4.9 years; 39% male) maintained a stable sleep schedule for 7 days followed by a 29.5-h in-home dim-light circadian assessment (~1.5 lux). On the final night of the protocol, children received a 1-h light exposure (randomized to one of 15 light levels, ranging 5–5000 lux, with ≥2 participants assigned to each light level) in the hour before habitual bedtime. Salivary melatonin was measured to calculate the magnitude of melatonin suppression during light exposure compared with baseline levels from the previous evening, as well as the degree of melatonin recovery 50 min after the end of light exposure. Melatonin levels were suppressed between 69.4% and 98.7% (M = 85.4 ± 7.2%) during light exposure across the full range of intensities examined. Overall, we did not observe a light intensity-dependent melatonin suppression response; however, children exposed to the lowest quartile of light intensities (5–40 lux) had an average melatonin suppression (77.5 ± 7.0%) which was significantly lower than that observed at each of the three higher quartiles of light intensities (86.4 ± 5.6%, 89.2 ± 6.3%, and 87.1 ± 5.0%, respectively). We further found that melatonin levels remained below 50% baseline for at least 50 min after the end of light exposure for the majority (62%) of participants, and recovery was not influenced by light intensity. These findings indicate that preschool-aged children are highly sensitive to light exposure in the hour before bedtime and suggest the lighting environment may play a crucial role in the development and the maintenance of behavioral sleep problems through impacts on the circadian timing system.","Circadian timing is determined by an individual's biology (e.g., circadian period) and their lighting environment. Light influences the circadian clock chiefly through stimulation of the eye's intrinsically photosensitive retinal ganglion cells (ipRGCs), melanopsin-expressing photoreceptors with a peak sensitivity to light of ~480  nm.1,2 When stimulated, the signal from the ipRGCs is transmitted via the retinohypothalamic tract to the suprachiasmatic nucleus (SCN), the master circadian clock. In turn, the SCN controls the pineal gland's production of the sleep-promoting hormone melatonin.3,4 Several findings suggest that this mechanism emerges early in mammalian development. In rodent models, the ipRGCs are functional and light sensitive from birth.5-7 Additionally, melanopsin is present in human eye tissue at eight weeks post-conception.8 Lastly, in pre-term infants, the pupillary light reflex is evoked by 470 nm blue light but not 635 nm red light,suggesting activation of the ipRGCs.9 Yet, despite the early development of this pathway, few experimental studies have examined the circadian response to light in early childhood. What is known about children's photosensitivity during the first decade of life suggests a strong melatonin suppression response to light. Compared with their parents, school-aged children (aged ~9 years) demonstrated nearly twice the melatonin suppression during an evening bright light exposure (580 lux).10 Additionally, 9-year olds’ melatonin was suppressed significantly more under home light levels(~140 lux) compared with dim-light conditions (<30 lux), a difference not observed in their parents.10 In preschoolers, we previously demonstrated that a 1-h exposure to bright light (1000 lux) in the hour before bedtime resulted in robust melatonin suppression (~90%) and that melatonin levelsremained attenuated 50 min afterthe end of the light stimulus.11 Children's photosensitivity is likely related to developmental changes in the eye, including larger pupils and clearer lenses than adults, allowing for greater light transmission.10,12-14 Together, these findings point to the importance of understanding the effects of the lighting environment on the maturing circadian clock. The adult circadian response to light is intensitydependent, and even low levels of evening light can suppress melatonin production.15-18 Zeitzer and colleagues19 established illuminance–response curves to a 6.5-h experimental light stimulus of varying intensities with light exposure centered 3.5 h before the fitted minimum of the endogenous core body temperature. They reported that 50% of the maximal melatonin suppression response occurred at ~50–130 lux, within the range of typical indoor room light. Recent data from a 5-h evening light exposure protocol suggest that the adult circadian system may be highly sensitive to evening light, with 50% of the melatonin suppression response occurring at an average of only ~25 lux.15 To date, young children's sensitivity to evening light intensity has not been examined and was the objective of this research. Employing a rigorous, experimentally controlled, randomized research design, healthy, preschool-aged children maintained a stable sleep schedule forseven days and then entered an in-home dim-light environment (29.5 h). On the final night of the dim-light protocol, they received a 1-h light exposure in the hour before their habitual bedtime, a time chosen to reflect when children are often exposed to artificial light in their everyday lives. Salivary melatonin was collected in order to calculate baseline dim-light melatonin onset (DLMO), melatonin suppression during the light exposure, and melatonin recovery following light termination. We hypothesized that evening light exposure would induce acute melatonin suppression in a nonlinear intensity-dependent manner","In summary, although we were unable to extend priorresearch with adults by establishing a nonlinearilluminance– response curve for melatonin suppression, our data indicate that melatonin secretion in preschool-aged children is highly sensitive to light in the 1 h before bedtime across a wide range of intensities. Children's evening lighting environments can disrupt the regular production of melatonin, which contributes to physiological changes that prepare the body for sleep in humans,35 and may contribute to the development of evening settling problems (i.e., sleep onset delay and bedtime resistance) in early childhood. These findings highlight the importance of reducing light levelsin the home before bedtime in order to support healthy sleep and circadian rhythms in young children.",https://twitter.com/foundmyfitness/status/1483524431603400704,
Identification of Putative Steroid Receptor Antagonists in Bottled Water: Combining Bioassays and High-Resolution Mass Spectrometry,"Bottled water was found to contain over 24,000 chemicals, including endocrine disruptors. Sixteen of the chemicals inhibited estrogen and androgen receptors up to 90%.","Endocrine disrupting chemicals (EDCs) are man-made compounds interfering with hormone signaling and thereby adversely affecting human health. Recent reports provide evidence for the presence of EDCs in commercially available bottled water, including steroid receptor agonists and antagonists. However, since these findings are based on biological data the causative chemicals remain unidentified and, therefore, inaccessible for toxicological evaluation. Thus, the aim of this study is to assess the antiestrogenic and antiandrogenic activity of bottled water and to identify the causative steroid receptor antagonists. We evaluated the antiestrogenic and antiandrogenic activity of 18 bottled water products in reporter gene assays for human estrogen receptor alpha and androgen receptor. Using nontarget high-resolution mass spectrometry (LTQ-Orbitrap Velos), we acquired corresponding analytical data. We combined the biological and chemical information to determine the exact mass of the tentative steroid receptor antagonist. Further MSn experiments elucidated the molecule’s structure and enabled its identification. We detected significant antiestrogenicity in 13 of 18 products. 16 samples were antiandrogenic inhibiting the androgen receptor by up to 90%. Nontarget chemical analysis revealed that out of 24520 candidates present in bottled water one was consistently correlated with the antagonistic activity. By combining experimental and in silico MSn data we identified this compound as di(2-ethylhexyl) fumarate (DEHF). We confirmed the identity and biological activity of DEHF and additional isomers of dioctyl fumarate and maleate using authentic standards. Since DEHF is antiestrogenic but not antiandrogenic we conclude that additional, yet unidentified EDCs must contribute to the antagonistic effect of bottled water. Applying a novel approach to combine biological and chemical analysis this is the first study to identify so far unknown EDCs in bottled water. Notably, dioctyl fumarates and maleates have been overlooked by science and regulation to date. This illustrates the need to identify novel toxicologically relevant compounds to establish a more holistic picture of the human exposome.","By interfering with the organism’s complex hormone signaling endocrine disrupting chemicals (EDCs) might adversely affect development and reproduction. Moreover, recent research suggests an implication of EDCs in cancer, cardiovascular, and metabolic disorders. While research generates an ever-growing list of potential EDCs, few compounds, namely Bisphenol A (BPA) and phthalates, attract particular scientific attention and public controversy. Used in a vast variety of consumer products, these chemicals are ubiquitously detected in the environment as well as in human samples. With numerous studies documenting adverse effects, public health concerns have led to a voluntary or regulatory removal of BPA and phthalates in some products (e.g., baby bottles, toys) and countries. However, given the multitude of chemicals in use, these measures might not resolve the problem. This is illustrated by a recent study suggesting that plastic products marketed as BPA free release significant amounts of estrogenic activity. The authors employed a sensitive in vitro bioassay to characterize the total estrogenic burden leaching from plastics, including potential mixture effects and unidentified EDCs. Using a similar approach, a series of studies reported a widespread estrogenic contamination of commercially available bottled water. Another study adds to the picture by presenting new findings on androgenic, antiandrogenic, progestagenic, and glucocorticoid-like activity in bottled water. Attempts to explain the observed effects by targeted chemical analysis remained unsuccessful and it has soon become clear that ‘traditional’ EDCs are not responsible for the endocrine activity in bottled water. Since the causative chemical entity remains so far unidentified, the findings are not easy to interpret in a toxicological context and, consequently, prone to criticism. Here, we combine biological and chemical analysis to identify putative steroid receptor antagonists in bottled water. Most of the products were potently antiestrogenic and antiandrogenic in the bioassays. Nontarget high-resolution mass spectrometry pointed towards maleate and fumarate isomers as promising candidates and subsequently enabled the identification of di(2-ethylhexyl) fumarate. Because its concentration is too low to explain the observed activity, other compounds must contribute. However, further maleate/fumarate isomers are not only biologically active but structurally highly similar to phthalates. Hence, we speculate these compounds might represent a novel, so far overlooked group of EDCs","We have shown that antiestrogens and antiandrogens are present in the majority of bottled water products. To identify the causative chemical, we applied a novel correlation approach to integrate biological and high-resolution mass spectrometry data. Structural elucidation led to dioctyl maleate/fumarate isomers as promising candidates. While chemical analysis confirmed that DEHF is the putative steroid receptor antagonist, this compound was weakly antiestrogenic in the bioassays, only. We conclude that we have either missed active compound(s) or that another; untested maleate/fumarate isomer causes the antagonistic activity in bottled water. Two arguments support the latter: In addition to DEHF other isomers were antiestrogenic and antiandrogenic. Moreover, maleates are structurally highly similar to phthalate plasticizers, well-known antiandrogens. Therefore, we pose the hypothesis that dialkyl maleates and fumarates might represent a novel group of steroid receptor antagonists. This illustrates that in spite of the potentially relevant exposure and obvious resemblance to other EDCs such chemicals have been so far disregarded by the scientific and regulatory community. Therefore, we hope that our findings will give fresh impetus to the effect-directed identification of EDCs in beverages, foodstuff, and consumer products which, in the end, will help providing a more holistic picture of human exposure to EDCs.",https://twitter.com/foundmyfitness/status/1479549416662974469,
Association between Sugar-Sweetened Beverage Consumption and Executive Function in Children,Just one sugar-sweetened beverage per week may impair brain function in children.One serving of sugar-sweetened beverages per week was linked to worse executive function in children. This effect was even stronger in children consuming more servings.,"The association between sugar-sweetened beverages (SSB) and executive function among children has been less investigated. We aimed to explore this topic. We randomly recruited 6387 children aged 6–12 years from five elementary schools in Guangzhou, China in 2019. Information on frequency and servings of children’s SSB consumption was assessed using a questionnaire. Children’s executive function was evaluated using parents’ ratings of the Behavioral Rating Inventory of Executive Function (BRIEF), which comprises eight subscales—including inhibit, shift, emotional control, initiate, working memory, plan/organize, organization of materials and monitor, as well as three composite indexes including behavioral regulation index (BRI), metacognition index (MI), and global executive index (GEC). SSB consumption was positively associated with all subscales and composite scores of BRIEF as well as higher risks of elevated executive difficulties, indicating poorer executive function. For example, children who drank SSB ≥2 times/week were related to higher scores of GEC (estimates, 95% confidence interval (CI): 2.44, 1.79 to 3.09) compared with those who never drank SSB. The odds ratio of elevated GEC associated with SSB consumption ≥2 times/week was 1.62 (95% CI: 1.34, 1.96) than non-consumers. The results of this study indicated that SSB consumption was associated with poorer executive function in children.","The consumption of sugar-sweetened beverages (SSB) has been decreasing in most Western countries during the past two decades  but SSB is the major contributor of added sugar in the American diet and its consumption has been increasing worldwide . The harmful impact of SSB on cardiometabolic health has been well documented, and it is associated with greater risks of obesity, hypertension, type 2 diabetes (independently of adiposity), and cardiometabolic death. An estimation based on globally representative data calculated that about 184,000 yearly deaths worldwide in 2010 were attributed to SSB consumption, and three quarters of this burden occurred in low- and middle-income countries. This could be explained by the following several factors: SSB has poor satiating property that can be consumed excessively; SSB displaces more nutritional foods and beverages ; and SSB contain high levels of fructose , and fructose metabolism results in detrimental health outcomes at the organ and metabolic level.In contrast to extensive research on the physical health impacts of SSB consumption, the association between SSB consumption and executive function has been less investigated. Executive function is an umbrella term involving a variety of interrelated, higher-level cognitive skills that were requisite for complex reasoning, goal-oriented activity, and self-regulatory behavior . Although the windows of neurodevelopmental vulnerability occur during prenatal and early postnatal periods, high-order executive skills develop significantly from ages 6 to 10 which makes this period especially sensitive to perturbation. Animal studies have shown that sugar could induce the increases in mediators of inflammation (e.g., IL-6 and IL-1β) and oxidative stress as well as decrease in neurotrophins, and these intermediate factors subsequently altered brain structure and function. Therefore, it is reasonable to hypothesize that excessive consumption of SSB may harmfully impact the performance on executive function. To our knowledge, there were limited epidemiological studies investigating the association between SSB consumption and executive function in children, and results were inconsistent. In addition, the majority of these studies were implemented in high-income countries, and such studies from low- and middle-income countries such as China were scarce, where the level of SSB consumption was relatively low. We therefore aimed to investigate the association between SSB consumption and executive function in children by using data from a cross-sectional study in Guangzhou, Southern China.","In conclusion, the findings of this study found that SSB consumption was associated with poorer performance on executive function among children. Because excessive consumption of SSB is fairly common in many countries, the findings hold importance for informing policy makers to implement intervention strategies on reducing children’s access to SSB for promoting brain health.",https://twitter.com/foundmyfitness/status/1479195565908844544,
High “Normal” Blood Glucose Is Associated with Decreased Brain Volume and Cognitive Performance in the 60s: The PATH through Life Study,People with higher fasting blood sugar levels but still within the normal range were more likely to have a loss of brain volume in the brain regions of the hippocampus and the amygdala compared to those with lower blood sugar levels.,"Type 2 diabetes is associated with cerebral atrophy, cognitive impairment and dementia. We recently showed higher glucose levels in the normal range not to be free of adverse effects and to be associated with greater hippocampal and amygdalar atrophy in older community-dwelling individuals free of diabetes. This study aimed to determine whether blood glucose levels in the normal range (<6.1 mmol/L) were associated with cerebral volumes in structures other than the hippocampus and amygdale, and whether these glucose-related regional volumes were associated with cognitive performance. 210 cognitively healthy individuals (68–73 years) without diabetes, glucose intolerance or metabolic syndrome were assessed in the large, community-based Personality and Total Health Through Life (PATH) study. Baseline blood glucose levels in the normal range (3.2–6.1 mmol/l) were used to determine regional brain volumes and associated cognitive function at wave 3. Higher blood glucose levels in the normal range were associated with lower grey/white matter regional volumes in the frontal cortices (middle frontal gyrus, inferior frontal gyrus precentral gyrus). Moreover, identified cerebral regions were associated with poorer cognitive performance and the structure-function associations were gender specific to men. These findings stress the need to re-evaluate what is considered as healthy blood glucose levels, and consider the role of higher normal blood glucose as a risk factor for cerebral health, cognitive function and dementia. A better lifetime management of blood glucose levels may contribute to improved cerebral and cognitive health in later life and possibly protect against dementia.","It is well established that type 2 diabetes is associated with ‘accelerated brain ageing’, white matter lesions, atrophy, and the presence of infarcts, which in turn relate to reduced cognitive functioning, an increased risk of Alzheimer's disease  and vascular damage. A review by Awad et al.  of the relationship between impaired glucose tolerance, type 2 diabetes and cognitive function highlighted research linking sub-clinical levels of glucose in the high-normal range for glucose tolerance or impaired glucose tolerance (fasting glucose levels <7 mmol/L) with cognitive function, smaller hippocampal volumes and poor glucose regulation [10]. Notably, research has shown a decrement in cognitive function associated with impaired glucose tolerance in men, while women appear to demonstrate virtually identical scores to normoglycaemic women .We have recently shown that higher glucose levels in the normal range (<6.1 mmol/L) are not necessarily free of adverse effects, and are associated with greater hippocampal and amygdalar atrophy in older community-dwelling individuals free of diabetes. These findings are in accordance with animal studies demonstrating higher plasma glucose levels in rats to be associated with hippocampal neuronal loss, decreased neurogenesis, impaired spatial learning, reduced hippocampal dendritic spine density, and reduced long-term potentiation. Furthermore, in non-diabetics, experimentally raised plasma glucose levels have been associated with increased systemic inflammation , abnormal coagulation function, chronic stress and activation of the Hypothalamus-Pituitary-Adrenal axis, which are possible mechanisms that may explain these findings.What is not known is whether glucose levels in the normal range are associated with cerebral volumes in structures other than the hippocampus and amygdala, and whether glucose-related regional volumes are associated with cognitive function. The aim of this study therefore was to assess whether blood glucose levels in the normal range (<6.1 mmol/L) are associated with volumes of other brain regions and to determine whether there is an association between these glucose-related regions and cognitive function in a large cohort of community-based individuals free of diabetes or cognitive impairment.","This study investigated the association between normal blood glucose levels and grey and white matter regional volumes across the whole brain and its association with cognitive performance in a large, community-based sample of cognitively healthy individuals without diabetes, glucose intolerance or metabolic syndrome. Three important findings were made: 1) higher glucose levels in the normal range were associated with lower grey and white matter regional volumes; 2) lower glucose-related grey/white matter regional volumes were associated with poorer cognitive performance; 3) structure-function associations were gender specific.",https://twitter.com/foundmyfitness/status/1479198577658195970,
Serum zinc and pneumonia in nursing home elderly,"Participants who supplemented with a multivitamin containing 50% of the RDA for micronutrients, including zinc, and who raised their serum zinc levels over a year had a 48% lower incidence of pneumonia compared to those with low serum zinc levels.","Zinc plays an important role in immune function. The association between serum zinc and pneumonia in the elderly has not been studied.The objective was to determine whether serum zinc concentrations in nursing home elderly are associated with the incidence and duration of pneumonia, total and duration of antibiotic use, and pneumonia-associated and all-cause mortality. This observational study was conducted in residents from 33 nursing homes in Boston, MA, who participated in a 1-y randomized, double-blind, and placebo-controlled vitamin E supplementation trial; all were given daily doses of 50% of the recommended dietary allowance of essential vitamins and minerals, including zinc. Participants with baseline (n = 578) or final (n = 420) serum zinc concentrations were categorized as having low (<70 μg/dL) or normal (≥70 μg/dL) serum zinc concentrations. Outcome measures included the incidence and number of days with pneumonia, number of new antibiotic prescriptions, days of antibiotic use, death due to pneumonia, and all-cause mortality. Compared with subjects with low zinc concentrations, subjects with normal final serum zinc concentrations had a lower incidence of pneumonia, fewer (by almost 50%) new antibiotic prescriptions, a shorter duration of pneumonia, and fewer days of antibiotic use (3.9 d compared with 2.6 d) (P ≤ 0.004 for all). Normal baseline serum zinc concentrations were associated with a reduction in all-cause mortality (P = 0.049). Normal serum zinc concentrations in nursing home elderly are associated with a decreased incidence and duration of pneumonia, a decreased number of new antibiotic prescriptions, and a decrease in the days of antibiotic use. Zinc supplementation to maintain normal serum zinc concentrations in the elderly may help reduce the incidence of pneumonia and associated morbidity.","Pneumonia is a major public health problem in the elderly (1). An important predisposing factor to the increased incidence of infections such as pneumonia in the elderly is the age-associated decline in immune function (2). Such changes in immune response with age, in addition to malnutrition, contribute to the increased frequency and severity of pneumonia and to the mortality due to pneumonia in the elderly (1–4).Zinc has been shown to play an important role in the regulation of the T cell–mediated function (5–7). Zinc deficiency has been shown to cause thymus involution and to depress lymphocyte proliferation, interleukin-2 (IL-2) production, delayed-type hypersensitivity skin responses, and antibody response to T cell–dependent antigens (5, 8). Similar defects in T cell function have been observed with aging (2). Several investigators have reported low zinc status or decreased intakes in elderly subjects (9–11). Furthermore, low zinc status in the elderly has been shown to contribute to age-associated dysregulation of the immune response (3, 12), and zinc supplementation has been shown to improve T cell–mediated function in the elderly (9, 12–15). In children, low concentrations of circulating zinc have been shown to be associated with an increased risk of respiratory morbidity (16), and zinc supplementation has been shown to reduce both the risk and duration of pneumonia and deaths due to pneumonia in children (17, 18). The association between serum zinc and pneumonia in the elderly, however, has not been studied. From April 1998 to August 2001, a randomized controlled trial was carried out to investigate the effect of vitamin E supplementation on respiratory infections in an elderly nursing home population (19). We found a high proportion (≈30%) of nursing home elderly with low serum zinc concentrations at baseline and after 1 y of follow-up, despite the fact that all participants received 50% of the recommended dietary allowance (RDA) of essential vitamins and minerals, including zinc, during the trial. Because recently published studies in children (17, 18) have shown zinc supplementation to be beneficial in reducing morbidity and mortality from pneumonia and past research has shown the negative effect of zinc deficiency on immune function in the elderly, we examined the relations between serum zinc concentration and the incidence and duration of pneumonia, the number of new antibiotic prescriptions, the days of antibiotic use, death due to pneumonia, and all-cause mortality in elderly nursing home residents.","We observed that elderly nursing home residents with low serum zinc had a higher risk of pneumonia, a longer duration of pneumonia episodes, a greater number of new antibiotic prescriptions, and more days of antibiotic use for the treatment of pneumonia at the end of 1 y of daily micronutrient supplementation with 50% of the RDA, including zinc. In addition, low baseline concentrations of serum zinc in our elderly nursing home population were associated with increased all-cause mortality. The results from our current study, in addition to these earlier findings, suggest that elderly with low serum zinc concentrations might benefit from zinc supplementation. Such a measure has the potential to reduce not only the number of episodes and duration of pneumonia and the number of new antibiotic prescriptions and days of antibiotic use due to pneumonia but also all-cause mortality in the elderly. An adequately powered randomized, double-blind, controlled trial seems to be the likely next step. Such a study is needed to determine the efficacy of zinc supplementation as a potential low-cost intervention to reduce morbidity and mortality due to pneumonia in this vulnerable population.",https://twitter.com/foundmyfitness/status/1473737610661179396,
Alcohol consumption and site-specific cancer risk: a comprehensive dose–response meta-analysis,"Moderate daily alcohol consumption may increase the risk of some cancers.Moderate drinkers (1-3 drinks/day) increase their risk of colorectal cancer by 17%, breast cancer by 23%, larynx cancer by 44%, oral cancer by 83%, and esophageal cancer by 123%.","Alcohol is a risk factor for cancer of the oral cavity, pharynx, oesophagus, colorectum, liver, larynx and female breast, whereas its impact on other cancers remains controversial. We investigated the effect of alcohol on 23 cancer types through a meta-analytic approach. We used dose–response meta-regression models and investigated potential sources of heterogeneity.A total of 572 studies, including 486 538 cancer cases, were identified. Relative risks (RRs) for heavy drinkers compared with nondrinkers and occasional drinkers were 5.13 for oral and pharyngeal cancer, 4.95 for oesophageal squamous cell carcinoma, 1.44 for colorectal, 2.65 for laryngeal and 1.61 for breast cancer; for those neoplasms there was a clear dose–risk relationship. Heavy drinkers also had a significantly higher risk of cancer of the stomach (RR 1.21), liver (2.07), gallbladder (2.64), pancreas (1.19) and lung (1.15). There was indication of a positive association between alcohol consumption and risk of melanoma and prostate cancer. Alcohol consumption and risk of Hodgkin's and Non-Hodgkin's lymphomas were inversely associated. Alcohol increases risk of cancer of oral cavity and pharynx, oesophagus, colorectum, liver, larynx and female breast. There is accumulating evidence that alcohol drinking is associated with some other cancers such as pancreas and prostate cancer and melanoma.","It is estimated that alcohol is responsible for ∼2.5 million deaths each year and for 4.5% of the global burden of disease and injury . Alcohol is an established causal factor for cirrhosis of the liver, epilepsy, poisoning, road traffic accidents, violence and some types of cancer. With regard to cancer, alcohol consumption was estimated to have caused ∼500 000 cancer deaths worldwide in 2004 , and accounted for 4.4% of cancer deaths in China in 2005  and 3.5% in the United States in 2009 . In Europe, a large heterogeneity was observed in patterns and trends of alcohol consumption between countries , with proportion of cancer cases attributable to alcohol varying accordingly.The first published exploratory study on the carcinogenic effect of alcohol dates back to the beginning of the twentieth century, when an excess of cancer mortality due to alcohol consumption was reported. In the wake of the accumulating evidence on the carcinogenicity of alcohol , in 1988 the International Agency for Research on Cancer (IARC) listed alcohol among the carcinogens for oral cavity and pharynx, oesophagus, liver and larynx. Afterwards, given the consolidating data for a link between alcohol and cancer of colorectum and female breast , these two sites were added to the above list in 2010. The results on the association between alcohol and cancer at other sites, such as stomach, pancreas and prostate, are still conflicting. Given the vast and sometimes contradictory literature on the carcinogenicity of alcohol, our group has conducted in recent years a series of meta-analytic studies on the association between alcohol and several single cancersto shed light on the subject. With the present meta-analysis, we aim to provide a more global picture of the association between alcohol drinking and a large variety of cancers.","In conclusion, consumption of alcoholic beverages increases the risk of cancer of the oral cavity and pharynx, oesophagus (SCC), colorectum, liver, larynx and female breast. Some other cancers, such as pancreas and prostate cancer and melanoma, appear to be associated with consumption of alcohol, but more studies are needed to draw any final conclusion. The link of alcohol with stomach and lung cancer and lymphomas could be biased by unaccounted confounders and misclassification of exposure and should be further investigated. There seems to be no association between consumption of alcohol and adenocarcinoma of the oesophagus and gastric cardia, and cancer of the endometrium, urinary bladder and kidney.",https://twitter.com/foundmyfitness/status/1473382391389437962,
Interactions between SARS-CoV‑2 N‑Protein and α‑Synuclein Accelerate Amyloid Formation,Viral infection from influenza or SARS-CoV2 may increase Parkinson's risk (2 new studies). The nucleocapsid protein produced by SARS-CoV2 causes a Parkinson's protein to aggregate at 10x the normal rate. The spike protein had no effect on aggregation.,": First cases that point at a correlation between SARS-CoV-2 infections and the development of Parkinson’s disease (PD) have been reported. Currently, it is unclear if there is also a direct causal link between these diseases. To obtain first insights into a possible molecular relation between viral infections and the aggregation of α-synuclein protein into amyloid fibrils characteristic for PD, we investigated the effect of the presence of SARS-CoV-2 proteins on α-synuclein aggregation. We show, in test tube experiments, that SARS-CoV-2 spike protein (S-protein) has no effect on α-synuclein aggregation, while SARS-CoV-2 nucleocapsid protein (N-protein) considerably speeds up the aggregation process. We observe the formation of multiprotein complexes and eventually amyloid fibrils. Microinjection of N-protein in SH-SY5Y cells disturbed the α-synuclein proteostasis and increased cell death. Our results point toward direct interactions between the N-protein of SARS-CoV-2 and α-synuclein as molecular basis for the observed correlation between SARS-CoV-2 infections and Parkinsonism","Symptoms of SARS-CoV-2 infections that cause the current Covid-19 pandemic are not limited to the respiratory tract. The virus also affects other organs and tissues. SARS-CoV-2 has been found in neurons in different brain regions. For many of the patients infected with SARS-CoV-2, acute and subacute neurological complications have been reported. One of these complications, the loss of smell, is a common premotor symptom in Parkinson’s disease (PD). This symptom and the recent reports of cases of Parkinsonism in relatively young patients after a SARS-CoV-2 infection suggests that there may be a link between SARS-CoV-2 infections and the development of PD.The link between viral infections and neurodegeneration is established for some viruses. The most well-known example is the 1918 influenza pandemic (Spanish flu) which coincided with an increase in encephalitis lethargica, followed by numerous cases of post-encephalitic Parkinsonism. In more recent times, multiple indications of a relation between PD and viral infections have been reported. Whether viral infections indirectly cause neurodegeneration via the immune system or if the effect is direct is unclear. Neurodegenerative diseases such as Alzheimer’s disease (AD) and PD are protein aggregation diseases in which specific proteins, tau and Aβ peptide in AD and α-synuclein (αS) in PD, assemble into amyloid aggregates. Once started, the aggregation process spreads from cell to cell and the formed aggregates and deposits hamper brain function. In a direct mechanism,the virus itself triggers the protein aggregation process. The virus would thus be responsible for the onset of the pathological protein aggregation process. Indeed, such a direct relation has been found for Aβ peptide aggregation (AD) in model cell lines and animals infected with herpes simplex and respiratory syncytial virus. Motivated by the first reports on a potential relation between SARS-CoV-2 infections and the development of Parkinsonism, we set out to investigate if there are indications that these two diseases are molecularly linked. We investigate the direct effect of SARS-CoV-2 proteins on αS aggregation and αS proteostasis in model systems. We show, in test tube experiments, that the SARS-Cov-2 spike protein (S-protein) has no effect on αS aggregation, while SARS-CoV-2 nucleocapsid protein (N-protein) considerably speeds up the aggregation process. N-protein and αS directly interact and this interaction results in the formation of complexes that contain multiple proteins and eventually amyloid fibrils. Microinjection of N-protein in SH-SY5Y cells disturbed the αS proteostasis and increased cell death. Our results suggest that the observed link between SARS-CoV-2 infection and PD might originate from a molecular interaction between virus protein and αS.","We have identified a SARS-CoV-2 protein that induces the aggregation of αS in the test tube. In the initial interaction between the SARS-CoV-2 N-protein and αS, multiprotein complexes are formed. In the presence of N-protein, the onset of αS aggregation into amyloid fibrils is strongly accelerated, indicating that N-protein facilitates the formation of a critical nucleus for aggregation. Fibril formation is not only faster but it also proceeds in an unusual two-step process. In cells, the presence of N-protein changes the distribution of αS over different conformations that likely represent different functions at already short time scales. Disturbance of αS proteostasis might be a first step toward nucleation of fibrils. Our results point toward a direct interaction between the N-protein of SARS-CoV-2 and αS as a molecular basis for the observed relations between virus infections and Parkinsonism. The observed molecular interactions thus suggest that SARS-CoV-2 infections may have long-term implications and that caution is required in considering N-protein as an alternative target in vaccination strategies.",https://twitter.com/foundmyfitness/status/1473045339024216065,
Yogurt consumption and colorectal cancer incidence and mortality in the Nurses' Health Study and the Health Professionals Follow-Up Study,This partially explains why observational studies have shown that the consumption of at least 1 serving of yogurt per week was associated with a 16% reduced risk of colon cancer.,"Yogurt is a commonly consumed fermented food. Regular yogurt consumption may contribute to a favorable gut microbiome and gut health, but few epidemiologic studies have considered the relation between regular yogurt consumption and the incidence of and mortality from colorectal cancer.We used data from 2 large, prospective cohort studies, the Nurses’ Health Study and the Health Professionals Follow-Up Study, to examine the role of yogurt consumption on colorectal cancer incidence and mortality. During 32 years of follow-up in 83,054 women (mean age at baseline, 45.7 years) and 26 years of follow-up in 43,269 men (mean age at baseline, 52.3 years), we documented a total of 2666 newly diagnosed cases of colorectal cancer in these cohorts. We modeled yogurt consumption at baseline and cumulatively updated it throughout follow-up. Baseline yogurt consumption was associated with a reduced risk of colon cancer in age-adjusted analyses (P for trend < 0.001). Associations remained statistically significant after adjusting for potential confounders, including calcium and fiber intake (P for trend = 0.03), and were restricted to proximal colon cancer. The consumption of 1 + servings per week of yogurt at baseline, compared to no yogurt consumption, was associated with a multivariable HR of 0.84 (95% CI, 0.70–0.99; P trend = 0.04) for the proximal colon cancer incidence. Latency analyses suggested that the most important window of opportunity for regular yogurt consumption to prevent colorectal cancer was 16–20 years in the past. When yogurt consumption was cumulatively updated, associations attenuated and were no longer significant. No statistically significant inverse trend was observed between yogurt consumption and the colorectal cancer mortality. In these large cohorts, the frequency of yogurt consumption was associated with a reduced risk of proximal colon cancer with a long latency period. No significant inverse trend was observed for colorectal cancer mortality.","Colorectal cancer (CRC) is the third most common cancer among women and men worldwide. Several important risk factors for CRC have been identified, including genetic predisposition and epigenetic factors, cigarette smoking, obesity, and low physical activity . Moreover, this cancer of the digestive tract is affected by a number of dietary factors, including regular alcohol intake and red and processed meat consumption. In this context, the microbiome is likely an important mediator, with the intestinal microflora harboring a wealth of antagonist organisms; their equilibrium is affected by diet and creates an environment that may prevent or foster tumorigenesis of the intestinal system. The human microbiota is a composite of trillions of microbial cells and viruses that affect many aspects of human health and physiology. Fermented foods contain a large number of live microbes and thus act as probiotics, enriching the gut with beneficial bacteria that help the body absorb nutrients and enhance immune functions by stimulating phagocytosis and preventing inflammation . As these resident microbes nurture on nondigestible fibers (prebiotics), they produce immune-modulating metabolites, such as short-chain fatty acids. The consumption of fermented foods increases beneficial gut microbes by up to 10,000-fold, since a large fraction of bacteria produced during the fermentation process survive passage through the digestive system. Lactic acid bacteria compete for receptors or adhesion to endothelial cells, therefore preventing the access of pathogens to the intestinal epithelium, generating antimicrobial compounds, and producing proteolytic enzymes. Moreover, lactic acid bacteria can inactivate carcinogenic substances, such as N-nitrosamines and heterocyclic aromatic amines, due to the peptidoglycans and polysaccharides in their cell walls that bind these mutagens. Lactic acid bacteria also reduce the fecal enzyme activity of b-glucoronidase, nitroreductase, and azoreductase, which convert procarcinogens to carcinogens in the colon. Yogurt is among the most commonly consumed fermented foods. An early mention of fermented milk dates back to 76 BC when the Roman historian Plinius recommended its use to treat gastrointestinal infections. In the early 1900s, Metchnikoff (24) attributed the longevity of Bulgarians to their frequent consumption of yogurt containing Lactobacillus; he suggested that enrichment with these beneficial bacteria would counterbalance the toxin-producing bacteria. Few studies have considered the effect of frequent yogurt consumption on the CRC incidence. Proximal and distal colon cancer have been associated with different origins and risk factors; Fusobacterium nucleatum is more abundant in proximal than distal colon cancer. Hence, it is important to study these subtypes of colon cancer separately. Since gut bacteria ferment soluble fiber, the interaction between probiotics and prebiotics has recently garnered interest. We used data from 2 large, prospective US cohorts, the Nurses’ Health Study (NHS) and the Health Professionals Follow-Up Study (HPFS), to assess the association of yogurt consumption and the incidence of cancer of the colon and rectum. Because some of the proposed mechanisms involve factors such as immunity that could influence cancer progression and metastases independent of the cancer incidence and since other dietary and lifestyle factors have been associated with both the colorectal incidence and mortality, we also examined the latter.","In summary, we observed a significant trend in proximal colon cancer risk reduction with more frequent consumption of yogurt decades earlier. Whether diet in adolescence and early adulthood may induce long-lasting changes in the microbiota remains to be elucidated. For the distal colon cancer incidence, long-term yogurt consumption and the beneficial effect of its calcium content may be more relevant. Residual confounding, however, cannot be excluded as an explanation for our findings, especially since yogurt consumption is associated with many healthy behaviors, not all of which may have been assessed or were even quantifiable. Future studies need to confirm these findings, in particular with respect to heterogeneity in the anatomic subsites.",https://twitter.com/foundmyfitness/status/1469020143174832130,
Effect of Aspirin on Cancer Incidence and Mortality in Older Adults,"To add fuel to the fire..A large randomized controlled trial (19K people) found older adults taking 100 mg of daily aspirin were more likely to be diagnosed with advanced, metastatic cancers & more likely to die from cancer than those taking a placebo.","ASPirin in Reducing Events in the Elderly, a randomized, double-blind, placebo-controlled trial of daily low-dose aspirin (100 mg) in older adults, showed an increase in all-cause mortality, primarily due to cancer. In contrast, prior randomized controlled trials, mainly involving younger individuals, demonstrated a delayed cancer benefit with aspirin. We now report a detailed analysis of cancer incidence and mortality.19114 Australian and US community-dwelling participants aged 70 years and older (US minorities 65 years and older) without cardiovascular disease, dementia, or physical disability were randomly assigned and followed for a median of 4.7 years. Fatal and nonfatal cancer events, a prespecified secondary endpoint, were adjudicated based on clinical records. 981 cancer events occurred in the aspirin and 952 in the placebo groups. There was no statistically significant difference between groups for all incident cancers (hazard ratio [HR] ¼ 1.04, 95% confidence interval [CI] ¼ 0.95 to 1.14), hematological cancer (HR ¼ 0.98, 95% CI ¼ 0.73 to 1.30), or all solid cancers (HR ¼ 1.05, 95% CI ¼ 0.95 to 1.15), including by specific tumor type. However, aspirin was associated with an increased risk of incident cancer that had metastasized (HR ¼ 1.19, 95% CI ¼ 1.00 to 1.43) or was stage 4 at diagnosis (HR ¼ 1.22, 95% CI ¼ 1.02 to 1.45), and with higher risk of death for cancers that presented at stages 3 (HR ¼ 2.11, 95% CI ¼ 1.03 to 4.33) or 4 (HR ¼ 1.31, 95% CI ¼ 1.04 to 1.64). In older adults, aspirin treatment had an adverse effect on later stages of cancer evolution. These findings suggest that in older persons, aspirin may accelerate the progression of cancer and, thus, suggest caution with its use in this age group.","The ASPirin in Reducing Events in the Elderly (ASPREE) was a randomized controlled trial (RCT) comparing daily low-dose aspirin (100 mg) vs placebo in 19 114 Australian and US adults aged 70 years or older (or aged 65 years or older among US African Americans and Hispanics) who were free of known cardiovascular disease, dementia, or physical disability at trial entry . We recently reported that ASPREE participants randomized to aspirin experienced higher all-cause mortality. This became evident at 3 years postrandomization and was largely attributable to death from cancer. This finding was unexpected in the context of results from prior RCTs and meta-analyses . Generally, these studies, conducted in a younger average age group, reported that aspirin did not affect the short-term risk of cancer although Rothwell recently reported an increase in cancer incidence during the early years of follow-up among older trial participants. With more prolonged follow-up, participants randomized to daily aspirin had a reduced risk of incident cancer and death from cancer, particularly colorectal cancer . This led the US Preventive Services Task Force in 2016 to recommend low-dose aspirin for primary prevention of cardiovascular events and colorectal cancer among US adults aged 50-59 years with a greater than 10% 10-year risk of a cardiovascular event . However, this advice did not extend to adults aged 70 years and older where the evidence was considered insufficient . The present report now provides a more detailed analysis of the effect of aspirin on cancer incidence and mortality occurring in the ASPREE participants with the aim of better understanding the findings and implications of the study ","In summary, among generally healthy adults predominantly 70 years of age or older at enrollment and followed for a median of 4.7 years, daily low-dose aspirin was associated with an increased risk of incident solid cancers presenting at an advanced stage. Mortality from both localized and advanced cancers was higher in those taking aspirin, suggesting a possible adverse effect of aspirin on cancer evolution in older adults. Cancer molecular and genetic data give reason to suggest that the potential adverse impact of aspirin identified in ASPREE might be specific to this age group. The cohort continues to be followed to explore the possibility of a delayed reduction in cancer incidence and/or mortality that may emerge with longerterm observation",https://twitter.com/foundmyfitness/status/1466533857667153920,
The majority of SARS-CoV-2-specific antibodies in COVID-19 patients with obesity are autoimmune and not neutralizing,"Two new studies suggest possible mechanisms why obesity increases severe COVID-19 risk First, the majority of antibodies found in obese people infected with SARS-CoV2 were non-neutralizing autoimmune antibodies which could cause severe tissue damage.","Obesity decreases the secretion of SARS-CoV-2-specific IgG antibodies in the blood of COVID-19 patients. How obesity impacts the quality of the antibodies secreted, however, is not understood. Therefore, the objective of this study is to evaluate the presence of neutralizing versus autoimmune antibodies in COVID-19 patients with obesity. Thirty serum samples from individuals who tested positive for SARS-CoV-2 infection by RT-PCR were collected from inpatient and outpatient settings. Of these, 15 were lean (BMI < 25) and 15 were obese (BMI ≥ 30). Control serum samples were from 30 uninfected individuals, age-, gender-, and BMI-matched, recruited before the current pandemic. Neutralizing and autoimmune antibodies were measured by ELISA. IgG autoimmune antibodies were specific for malondialdehyde (MDA), a marker of oxidative stress and lipid peroxidation, and for adipocyte-derived protein antigens (AD), markers of virus-induced cell death in the obese adipose tissue. SARS-CoV-2 infection induces neutralizing antibodies in all lean but only in few obese COVID-19 patients. SARS-CoV-2 infection also induces anti-MDA and anti-AD autoimmune antibodies more in lean than in obese patients as compared to uninfected controls. Serum levels of these autoimmune antibodies, however, are always higher in obese versus lean COVID-19 patients. Moreover, because the autoimmune antibodies found in serum samples of COVID-19 patients have been correlated with serum levels of C-reactive protein (CRP), a general marker of inflammation, we also evaluated the association of anti-MDA and anti-AD antibodies with serum CRP and found a positive association between CRP and autoimmune antibodies.Our results highlight the importance of evaluating the quality of the antibody response in COVID-19 patients with obesity, particularly the presence of autoimmune antibodies, and identify biomarkers of self-tolerance breakdown. This is crucial to protect this vulnerable population at higher risk of responding poorly to infection with SARS-CoV-2 than lean controls.","The novel single-stranded RNA coronavirus SARS-CoV-2 (severe acute respiratory syndrome corona virus-2) emerged in the last months of 2019, caused the worldwide coronavirus disease 2019 (COVID-19) pandemic, and was responsible for different clinical manifestations ranging from mild disease to severe respiratory tract infections, multiorgan failure, and death. The severe manifestations of the disease are associated with an exuberant inflammatory response and the development of a condition known as cytokine storm . Published data have indicated that inflammaging, the chronic low-grade systemic inflammation, is a major cause of the cellular and molecular changes induced by SARS-CoV-2 and can be responsible for the highest number of deaths. In addition, inflammaging induces chronic immune activation (IA) and dysfunctional immunity.Resolution of SARS-CoV-2 infection requires both innate and adaptive immune responses that lead to the clearance and elimination of the virus from the organism. B cells contribute to this process by producing neutralizing antibodies that prevent the spread of infectious virions, control virus dissemination, and reduce tissue damage. Neutralizing antibodies generated against the Spike glycoprotein of the SARS-CoV-1 in the 2002–2003 pandemic have shown efficacy in protecting from severe disease. Moreover, during the current pandemic, it has been shown that neutralizing antibodies against the Spike glycoprotein of SARS-CoV-2, found in plasma from convalescent COVID-19 patients, induced fast recovery when transfused into critically ill patients.Obesity is an inflammatory condition associated with inflammaging and chronic IA, contributing to functional impairment of immune cells, and decreased immunity. Obese individuals have been shown to respond poorly to infections , vaccination, and therapies for autoimmune conditions. Therefore, obesity represents an additional risk factor for COVID-19 patients. A strong association between obesity, obesity-associated comorbidities, and severe outcomes of COVID-19 has indeed been shown, with adult COVID-19 symptomatic patients with Body Mass Index (BMI) >30 showing higher admission to acute and critical care compared to lean and overweight individuals (BMI < 30) . These results have been confirmed in part in a multi-site prospective cohort of non-hospitalized individuals in which obesity was found to be associated with the presence of multiple COVID-19 symptoms However, in this cohort obesity was not associated with increased risk of infection.","In conclusion, our results highlight the importance of identifying protective (neutralizing) versus pathogenic (autoimmune) antibodies in COVID-19 patients with obesity. In addition, similar autoimmune antibodies may also be secreted following COVID-19 vaccination. However, the reactogenicity of lipid nanoparticle-formulated COVID-19 mRNA vaccines in individuals with obesity, characterized by dysregulation of immune responses, has not been investigated yet. Therefore, evaluating the quality of the antibody response of obese COVID-19 patients is crucial to protect this vulnerable population at higher risk of responding poorly to infection with SARS-CoV-2 and vaccination against SARS-CoV-2 lean controls. Although our study has the limitation of having a small number of recruited participants, we believe our results have a high impact showing that obese individuals may be at a higher risk to respond poorly to SARS-CoV-2 infection.",https://twitter.com/foundmyfitness/status/1462844248458293248,
SARS-CoV-2 infects human adipose tissue and elicits an inflammatory response consistent with severe COVID-19,"Second, the SARS-CoV-2 virus infects adipose tissue, increasing inflammation during COVID-19. Immune cells called macrophages in fat cells were infected with virus leading to immune hyperreactivity and excessive production of pro-inflammatory proteins.","The COVID-19 pandemic, caused by the viral pathogen SARS-CoV-2, has taken the lives of millions of individuals around the world. Obesity is associated with adverse COVID-19 outcomes, but the underlying mechanism is unknown. In this report, we demonstrate that human adipose tissue from multiple depots is permissive to SARS-CoV-2 infection and that infection elicits an inflammatory response, including the secretion of known inflammatory mediators of severe COVID-19. We identify two cellular targets of SARS-CoV-2 infection in adipose tissue: mature adipocytes and adipose tissue macrophages. Adipose tissue macrophage infection is largely restricted to a highly inflammatory subpopulation of macrophages, present at baseline, that is further activated in response to SARS-CoV-2 infection. Preadipocytes, while not infected, adopt a proinflammatory phenotype. We further demonstrate that SARS-CoV-2 RNA is detectable in adipocytes in COVID-19 autopsy cases and is associated with an inflammatory infiltrate. Collectively, our findings indicate that adipose tissue supports SARS-CoV-2 infection and pathogenic inflammation and may explain the link between obesity and severe COVID-19.","The COVID-19 pandemic, caused by the viral pathogen SARS-CoV-2, has infected over 250 million people and taken the lives of over 4.5 million individuals globally as of October 2021. The clinical manifestations of COVID-19 range from asymptomatic infection, mild cold-like symptoms, to severe pulmonary and extrapulmonary manifestations characterized by extreme inflammation and cytokine storm. Obesity has emerged as an independent risk factor for infection, severe disease, and mortality. While obesity is associated with comorbid conditions also related to severe COVID-19, the independent relative risk of obesity is higher than that of hypertension and type 2 diabetes . Further, obesity is a risk factor even in young adults and children who do not have other comorbid conditions. Several distinct mechanisms could underlie this association. Impaired respiratory mechanics may result from a heavy chest wall, airway resistance, and/or presence of obstructive sleep apnea. The metabolic milieu in obesity, particularly among individuals with insulin resistance, is characterized by systemic inflammation and hypercoagulability and could thus stimulate a more robust inflammatory response to SARS-CoV-2. Impaired immune responses to viral infection are another possibility, as obese individuals exhibit altered immune cell profiles at baseline  and in response to influenza infection . Furthermore, a recent report demonstrated that SARS-CoV-2 can infect adipocytes in vitro; it is not known whether SARS-CoV-2 infects other adipose tissue-resident cells and/or drives an inflammatory response in adipose tissue. Other viruses have been shown to infect several cell types within adipose tissue, including adipocytes (influenza A virus), adipose-stromal cells (adenovirus 36, human cytomegalovirus), macrophages (simian immunodeficiency virus (SIV)), and T cells (human immunodeficiency virus (HIV)). Complex interactions between various cell types and adipocytes can drive significant inflammation, with reports of adipocyte-derived chemoattractants such as monocyte chemoattractant protein-1 (MCP-1) leading to macrophage infiltration (20), tumor necrosis factor alpha (TNF-ɑ) activating nuclear factor kappa B , and free fatty acids driving toll-like receptor 4-mediated inflammation and insulin resistance. Thus, pronounced and/or prolonged SARS-CoV-2 viral replication and inflammation might occur in those with obesity and contribute to severe disease. Of particular concern is the possibility that viral infection of peri-organ fat could contribute to organ damage via inflammation and downstream processes such as extracellular matrix deposition/fibrosis, edema, impaired cellular function, endothelial dysfunction, and hypercoagulability. To date, COVID-19 profiling studies have generally excluded adipose tissue from analyses; in other cases adipose tissue may have been lumped in with analyses of adjoining tissues These studies have shown that SARS-CoV-2 RNA and proteins are detected across numerous tissues, including the lung, brain, intestine, and pancreas. Thus, we undertook a study to test the hypothesis that SARS-CoV-2 infects cells within human adipose tissue and incites an inflammatory response. We harvested adipose tissue from multiple depots in uninfected obese humans for in vitro infection and obtained autopsy specimens of various adipose depots in individuals who died from COVID-19. Our results clearly show SARS-CoV-2 infection in macrophages and adipocytes from multiple adipose depots, with an attendant increase in inflammatory profile.","Overall, here we provide evidence that two cell types within human adipose tissue are permissive to SARS-CoV-2 infection. This adds to data showing susceptibility of other tissues including heart, kidneys, pharynx, liver, brain, and pancreas . SARS-CoV-2 RNA was detected in autopsy samples at a higher viral load than in adjacent organs, the kidney or heart. Importantly, infection of adipose tissue drives an inflammatory response in infected macrophages and preadipocytes. Thus, multiple cells within adipose tissue likely participate in both viral replication and inflammation. Importantly, we demonstrated infection and inflammation in adipose tissue adjacent to critical organs such as the heart and intestine, thus pointing to the potential for adipose tissue potentiation of organ damage in severe COVID-19. Furthermore, if adipose cells constitute a reservoir for viral infection, obesity may contribute not only to severe acute disease, but also to long-COVID syndrome. Collectively, our data implies that infection in adipose tissue may partially explain the link between obesity and severe COVID-19. More efforts to understand the complexity and contributions of this tissue to COVID-19 pathogenesis are warranted.",https://twitter.com/foundmyfitness/status/1462844249691475968,
"Crosstalk among intestinal barrier, gut microbiota and serum metabolome after a polyphenol-rich diet in older subjects with “leaky gut”: The MaPLE trial","People with ""leaky gut syndrome"" who ate a polyphenol-rich diet for 8 weeks improved the composition and function of the gut microbiome, lowered zonulin (a biomarker of leaky gut), and had other beneficial effects on gut metabolism.","The MaPLE study was a randomized, controlled, crossover trial involving adults ≥60 y.o. (n = 51) living in a residential care facility during an 8-week polyphenol-rich (PR)-diet. Results from the MaPLE trial showed that the PR-diet reduced the intestinal permeability (IP) in older adults by inducing changes to gut microbiota (GM). The present work aimed at studying the changes in serum metabolome in the MaPLE trial, as a further necessary step to depict the complex crosstalk between dietary polyphenols, GM, and intestinal barrier. Serum metabolome was monitored using a semi-targeted UHPLC-MS/MS analysis. Metataxonomic analysis (16S rRNA gene profiling) of GM was performed on faecal samples. Clinical characteristics and serum levels of the IP marker zonulin were linked to GM and metabolomics data in a multi-omics network.Compared to the control diet, the PR-diet increased serum metabolites related to polyphenols and methylxanthine intake. Theobromine and methylxanthines, derived from cocoa and/or green tea, were positively correlated with butyrate-producing bacteria (the order Clostridiales and the genera Roseburia, Butyricicoccus and Faecalibacterium) and inversely with zonulin. A direct correlation between polyphenol metabolites hydroxyphenylpropionic acid-sulfate, 2-methylpyrogallol-sulfate and catechol-sulfate with Butyricicoccus was also observed, while hydroxyphenylpropionic acid-sulfate and 2-methylpyrogallol-sulfate negatively correlated with Methanobrevibacter. The multi-omics network indicated that participant's age, baseline zonulin levels, and changes in Porphyromonadaceae abundance were the main factors driving the effects of a PR-diet on zonulin.Overall, these results reveal the complex relationships among polyphenols consumption, intestinal permeability, and GM composition in older adults, and they may be important when setting personalized dietary interventions for older adults.","Increased intestinal permeability (IP), a condition also known as “leaky gut”, has been proposed as a potential contributor to inflamm-aging and a wide range of intestinal disorders such as inflammatory bowel disease, coeliac disease, and Crohn's disease, as well as several chronic diseases such as cardio-renal-metabolic diseases . Increased IP is characterized by a low-grade systemic inflammation triggered by the diffusion of toxins or bacterial factors to the bloodstream. Age has been reported as an independent risk factor for altered IP, and some studies have shown an increased IP over the age of 50. Moreover, gut microbiota (GM) is another regulator of IP implicated in the renovation of the intestinal epithelial cells and in maintaining the integrity of tight junctions. Indeed, a detrimental modification of the microbial community structure in the gut (dysbiosis) can lead to a loss of immune tolerance and to the development of a gut inflammatory environment coupled with increased IP. In consequence, dysbiosis and/or altered IP may not only lead to the overproduction and absorption of toxic metabolites with potential deleterious effects on host-health, but also compromise the bioavailability of nutrients and beneficial food components, such as polyphenols. Among the strategies to prevent “leaky gut” and to decrease IP associated with aging or chronic diseases, changes of lifestyle factors, including diet, should be the most feasible. Higher consumption of fruits, vegetables and other plant-based foods provides dietary fibre and polyphenols which might help to counteract an impairment of IP through aging. In addition, GM activity may improve the structure of the tight junctions and modulate the inflammatory environment in the gut through lower molecular weight compounds derived from food components. In the MaPLE trial, we found that an 8-week polyphenol-rich (PR) diet comprising 3 daily portions of PR-foods such as cocoa, green tea and berries (1391 mg/day of dietary polyphenols vs. 812 mg/day registered in the control diet) led to a significant reduction of the IP marker, zonulin, in older subjects affected by “leaky gut” . Thus, our primary hypothesis is that serum metabolome changes will be associated with the improvement of IP in older adults through GM-dependent and GM-independent pathways. Here, we also investigated the crosstalk between intestinal barrier and the changes of GM composition and serum metabolome linking clinical characteristics to metataxonomics and metabolomics data through a multi-omics network using Mixed Graphical Models.","Understanding the impact of IP on host responses to dietary interventions is mandatory for personalized dietary counselling. For this purpose, for the first time we present a metabolomics study on the effect of a PR-diet on IP in older subjects with “leaky gut”, also evaluating the impact of changes in GM composition on the main outcome of this intervention. Metabolomics affords important data about the molecular effectors that link the consumption of certain foods to their biological activity, and these should be used as pieces of the complex puzzle that represents the different molecular pathways involved. In light of the close interrelations among IP, food constituents and GM composition, such analyses could reveal the complex interplay between GM, the bioavailability of specific dietary constituents, and their effects on the intestinal barrier. In conclusion, IP was a main factor modulating the serum metabolome changes during a PR dietary intervention in older adults with “leaky gut”. The effects of the PR-diet on IP were mainly related with age, baseline IP and relative abundance of Porphyromonadaceae. Possible relationships between cocoa-derived methylxanthines and IP merit further studies.",https://twitter.com/foundmyfitness/status/1461061858249490435,
Enhanced Protein Translation Underlies Improved Metabolic and Physical Adaptations to Different Exercise Training Modes in Young and Old Humans,Optimizing for healthy mitochondria may be crucial for staving off neurodegenerative diseases. One way to do this is exercise! High-intensity interval training increased mitochondrial capacity in younger people by 49% and 69% in older people after 3 wks.,"The molecular transducers of benefits from different exercise modalities remain incompletely defined. Here we report that 12 weeks of high-intensity aerobic interval (HIIT), resistance (RT), and combined exercise training enhanced insulin sensitivity and lean mass, but only HIIT and combined training improved aerobic capacity and skeletal muscle mitochondrial respiration. HIIT revealed a more robust increase in gene transcripts than other exercise modalities, particularly in older adults, although little overlap with corresponding individual protein abundance was noted. HIIT reversed many age-related differences in the proteome, particularly of mitochondrial proteins in concert with increased mitochondrial protein synthesis. Both RT and HIIT enhanced proteins involved in translational machinery irrespective of age. Only small changes of methylation of DNA promoter regions were observed. We provide evidence for predominant exercise regulation at the translational level, enhancing translational capacity and proteome abundance to explain phenotypic gains in muscle mitochondrial function and hypertrophy in all ages.","Health benefits of exercise are indisputable in combating age-related risks for disease and disability, and understanding the transducers of such benefits is of high national interest. Aerobic exercise training leads to skeletal muscle protein remodeling and stimulates multiple molecular steps,including DNA methylation and synthesis of new proteins . Many studies have demonstrated changes to mRNA content, but the extent to which transcriptional changes lead to changes in protein abundance remains inconclusive  Understanding the regulation of skeletal muscle molecular adaptations to diverse types of exercise training can help to develop future targeted therapies and exercise recommendations. There is a gap in knowledge about age effects on pathways regulating exercise adaptations in response to different exercise modalities. Different types of exercise can stimulate variable, but specific, responses in muscle functions. Aerobic exercise training enhances mitochondrial oxidative enzymes’ capacity and coincides with improvements to insulin sensitivity with age . It remains to be determined whether age-related decline in muscle mitochondrial protein synthesis  is reversed by aerobic training. High-intensity aerobic interval training (HIIT) involves repeating short bouts of activity at near-maximal intensity, which rapidly and robustly increases aerobic capacity, mitochondrial respiration, and insulin sensitivity in young people . Resistance training (RT) reverses sarcopenia and age-related declines in myosin heavy-chain gene transcripts and synthesis rates of muscle proteins , but a comprehensive gene transcripts and proteome comparison with aerobic training has not been performed. Combined training (CT) offers many benefits of both aerobic and resistance training, although the intensity of aerobic and resistance components are lower than either HIIT or standard RT programs. Lower exercise intensity may limit training adaptations , particularly of mitochondria . A comprehensive approach to different exercise programs and the specific physiological and molecular adaptations and the potential impact of age on these adaptations remain to be determined. We performed comprehensive metabolic and molecular phenotyping of young and older adults in response to 12 weeks of aerobic training (using HIIT), RT, and 12 weeks of a sedentary period followed by CT of moderate-intensity aerobic plus resistance training. These measurements were performed 72 hr following the last bout of exercise to specifically determine the training effect. We hypothesized that skeletal muscle transcriptome, translation, and proteome would increase with training, and the pattern of responses would reflect the type of training modality and phenotype changes. HIIT robustly improved cardio-respiratory fitness, insulin sensitivity, mitochondrial respiration, and fat-free mass (FFM) in both age groups. RT improved FFM and insulin sensitivity in both age groups while CT had lesser gains, perhaps due to differences in training intensity. RNA sequencing of muscle biopsies revealed robust increases in mRNA expression with HIIT, more so than RT or CT, particularly of mitochondrial transcripts. Quantitative proteomics in response to HIIT revealed larger proteomic changes, particularly in mitochondrial and ribosome proteins, as well as reversal of many age-related changes. We report relatively small changes (<10%) to methylation of DNA promoter regions and low overlap between transcriptional and proteomic changes. Thus, our findings indicate that the regulation of exercise adaptation is tightly linked with protein translation and translational machinery.","We assessed the effects of three different exercise modalities on skeletal muscle adaptations in young and older adults and explained on the basis of changes in transcriptome, translational regulation, and proteome abundance. HIIT training in young and older adults increased VO2 peak, insulin sensitivity, mitochondrial respiration, FFM, and muscle strength. In contrast, RT increased insulin sensitivity and FFM, but not VO2 peak or mitochondrial function. CT involved lower intensity than HIIT or RT groups and resulted in modest gains in FFM and VO2 peak, with modest gains in insulin sensitivity, primarily in young people. Supervised HIIT appears to be an effective recommendation to improve cardio-metabolic health parameters in aging adults.We were interested in understanding the molecular transducers of exercise adaptations and performed RNA sequencing to determine changes to gene transcripts in skeletal muscle biopsies. HIIT robustly increased gene expression, particularly in older adults, while RT and CT had less pronounced effects in both age groups. Of interest, a set of gene transcripts were increased with HIIT in both young and older groups despite select genes having either greater or lower content at baseline in older adults. These data demonstrated that HIIT induced a pattern of gene expression regardless of age. Finally, HIIT also had robust increases in transcriptional and translational regulation of muscle growth and mitochondrial pathways.Our study was powered to detect relevant effect sizes at the proteomic level, which demonstrated robust gains, particularly in proteins regulating translation. There were also robust effect sizes for training groups on metabolic phenotypes. For example, HIIT training in older adults had strong effect sizes in multiple outcomes, including mitochondrial respiration (1.7), aerobic fitness (0.99), insulin sensitivity (0.5), and smaller effect sizes for 1RM leg press (0.3) and FFM (0.1). Other parameters, such as DNA methylation, did not detect differences, and we cannot exclude the possibility of type II error. Additionally, a source of variability between mRNA content and protein abundance is the potential for splice variants to generate peptides that may not be annotated in mass spectrometry libraries. A high degree of alternative splicing could impact our datasets and potentially underestimate the relationship between protein abundance and mRNA content, as multiple splice variants may lead to the same peptide.The increases in specific muscle proteins were greater relative to changes in mRNA content, particularly in mitochondrial and ribosomal proteins, and demonstrate a lack of direct relation between transcriptional and proteomic adaptations. DNA methylation is a regulatory point for transcription and had relatively small changes. Collectively, these data suggest that exercise adaptations are regulated to a greater extent at the post-transcriptional level. Increased ribosome protein content and other proteins involved in the translational machinery were detected following HIIT and provide for increased translational capacity. Mitochondrial protein synthesis was increased with HIIT as directly measured by isotope incorporation (representing translation). These data demonstrate an increase in both the protein translation machinery and synthesis rates of proteins. We also found a lowering of post-translational protein damages (oxidation and deamidation) following exercise training that may improve the functional quality of proteins. The increased mitochondrial protein synthesis, along with proteomic gains, despite differences in mRNA transcripts, support the hypothesis that translational level regulation is a predominant factor of mitochondrial biogenesis in human in response to exercise training. Further support for the above notion is provided by the increase in ribosomal protein content despite a fall in ribosomal transcript levels. The increases in specific proteins in muscle were greater relative to the changes in mRNA content, particularly in mitochondrial proteins and ribosomal proteins, and this demonstrates a lack of direct relation between transcriptional and proteomic abundances when measured 72 hr following the last bout of exercise. Together, the current results demonstrated a predominant regulation of exercise adaptations at the post-transcriptional level.",https://twitter.com/foundmyfitness/status/1456328845108252688,
"Vitamin D and the omega-3 fatty acids control serotonin synthesis and action, part 2: relevance for ADHD, bipolar disorder, schizophrenia, and impulsive behavior","Vitamin D as a regulator of serotonin could implicate it in neurodevelopment and neuropsychiatric disorders beyond ADHD. In my 2015 follow-up, co-authored with Dr. Bruce Ames, we discuss relevance for ADHD, bipolar disorder, schizophrenia, and more.","Serotonin regulates a wide variety of brain functions and behaviors. Here, we synthesize previous findings that serotonin regulates executive function, sensory gating, and social behavior and that attention deficit hyperactivity disorder, bipolar disorder, schizophrenia, and impulsive behavior all share in common defects in these functions. It has remained unclear why supplementation with omega-3 fatty acids and vitamin D improve cognitive function and behavior in these brain disorders. Here, we propose mechanisms by which serotonin synthesis, release, and function in the brain are modulated by vitamin D and the 2 marine omega-3 fatty acids, eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA). Brain serotonin is synthesized from tryptophan by tryptophan hydroxylase 2, which is transcriptionally activated by vitamin D hormone. Inadequate levels of vitamin D (∼70% of the population) and omega-3 fatty acids are common, suggesting that brain serotonin synthesis is not optimal. We propose mechanisms by which EPA increases serotonin release from presynaptic neurons by reducing E2 series prostaglandins and DHA influences serotonin receptor action by increasing cell membrane fluidity in postsynaptic neurons. We propose a model whereby insufficient levels of vitamin D, EPA, or DHA, in combination with genetic factors and at key periods during development, would lead to dysfunctional serotonin activation and function and may be one underlying mechanism that contributes to neuropsychiatric disorders and depression. This model suggests that optimizing vitamin D and marine omega-3 fatty acid intake may help prevent and modulate the severity of brain dysfunction.","Serotonin regulates a wide variety of brain functions and behaviors. Here, we synthesize previous findings that serotonin regulates executive function, sensory gating, and social behavior and that attention deficit hyperactivity disorder, bipolar disorder, schizophrenia, and impulsive behavior all share in common defects in these functions. It has remained unclear why supplementation with omega-3 fatty acids and vitamin D improve cognitive function and behavior in these brain disorders. Here, we propose mechanisms by which serotonin synthesis, release, and function in the brain are modulated by vitamin D and the 2 marine omega-3 fatty acids, eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA). Brain serotonin is synthesized from tryptophan by tryptophan hydroxylase 2, which is transcriptionally activated by vitamin D hormone. Inadequate levels of vitamin D (~70% of the population) and omega-3 fatty acids are common, suggesting that brain serotonin synthesis is not optimal. We propose mechanisms by which EPA increases serotonin release from presynaptic neurons by reducing E2 series prostaglandins and DHA influences serotonin receptor action by increasing cell membrane fluidity in postsynaptic neurons. We propose a model whereby insufficient levels of vitamin D, EPA, or DHA, in combination with genetic factors and at key periods during development, would lead to dysfunctional serotonin activation and function and may be one underlying mechanism that contributes to neuropsychiatric disorders and depression. This model suggests that optimizing vitamin D and marine omega-3 fatty acid intake may help prevent and modulate the severity of brain dysfunction.—Patrick, R. P., Ames, B. N. Vitamin D and the omega-3 fatty acids control serotonin synthesis and action, part 2: relevance for ADHD, bipolar disorder, schizophrenia, and impulsive behavior","We propose that serotonergic dysfunction is a common denominator in a wide range of neuropsychiatric illnesses including ASD, ADHD, bipolar disorder, schizophrenia, impulsive behavior disorders, and depression. This proposal is based on evidence that executive function, sensory gating, and prosocial behavior are all regulated by serotonin and that serotonin levels are low and polymorphisms in serotonin-related genes are common in many of these disorders. We propose that an underlying mechanism is by vitamin D hormone regulating serotonin synthesis, thus modulating the severity of the aforementioned defects. We also provide evidence supporting mechanisms by which EPA regulates the release of serotonin by inhibiting the production of E2 series prostaglandins and DHA controls serotonin function by increasing neuronal cell membrane fluidity. Our proposed mechanism explains how vitamin D and the marine omega-3 fatty acids work in concert with each other to improve cognitive function, health, and behavior. This synergy can be explained in part by their effects on the serotonin system: vitamin D regulates serotonin synthesis, EPA influences serotonin release, and DHA improves membrane embedded serotonin receptor accessibility. It also partially explains why supplementation with vitamin D, EPA, and DHA improves some behaviors associated with ADHD, bipolar disorder, schizophrenia, and impulsive behavior by controlling serotonin production and function. Although many intervention studies with vitamin D, EPA, and DHA have shown an apparent benefit, larger clinical trials need to be done to determine efficacious doses for these various disorders.We also review evidence demonstrating how estrogen can overcome the defects on sensory gating and executive function when serotonin is experimentally lowered. We propose that this effect may be caused by estrogen's ability to activate TPH2, thus explaining the lower female prevalence in psychiatric disorders. The role of the activating VDRE found in TPH2 offers a novel explanation of why vitamin D hormone is required for normal serotonin synthesis in the brain and of how low vitamin D could affect the trajectory and development of neuropsychiatric illness. Likewise, estrogen's capacity to boost TPH2 expression serves as an explanation of why females are more protected from mental illness.",https://twitter.com/foundmyfitness/status/1455256870986256385,
Sleep and longitudinal cognitive performance in preclinical and early symptomatic Alzheimer’s disease ,"There may be a sleep ""sweet spot"" with too little or too much sleep associated with impaired brain function. Older adults who consistently get 6-8 hours of sleep per night delay cognitive dysfunction and have sharper brains.","Sleep monitoring may provide markers for future Alzheimer’s disease; however, the relationship between sleep and cognitive function in preclinical and early symptomatic Alzheimer’s disease is not well understood. Multiple studies have associated short and long sleep times with future cognitive impairment. Since sleep and the risk of Alzheimer’s disease change with age, a greater understanding of how the relationship between sleep and cognition changes over time is needed. In this study, we hypothesized that longitudinal changes in cognitive function will have a non-linear relationship with total sleep time, time spent in non-REM and REM sleep, sleep efficiency and non-REM slow wave activity.  To test this hypothesis, we monitored sleep-wake activity over 4–6 nights in 100 participants who underwent standardized cognitive testing longitudinally, APOE genotyping, and measurement of Alzheimer’s disease biomarkers, total tau and amyloid-β42 in the CSF. To assess cognitive function, individuals completed a neuropsychological testing battery at each clinical visit that included the Free and Cued Selective Reminding test, the Logical Memory Delayed Recall assessment, the Digit Symbol Substitution test and the Mini-Mental State Examination. Performance on each of these four tests was Z-scored within the cohort and averaged to calculate a preclinical Alzheimer cognitive composite score. We estimated the effect of cross-sectional sleep parameters on longitudinal cognitive performance using generalized additive mixed effects models. Generalized additive models allow for non-parametric and non-linear model fitting and are simply generalized linear mixed effects models; however, the linear predictors are not constant values but rather a sum of spline fits. We found that longitudinal changes in cognitive function measured by the cognitive composite decreased at low and high values of total sleep time (P < 0.001), time in non-REM (P < 0.001) and REM sleep (P < 0.001), sleep efficiency (P < 0.01) and <1 Hz and 1–4.5 Hz non-REM slow wave activity (P < 0.001) even after adjusting for age, CSF total tau/amyloid-β42 ratio, APOE ε4 carrier status, years of education and sex. Cognitive function was stable over time within a middle range of total sleep time, time in non-REM and REM sleep and <1 Hz slow wave activity, suggesting that certain levels of sleep are important for maintaining cognitive function. Although longitudinal and interventional studies are needed, diagnosing and treating sleep disturbances to optimize sleep time and slow wave activity may have a stabilizing effect on cognition in preclinical or early symptomatic Alzheimer’s disease. ","Deposition of amyloid-β as insoluble parenchymal plaques and intracellular accumulation of aggregated, hyperphosphorylated tau as neurofibrillary tangles throughout the neuropil are key steps in the pathogenesis of Alzheimer’s disease that lead to synaptic and neuronal loss, cognitive dysfunction and eventual dementia Tau hyperphosphorylation (p-tau) is an early step in tau-mediated neurodegeneration. Amyloid PET scans show deposition of amyloid as insoluble fibrillar amyloid-β deposits (i.e. amyloid-positive). The concentration of amyloid-β42 in the CSF decreases with amyloid deposition and is also a marker of amyloid status. Amyloid PET scans show increasing amounts of amyloid deposition while an individual is still cognitively normal, although CSF total tau (t-tau) begins to increase. Tau PET scans, which show paired helical filament pathology, only become positive many years after amyloid PET scans become positive, and there are already decreases in CSF amyloid-β42 and increases in t-tau and p-tau around the time that clinical symptoms appear.Although soluble amyloid-β42, p-tau and t-tau in human CSF are biomarkers for early (amyloid-β42) and increasing (t-tau and p-tau) amyloid deposition, the CSF t-tau/amyloid-β42 ratio is associated with increased amyloid deposition in the brain4 and is superior to single biomarkers at predicting the risk of clinical decline and conversion to dementia.","In this study, we observed that the relationship between cross-sectional measures of total sleep time, time in NREM stage 2 and 3, time in REM, and <1 Hz NREM SWA and cognitive function over time, as assessed by the PACC, was non-linear. This relationship was seen even after adjusting for multiple potential confounders that can affect sleep and cognition, including age, CSF markers of Alzheimer’s disease pathology, APOE ε4 allele carrier status, years of education and sex. These findings have important implications for using sleep to track the risk of developing cognitive impairment in the clinic or in response to an intervention in a clinical trial. Furthermore, these results support the suggestion that sleep measures have an optimal middle range where PACC scores are stable and suggest targets for sleep interventions to help maintain cognitive function in individuals at risk of developing Alzheimer’s disease. We also found that the Logical Memory Test, a story memory test, was significantly associated with both total sleep time and <1 Hz NREM SWA. The MMSE, a general cognitive test, was associated with total sleep time while the DSST, a test of multiple cognitive functions, was associated with <1 Hz NREM SWA. The FCSRT, a word list test of episodic memory, was associated with both total sleep time and <1 Hz NREM SWA. Further research is needed to determine if specific sleep measures are associated with longitudinal changes on specific neuropsychological tests.",https://twitter.com/foundmyfitness/status/1450894695257673730,
Somnogenic Cytokines and Models Concerning Their Effects on Sleep,"From what we know about sleep, the idea that too much is deleterious has always felt jarring. Should we practice sleep restriction if we're sleeping too much. Reality: It may be a spurious association since cytokines can have a somnogenic quality","All the sleep-promoting substances currently identified also have other biological activities. Despite years of effort, a single specific central nervous system sleep center has not been described. These observations led us to propose a biochemical model of a sleep activational system in which the effects of several sleep factors are integrated into a regulatory scheme. These sleep factors interact by altering the metabolism, production, or activity of each other and thereby result in multiple feedback loops. This web of interactions leads to sleep stability in that minor challenges to the system will not greatly alter sleep. The system, however, is responsive to strong perturbations, such as sleep deprivation and infectious disease. The sleep-promoting effects of cytokines and their interactions with prostaglandins and the neuroendocrine system are used to illustrate the functioning of a part of the sleep activational system under normal conditions and during infectious disease. Although the actions of individual sleep factors are not specific to sleep, their interactions at various levels of the neuraxis can mediate a specific sleep response. Such a system would also be responsive to the autonomic and environmental parameters that alter sleep.","The concept that sleep is regulated in part by humoral mechanisms is supported by experimental demonstrations of somnogenic substances in tissue fluids. These experiments began at the turn of the century with Legendre and Pieron and Ishimori, who described the accumulation during prolonged wakefulness of substances in cerebrospinal fluid (CSF) that induce excess sleep when transferred to recipient animals. Although the chemical identity of these substances was never established, today over 30 putative sleep factors (SFs) have been identified. Most of these putative SFs are hormones, immunomodulators, and/or substances  involved in endocrine and/or immune regulation.Until recently, it was postulated that a SF, herein defined as any substance found within the body that alters sleep, would have biological actions specific to sleep and would act on central nervous system (CNS) executive sleep centers, which were also assumed to be concerned primarily with sleep regulation. A single CNS center necessary for sleep has not, however, been demonstrated. Furthermore, all SFs identified to date have multiple biological activities. Thus, a revision of the original assumptions is necessary. This review describes a model that links the sleep effects of many putative SFs in a sleep activational system. The sleep activational system presented is fundamentally a biochemical model and, like all models, is provisional; the cellular localization of many of the specific components that are germane to sleep regulation is not known. Nevertheless, it is assumed that specific components of the model ultimately interact with either glia and/or neurons at various levels of the CNS and that such interactions lead to altered sleep. We propose that such a system would be responsive to the many autonomic and environmental parameters that influence sleep, yet would retain the capacity to regulate sleep selectively. Rapid-eye-movement sleep (REMS) and non-rapid-eye-movement sleep (NREMS) are the two major types of sleep, although subdivisions of each of these classes have been made. To identify these states of vigilance, the electroencephalogram (EEG) and other physiological measurements, e.g., electromyogram, brain temperature, and motor activity, are recorded. REMS and NREMS states exist in most mammals. Sleep in animals, however, is slightly different from that of humans. Thus, species often used in sleep research, such as rats, cats, and rabbits, have sleep episodes that are relatively short, typically lasting only for a few minutes. These bouts of sleep occur sporadically throughout the 24-hour day, alternating with episodes of wakefulness, although the percentage of any given hour spent in sleep is strongly influenced by circadian rhythms","From the discussions presented here, it is clear that cytokines and many other substances alter sleep. We have presented a model of how the regulation of many SFs may be interrelated and another model illustrating how SFs may interact with various neuronal sets to produce specific sleep responses. Ten years ago, it would have been unrealistic to present such models because the roster of putative SFs was much smaller, and there was very little information concerning how SFs might affect each other; however, despite the great number of advances within the field of SFs, many questions remain unanswered. Major challenges that need to be addressed include: (1) How do SF concentrations change with sleep/wake cycles and under pathological conditions? (2) What is the timing of these changes? (3) Where do these changes take place? (4) Is one change linked directly to another?",https://twitter.com/foundmyfitness/status/1450894696448815107,
"Vitamin C supplementation promotes mental vitality in healthy young adults: results from a cross-sectional analysis and a randomized, double-blind, placebo-controlled trial",Young adults given 500 mg of vitamin C twice a day for 4 weeks had better cognitive performance requiring sustained attention and improved work motivation compared to those given a placebo.,"We aimed to investigate the link of vitamin C status with vitality and psychological functions in a cross-sectional study, and examine their causal relationship through a randomized controlled trial (RCT).We first conducted a population-based cross-sectional investigation of healthy young adults (n = 214, 20–39 years), and analyzed the associations of serum vitamin C concentrations with vitality (fatigue and attention) and mood status (stress, depression, and positive and negative affect) using Pearson’s correlation and multiple linear regression analyses. Next, we performed a double-blind RCT in healthy subjects whose serum vitamin C concentrations were inadequate (< 50 μmol/L). Subjects were randomly allocated to receive 500 mg of vitamin C twice a day for 4 weeks (n = 24) or a placebo (n = 22). We assessed vitality, which included fatigue, attention, work engagement, and self-control resources, and measured mood status, including stress, depression, positive and negative affect, and anxiety. ELISA determined serum brain-derived neurotrophic factor (BDNF), and a Stroop color–word test evaluated attention capacity and processing speed.In the cross-sectional data, the serum vitamin C concentration was positively associated with the level of attention (r = 0.16, p = 0.02; standardized β = 0.21, p = 0.003), while no significant associations with the levels of fatigue and mood variables being found. In the RCT, compared to the placebo, the vitamin C supplementation significantly increased attention (p = 0.03) and work absorption (p = 0.03) with distinct tendency of improvement on fatigue (p = 0.06) and comprehensive work engagement (p = 0.07). The vitamin C supplementation did not affect mood and serum concentrations of BDNF. However, in the Stroop color–word test, the subjects supplemented with vitamin C showed better performance than those in the placebo group (p = 0.04).Inadequate vitamin C status is related to a low level of mental vitality. Vitamin C supplementation effectively increased work motivation and attentional focus and contributed to better performance on cognitive tasks requiring sustained attention.","Vitamin C (ʟ-ascorbic acid or ascorbate) is an essential nutrient in humans that functions as an indispensable electron donor and a cofactor in various biological reactions such as hydroxylation of collagen, biosynthesis of carnitine, and tyrosine metabolism . Interestingly, vitamin C presents its highest concentrations in the brain, and animal model and in vitro studies have reported that vitamin C performs critical roles in brain functions. Vitamin C protects neurons from oxidative stress, induces differentiation and maturation of neurons, and regulates the synthesis or release of neuro-modulating factors including serotonin, catecholamines, and glutamate . Accordingly, vitamin C is inferred to be important for maintaining normal mental health. Humans rely on dietary supply to obtain vitamin C due to the absence of a gene encoding ʟ-gulonolactone oxidase, which is critical for vitamin C synthesis from glucose. Although vitamin C deficiency can be prevented by consuming one or two servings of citrus fruits or vegetables; reports state that an inadequate vitamin C status is prevalent among young adults even in industrialized countries . The poor vitamin C status in the young can be attributed to external factors such as smoking, excessive drinking, and unhealthy eating habits that fail to provide a fresh and balanced diet rich in vitamin C. Thus, even healthy young individuals can be at risk of vitamin C deficiency, and consequently, poor body functions due to these lifestyle-related factors. However, compared to the elderly, inadequate vitamin C status in the young is liable to remain undiagnosed or be considered as being of little importance.","In conclusion, this study is the first, to our knowledge, to show the link between vitamin C status with mental functions in healthy young adults using both population-based observational studies and randomized clinical trials. The cross-sectional study suggests inadequate vitamin C status is related to a low level of mental vitality. In the randomized clinical trial, vitamin C supplementation at 1000 mg/day for 4 weeks effectively increases serum vitamin C concentrations in subjects with suboptimal vitamin C status. The supplementation promotes their mental vitality, especially work motivation and attentional focus, contributing to better performance on cognitive tasks that require sustained attention.",https://twitter.com/foundmyfitness/status/1445106700164292608,
"COVID-19 mortality risk correlates inversely with vitamin D3 status, and a mortality rate close to zero could theoretically be achieved at 50 ng/ml 25(OH)D3: Results of a systematic review and meta-analysis",A population study and 7 clinical studies independently found that higher vitamin D levels were linked to lower COVID-19 mortality risk. The sweet spot? 50 ng/ml (Math regressions of data suggested this as theoretical point of close to zero mortality.),"Much research shows that blood calcidiol (25(OH)D3) levels correlate strongly with SARS-CoV-2 infection severity. There is open discussion regarding whether low D3 is caused by the infection or if deficiency negatively affects immune defense. The aim of this study was to collect further evidence on this topic. Systematic literature search was performed to identify retrospective cohort as well as clinical studies on COVID-19 mortality rates versus D3 blood levels. Mortality rates from clinical studies were corrected for age, sex and diabetes. Data were analyzed using correlation and linear regression. One population study and seven clinical studies were identified, which reported D3 blood levels pre-infection or on the day of hospital admission. They independently showed a negative Pearson correlation of D3 levels and mortality risk (r(17)=-.4154, p=.0770/r(13)=-.4886, p=.0646). For the combined data, median (IQR) D3 levels were 23.2 ng/ml (17.4 – 26.8), and a significant Pearson correlation was observed (r(32)=-.3989, p=.0194). Regression suggested a theoretical point of zero mortality at approximately 50 ng/ml D3. The two datasets provide strong evidence that low D3 is a predictor rather than a side effect of the infection. Despite ongoing vaccinations, we recommend raising serum 25(OH)D levels to above 50 ng/ml to prevent or mitigate new outbreaks due to escape mutations or decreasing antibody activity.","The SARS-CoV-2 pandemic causing acute respiratory distress syndrome (ARDS) has lasted for more than 18 months. It has created a major global health crisis due to the high number of patients requiring intensive care, and the high death rate has substantially affected everyday life through contact restrictions and lockdowns. According to many scientists and medical professionals, we are far from the end of this disaster and hence must learn to coexist with the virus for several more years, perhaps decades.It is realistic to assume that there will be new mutations, which are possibly more infectious or more deadly. In the known history of virus infections, we have never faced a similar global spread. Due to the great number of viral genome replications that occur in infected individuals and the error-prone nature of RNA-dependent RNA polymerase, progressive accrual mutations do and will continue to occur . Thus, similar to other virus infections such as influenza, we have to expect that the effectiveness of vaccination is limited in time, especially with the current vaccines designed to trigger an immunological response against a single viral protein.We have already learned that even fully vaccinated people can be infected . Currently, most of these infections do not result in hospitalization, especially for young individuals without comorbidities. However, these infections are the basis for the ongoing dissemination of the virus in a situation where worldwide herd immunity against SARS-CoV-2 is rather unlikely. Instead, humanity could be trapped in an insuperable race between new mutations and new vaccines, with an increasing risk of newly arising mutations becoming resistant to the current vaccines. Thus, a return to normal life in the near future seems unlikely. Mask requirements as well as limitations of public life will likely accompany us for a long time if we are not able to establish additional methods that reduce virus dissemination.Vaccination is an important part in the fight against SARS-CoV-2 but, with respect to the situation described above, should not be the only focus. One strong pillar in the protection against any type of virus infection is the strength of our immune system. Unfortunately, thus far, this unquestioned basic principle of nature has been more or less neglected by the responsible authorities. It is well known that our modern lifestyle is far from optimal with respect to nutrition, physical fitness and recreation. In particular, many people are not spending enough time outside in the sun, even in summer. The consequence is widespread vitamin D deficiency, which limits the performance of their immune systems, resulting in the increased spread of some preventable diseases of civilization, reduced protection against infections and reduced effectiveness of vaccination In this publication, we will demonstrate that vitamin D3 deficiency, which is a well-documented worldwide problem, is one of the main reasons for severe courses of SARS-CoV-2 infections. The fatality rates correlate well with the findings that elderly people, black people and people with comorbidities show very low vitamin D3 levels. Additionally, with only a few exceptions, we are facing the highest infection rates in the winter months and in northern countries, which are known to suffer from low vitamin D3 levels due to low endogenous sun-triggered vitamin D3 synthesis.","lthough there are a vast number of publications supporting a correlation between the severity and death rate of SARS-CoV-2 infections and the blood level of vitamin D3, there is still an open debate about whether this relation is causal. This is because in most studies, the vitamin D level was determined several days after the onset of infection; therefore, a low vitamin D level may be the result and not the trigger of the course of infection.In this publication, we used a meta-analysis of two independent sets of data. One analysis is based on the long-term average vitamin D3 levels documented for 19 countries. The second analysis is based on 1601 hospitalized patients, 784 who had their vitamin D levels measured within a day after admission, and 817 whose vitamin D levels were known pre-infection. Both datasets show a strong correlation between the death rate caused by SARS-CoV-2 and the vitamin D blood level. At a threshold level of 30 ng/ml, mortality decreases considerably. In addition, our analysis shows that the correlation for the combined datasets intersects the axis at approximately 50 ng/ml, which suggests that this vitamin D3 blood level may prevent any excess mortality. These findings are supported not only by a large infection study, showing the same optimum, but also by the natural levels observed in traditional people living in the region where humanity originated from that were able to fight down most (not all) infections in most (not all) individuals. Vaccination is and will be an important keystone in our fight against SARS-CoV-2. However, current data clearly show that vaccination alone cannot prevent all SARS-CoV-2 infections and dissemination of the virus. This scenario possibly will become much worse in the case of new virus mutations that are not very susceptible to the current vaccines or even not sensitive to any vaccine.Therefore, based on our data, the authors strongly recommend combining vaccination with routine strengthening of the immune system of the whole population by vitamin D3 supplementation to consistently guarantee blood levels above 50 ng/ml (125 nmol/l). From a medical point of view, this will not only save many lives but also increase the success of vaccination. From a social and political point of view, it will lower the need for further contact restrictions and lockdowns. From an economical point of view, it will save billions of dollars worldwide, as vitamin D3 is inexpensive and – together with vaccines – provides a good opportunity to get the spread of SARS-CoV-2 under control.Although there exists very broad data-based support for the protective effect of vitamin D against severe SARS-CoV-2 infections, we strongly recommend initiating well-designed observational studies as mentioned and/or double-blind randomized controlled trials (RCTs) to convince the medical community and the health authorities that vitamin D testing and supplementation are needed to avoid fatal breakthrough infections and to be prepared for new dangerous mutations.",https://twitter.com/foundmyfitness/status/1442587635835293699,
Magnesium intake and incidence of pancreatic cancer: the VITamins and Lifestyle study,"Large study including over 66,000 men and women found that for every 100 mg decrease in magnesium intake there was a 24% increase in pancreatic cancer independent of age, body mass, and gender. ~45% of the US population has insufficient magnesium intake.","Studies document that magnesium is inversely associated with the risk of diabetes, which is a risk factor of pancreatic cancer. However, studies on the direct association of magnesium with pancreatic cancer are few and findings are inconclusive. In this study, we aimed to investigate the longitudinal association between magnesium intake and pancreatic cancer incidence in a large prospective cohort study. A cohort of 66 806 men and women aged 50–76 years at baseline who participated in the VITamins And Lifestyle (VITAL) study was followed from 2000 to 2008. Multivariable-adjusted Cox regression models were used to estimate hazard ratios (HRs) and 95% confidence intervals (CIs) of pancreatic cancer incidence by magnesium intake categories. During an average of 6.8-year follow-up, 151 participants developed pancreatic cancer. Compared with those who met the recommended dietary allowance (RDA) for magnesium intake, the multivariable-adjusted HRs (95% CIs) for pancreatic cancer were 1.42 (0.91, 2.21) for those with magnesium intake in the range of 75–99% RDA and 1.76 (1.04, 2.96) for those with magnesium intake o75% RDA. Every 100 mg per day decrement in magnesium intake was associated with a 24% increase in the incidence of pancreatic cancer (HR: 1.24; 95% CI: 1.02, 1.50; Ptrend ¼ 0.03). The observed inverse associations appeared not to be appreciably modified by age, gender, body mass index, and non-steroidal anti-inflammatory drug use but appeared to be limited to those taking magnesium supplementation (from multivitamins or individual supplement). Findings from this prospective cohort study indicate that magnesium intake may be beneficial in terms of primary prevention of pancreatic cancer.","Pancreatic cancer is the fourth leading cause of cancer-related death in both men and women in the United States . The overall incidence of pancreatic cancer has not significantly changed since 2002 but the mortality rate has increased an average of 0.4% annually from 2002-2011 . It was estimated that about 46 420 people in the United States would be diagnosed with pancreatic cancer and about 39 590 would die of this disease in 2014. Approximately 80% of pancreatic cancer patients have concomitant diabetes. Studies show that pancreatic tumour cells have receptors for insulin and have high levels of iinsulin as in type-2 diabetes and insulin resistance. Insulin and insulin-like growth factor (IGF) promote pancreatic tumour cell growth . Because epidemiological studies suggest that magnesium intake is inversely associated with the risk of type-2 diabetes  – a risk factor of pancreatic cancer , it is reasonable to hypothesise that magnesium intake may decrease the risk of pancreatic cancer. However, data directly relating magnesium intake to the incidence of pancreatic cancer are sparse and the findings are inconsistent. Therefore, we aimed to (1) investigate the longitudinal association between magnesium intake and the incidence of pancreatic cancer, and (2) explore whether age, gender, body mass index (BMI, kg m 2 ), non-steroidal anti-inflammatory drugs (NSAIDs) use and magnesium supplementation are effect modifiers in this large prospective cohort study.","In conclusion, a high level of magnesium intake (that meet RDA) may be beneficial in terms of primary prevention of pancreatic cancer. Adhering to the RDA for magnesium intake is recommended. To achieve that level, dietary magnesium intake alone may not be sufficient. Magnesium supplementation may help achieve the RDA for magnesium, especially for those who may have an elevated risk of pancreatic cancer, such as those with family history of pancreatic cancer or diabetes mellitus. Further research is needed to confirm our findings and to establish causal inference.",https://twitter.com/foundmyfitness/status/1440743052188913668,
Whole-body repeated hyperthermia increases irisin and brain-derived neurotrophic factor: A randomized controlled trial,"Exercise causes a hormone called irisin to be released from muscle. Irisin stimulates BDNF, a growth factor that can help grow new neurons. A new study shows whole-body hyperthermia (similar to a sauna) increases irisin & BDNF in young adults.","Hyperthermia is known to be beneficial to patients affected by various diseases. Irisin is a key regulators of fat metabolism known to be released as response to cold. Brain Derived Neurotrophic Factor (BDNF) is a marker of neuroplasticity usually increased as response to acute exposure to human body stressors. Effect of a repeated hyperthermia exposure programme on changes in circulating irisin and serum BDNF in healthy humans.Setting, Participants: Randomized, single-blind, cross-over trial in healthy humans conducted at Sechenov University Physiology Laboratory from April 2019. The treatment period was 2 weeks (wash-out 3 weeks). Researchers analysing serum biomarkers and questionnaire data were blinded to participants allocation. Par ticipants were 20 healthy male (age 21.5 ± 2.1 years).  Hyperthermia exposure programme (WBPH) versus sham exposure (SHAM) to hyperthermia (10 sessions in two weeks). Main outcome measure: Changes in irisin and BDNF before and after short hyperthermia exposure. Twenty participants were analyzed. Irisin increased significantly in group WBPH only: 6.3 μg/ml (mean with SD = 1.6) compared to 5.4 μg/ml (SD = 1.7) in SHAM group; This value was also higher than baseline (5.0 mean with SD = 1.1) in WBPH. After 10 sessions mean change in BDNF was higher in WBPH group vs SHAM:BDNF was 28,263 (SD = 4213) pg/ml in WBPH group and 24,064 (SD = 5600) pg/ml in SHAM group. BDNF concentrations were significantly higher than baseline values in WBPH group only, 28,263 (SD 4213) vs 25,888 (SD 4316) pg/ml.In healthy young humans a 2-week, ten sessions programme consisting of repeated exposure to hyperthermia resulted in a significantly higher increase of circulating Irisin and BDNF","Adaptations of the human body to various forms of heat stress and hyperthermia have been shown to play a positive role in protecting from developing major chronic and degenerative diseases, including cardiovascular, neurodegenerative and metabolic disorders . According to our previous studies on healthy humans, adaptation to 10 weeks of passive hyperthermia sessions is characterised by an increase in aerobic performance and cardiorespiratory endurance, an increase in subjectively reported quality of life and in the level of serum Brain-Derived Neurotrophic Factor (BDNF) . Since the synthesis of BDNF has been suggested to be modulated by the myokine irisin (Askari et al., 2018) and hyperthermia can induce BDNF release, it is worth investigating the interplay between the two to better elucidate the cross-talk between skeletal muscles and adipocytes. Irisin is known as an exercise-inducedand cold-induced myokine that is released primarily by skeletal muscle and white subcutaneous adipose tissues. It has keyregulatory roles in muscles and bones regeneration, antioxidant defence, endothelial functions, conversion of white fat to brown fat, thus a potential in transmitting some of the metabolic benefits of muscular exercise to the adipose tissue (browning and thermogenesis). Circulating irisin levels are increased in individuals regularly exercising and progressively reduced in those less active or sedentary and correlate positively with muscle mass. A 10-week exercise programme increases circulating levels of irisin and this myokine can also play a regulating role on enhancing muscle mass and strength and this effect has been shown being associated to an increased IGF-1 and decreased myostatin levels in a dose-dependent manner. Its role seems to be increasingly relevant in a wider cross-talk between muscle, adipose tissues, brain and circulation. The mechanisms of irisin secretion and its relationship with other substances involved in the body’s response to stressors, including cortisol and growth factors, are not yet clear. A better understanding of the integration between catabolic and anabolic hormones and myokines would facilitate to assess and to predict the development of adaptive cross-effects when using various stimuli, including thermal stress.","In conclusion, even if our data were collected in healthy humans and if we are not able to differentiate between the two main tissues (skeletal muscle and adipose) known as sources of circulating irisin our findings are novel and indicative of a new role for hyperthermia as potentially useful non-pharmacological, non-invasive treatment for people with glucose and lipid metabolisms impairments . Our data suggest that irisin is released as acute response to an acutely increased body temperature and support the role of hyperthermia (without exercising) as sufficient stimulus to increase both irisin and BDNF. Since irisin is a key mediator in muscle and adipose tissues metabolisms further research should focus on clarifying the role of passive heating on modulating the thermogenesis process in healthy humans as well as in chronically ill patients, and on investigating the cross-talk between skeletal muscle, adipose tissue and brain triggered by passive heating of the whole body",https://twitter.com/foundmyfitness/status/1438203650162053121,